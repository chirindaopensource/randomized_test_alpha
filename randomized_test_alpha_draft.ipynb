{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9D1_5XVhKwK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# README.md\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![Statsmodels](https://img.shields.io/badge/Statsmodels-150458.svg?style=flat&logo=python&logoColor=white)](https://www.statsmodels.org/stable/index.html)\n",
        "[![Joblib](https://img.shields.io/badge/Joblib-013243.svg?style=flat&logo=python&logoColor=white)](https://joblib.readthedocs.io/en/latest/)\n",
        "[![Matplotlib](https://img.shields.io/badge/Matplotlib-%23ffffff.svg?style=flat&logo=Matplotlib&logoColor=black)](https://matplotlib.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2507.17599v1-b31b1b.svg)](https://arxiv.org/abs/2507.17599v1)\n",
        "[![Research](https://img.shields.io/badge/Research-Empirical%20Asset%20Pricing-green)](https://github.com/chirindaopensource/randomized_test_alpha)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Econometrics-blue)](https://github.com/chirindaopensource/randomized_test_alpha)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Panel%20Data%20%7C%20Monte%20Carlo-orange)](https://github.com/chirindaopensource/randomized_test_alpha)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/randomized_test_alpha)\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/randomized_test_alpha`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"A General Randomized Test for Alpha\"** by:\n",
        "\n",
        "*   Daniele Massacci\n",
        "*   Lucio Sarno\n",
        "*   Lorenzo Trapani\n",
        "*   Pierluigi Vallarino\n",
        "\n",
        "The project provides a complete, end-to-end pipeline for testing the joint null hypothesis of zero alpha in high-dimensional linear factor models. It replicates the paper's novel randomized testing procedure, which is robust to common violations of classical statistical assumptions such as non-Gaussianity, conditional heteroskedasticity, and strong cross-sectional dependence. The goal is to provide a transparent, robust, and computationally efficient \"truth detector\" for asset pricing models.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: run_full_study](#key-callable-run_full_study)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"A General Randomized Test for Alpha.\" The core of this repository is the iPython Notebook `randomized_test_alpha_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation and cleansing to the final generation of empirical results and Monte Carlo simulations.\n",
        "\n",
        "The central question in empirical asset pricing is whether a proposed factor model can fully explain the cross-section of expected returns. A model is considered well-specified if the pricing errors, or \"alphas,\" are jointly indistinguishable from zero. This project provides the tools to rigorously test this hypothesis in challenging, high-dimensional settings (where the number of assets `N` can be larger than the number of time periods `T`).\n",
        "\n",
        "This codebase enables users to:\n",
        "-   Rigorously validate, clean, and align large panels of asset and factor return data.\n",
        "-   Apply the novel randomized alpha test to various factor model specifications in a rolling-window fashion.\n",
        "-   Conduct comprehensive Monte Carlo simulations to verify the test's statistical properties (size and power) under various data generating processes.\n",
        "-   Benchmark the test's performance against other methods from the literature.\n",
        "-   Automatically generate publication-quality tables and visualizations to report the findings.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in modern panel data econometrics and Extreme Value Theory.\n",
        "\n",
        "**The Linear Factor Model:** The analysis begins with the standard time-series regression for a panel of `N` assets over `T` periods:\n",
        "$$\n",
        "y_{i,t} = \\alpha_i + \\beta'_i f_t + u_{i,t}\n",
        "$$\n",
        "The null hypothesis is that the model is correctly specified, meaning all pricing errors (`α_i`) are jointly zero:\n",
        "$$\n",
        "H_0: \\max_{1 \\le i \\le N} |\\alpha_i| = 0\n",
        "$$\n",
        "\n",
        "**Econometric Challenges:** Traditional tests like the GRS test fail in modern settings due to:\n",
        "1.  **High Dimensionality:** When `N > T`, the `N x N` residual covariance matrix is singular and cannot be inverted.\n",
        "2.  **Restrictive Assumptions:** Classical tests often assume normally distributed, homoskedastic, and serially uncorrelated errors, which are frequently violated by financial returns.\n",
        "\n",
        "**The Randomized Test Methodology:** The paper's key innovation is a multi-step procedure that circumvents these issues:\n",
        "1.  **OLS Estimation:** Estimate `α̂_i` and residuals `û_{i,t}` for each asset `i`.\n",
        "2.  **Normalization:** Transform the alphas into a scale-free statistic `ψ_{i,NT}` that converges to zero under `H₀` but diverges under the alternative `Hₐ`.\n",
        "    $$ \\psi_{i,NT} = \\left| \\frac{T^{1/\\nu} \\hat{\\alpha}_{i,T}}{\\hat{s}_{NT}} \\right|^{\\nu/2} $$\n",
        "3.  **Randomization:** Perturb the `ψ` statistics with i.i.d. standard normal shocks `ω_i` to create `z_{i,NT} = ψ_{i,NT} + ω_i`. Under `H₀`, the `z` statistics behave like a standard normal sample.\n",
        "4.  **Test Statistic:** The final test statistic is the maximum of the perturbed series, `Z_{N,T} = max(z_{i,NT})`. By Extreme Value Theory, the distribution of `Z_{N,T}` under `H₀` is asymptotically Gumbel, providing a known distribution for inference without estimating any covariance matrices.\n",
        "5.  **De-randomization:** To ensure a deterministic result, the randomization is repeated `B` times, and the final decision is based on the fraction of times `Z_{N,T}` falls below its asymptotic critical value.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`randomized_test_alpha_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Data Pipeline:** A robust validation and cleansing module for preparing large panel datasets.\n",
        "-   **Empirical Analysis Engine:** A high-level orchestrator that automates the rolling-window analysis for multiple user-defined factor models.\n",
        "-   **Core Test Implementation:** A precise, numerically stable implementation of the complete randomized testing procedure, from OLS estimation to the final de-randomized decision.\n",
        "-   **Monte Carlo Framework:** A powerful, parallelized simulation engine to evaluate the test's size and power under various DGPs (Gaussian, Student's t, GARCH errors; weak, semi-strong, and strong factors).\n",
        "-   **Comparative Analysis:** A framework for benchmarking the test against other methods from the literature (e.g., FLY, AS tests).\n",
        "-   **Automated Reporting:** Functions to automatically generate publication-quality summary tables and visualizations for both empirical and simulation results.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Data Preparation (Tasks 1-2):** The pipeline ingests raw asset and factor returns, validates their structure, handles missing values via constrained imputation, treats outliers with cross-sectional winsorization, and aligns the datasets.\n",
        "2.  **Empirical Analysis (Tasks 3-9):** It sets up a rolling-window schedule and, for each window and each model, executes the full testing procedure:\n",
        "    -   Vectorized OLS estimation (Task 4).\n",
        "    -   `ψ` statistic computation (Task 5).\n",
        "    -   Randomization and `Z` statistic computation (Task 6).\n",
        "    -   De-randomization and `Q` statistic computation (Task 7).\n",
        "    -   Application of the final decision rule (Task 8).\n",
        "3.  **Monte Carlo Analysis (Tasks 13-17):** It sets up a grid of simulation experiments, generates data from complex DGPs (Task 14), and runs the test `M` times to compute empirical size and power (Task 15), managed by a high-level orchestrator (Task 16).\n",
        "4.  **Reporting (Tasks 11, 12, 18, 19):** It synthesizes all quantitative outputs into summary tables, power curve plots, and a structured textual interpretation of the findings.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `randomized_test_alpha_draft.ipynb` notebook is structured as a logical pipeline with modular functions for each task:\n",
        "\n",
        "-   **Tasks 1-3:** Data Validation, Cleansing, and Empirical Setup.\n",
        "-   **Tasks 4-8:** The core testing pipeline for a single window (Estimation, Statistic Computation, Randomization, De-randomization, Decision).\n",
        "-   **Task 9:** `EmpiricalAnalysisOrchestrator` class to run the empirical study.\n",
        "-   **Tasks 10-12:** Empirical Robustness, Compilation, and Visualization.\n",
        "-   **Tasks 13-16:** Monte Carlo Setup, DGP, Simulation Engine, and Orchestrator.\n",
        "-   **Tasks 17-19:** Monte Carlo Robustness, Compilation, and Interpretation.\n",
        "-   **Task 20:** `run_full_study` master function to execute the entire project.\n",
        "\n",
        "## Key Callable: run_full_study\n",
        "\n",
        "The central function in this project is `run_full_study`. It orchestrates the entire analytical workflow from raw data to a final, comprehensive report object.\n",
        "\n",
        "```python\n",
        "def run_full_study(\n",
        "    asset_returns: pd.DataFrame,\n",
        "    factor_returns: pd.DataFrame,\n",
        "    replication_config: Dict[str, Any],\n",
        "    run_empirical: bool = True,\n",
        "    run_monte_carlo: bool = True,\n",
        "    n_jobs: int = -1,\n",
        "    seed: int = 42\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the randomized alpha test.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `statsmodels`, `matplotlib`, `joblib`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/randomized_test_alpha.git\n",
        "    cd randomized_test_alpha\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy statsmodels matplotlib joblib tqdm\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires three inputs passed to the `run_full_study` function:\n",
        "\n",
        "1.  **`asset_returns`**: A `pandas.DataFrame` where the index is a monthly `DatetimeIndex` and columns are individual asset returns.\n",
        "2.  **`factor_returns`**: A `pandas.DataFrame` with the same monthly `DatetimeIndex` and columns for the factor returns (e.g., 'Mkt-RF', 'SMB', etc.).\n",
        "3.  **`replication_config`**: A nested Python dictionary that controls every aspect of the empirical and Monte Carlo analyses. A fully specified example is provided in the notebook.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `randomized_test_alpha_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your asset and factor returns into DataFrames and define the `replication_config` dictionary.\n",
        "2.  **Execute Pipeline:** Call the master orchestrator function:\n",
        "    ```python\n",
        "    full_study_results = run_full_study(\n",
        "        asset_returns=my_asset_returns_df,\n",
        "        factor_returns=my_factor_returns_df,\n",
        "        replication_config=my_config\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** Programmatically access any result from the returned `full_study_results` dictionary. For example, to view the main empirical summary table:\n",
        "    ```python\n",
        "    # The output is a pandas Styler object, access the data with .data\n",
        "    empirical_table = full_study_results['empirical_summary_table'].data\n",
        "    print(empirical_table)\n",
        "    ```\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `run_full_study` function returns a single, comprehensive dictionary with the following top-level keys:\n",
        "\n",
        "-   `empirical_timeseries_results`: A `pd.DataFrame` with the detailed, window-by-window results of the empirical analysis.\n",
        "-   `empirical_summary_table`: A styled `pd.DataFrame` summarizing the empirical rejection rates.\n",
        "-   `empirical_q_statistic_plot`: A tuple containing the `matplotlib` Figure and Axes objects for the main empirical plot.\n",
        "-   `monte_carlo_raw_results`: A tidy `pd.DataFrame` with the results from every Monte Carlo experimental cell.\n",
        "-   `monte_carlo_summary_tables`: A dictionary of styled `pd.DataFrame`s, one for each simulation scenario.\n",
        "-   `final_quantitative_summary`: A nested dictionary containing a quantitative interpretation of the key findings.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "randomized_test_alpha/\n",
        "│\n",
        "├── randomized_test_alpha_draft.ipynb  # Main implementation notebook\n",
        "├── requirements.txt                     # Python package dependencies\n",
        "├── LICENSE                              # MIT license file\n",
        "└── README.md                            # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the master `replication_config` dictionary. Users can easily modify:\n",
        "-   The `model_specifications` to test different factor models.\n",
        "-   The `rolling_window_size_months` and date ranges for the empirical study.\n",
        "-   The `N_grid`, `T_grid`, `scenarios`, and all DGP parameters for the Monte Carlo simulations.\n",
        "-   The parameters of the test itself, such as `nu` and `tau`.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{massacci2025general,\n",
        "  title={A general randomized test for Alpha},\n",
        "  author={Massacci, Daniele and Sarno, Lucio and Trapani, Lorenzo and Vallarino, Pierluigi},\n",
        "  journal={arXiv preprint arXiv:2507.17599v1},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation of \"A general randomized test for Alpha\".\n",
        "GitHub repository: https://github.com/chirindaopensource/randomized_test_alpha\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to Daniele Massacci, Lucio Sarno, Lorenzo Trapani, and Pierluigi Vallarino for their innovative and rigorous research.\n",
        "-   Thanks to the developers of the scientific Python ecosystem (`numpy`, `pandas`, `scipy`, `statsmodels`, `matplotlib`, `joblib`) that makes this work possible.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `randomized_test_alpha_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "nWmnkZgRMFai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*A general randomized test for Alpha*\"\n",
        "\n",
        "Authors: Daniele Massacci, Lucio Sarno, Lorenzo Trapani, Pierluigi Vallarino\n",
        "\n",
        "E-Journal Submission Date: 23 July 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2507.17599v1\n",
        "\n",
        "Abstract:\n",
        "\n",
        "We propose a methodology to construct tests for the null hypothesis that the pricing errors of a panel of asset returns are jointly equal to zero in a linear factor asset pricing model -- that is, the null of \"zero alpha\". We consider, as a leading example, a model with observable, tradable factors, but we also develop extensions to accommodate for non-tradable and latent factors. The test is based on equation-by-equation estimation, using a randomized version of the estimated alphas, which only requires rates of convergence. The distinct features of the proposed methodology are that it does not require the estimation of any covariance matrix, and that it allows for both N and T to pass to infinity, with the former possibly faster than the latter. Further, unlike extant approaches, the procedure can accommodate conditional heteroskedasticity, non-Gaussianity, and even strong cross-sectional dependence in the error terms. We also propose a de-randomized decision rule to choose in favor or against the correct specification of a linear factor pricing model. Monte Carlo simulations show that the test has satisfactory properties and it compares favorably to several existing tests. The usefulness of the testing procedure is illustrated through an application of linear factor pricing models to price the constituents of the S&P 500."
      ],
      "metadata": {
        "id": "wY7xGtNdj4kH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "The authors propose a new, highly robust statistical test to determine if a set of assets has \"alpha\" – that is, returns that cannot be explained by a given linear factor model. Their method is designed for the modern, high-dimensional world of finance (many assets, `N`, relative to time periods, `T`).\n",
        "\n",
        "The key innovation is a **randomization technique** that circumvents the single biggest hurdle in this area: the need to estimate and invert a massive, often ill-conditioned, `N x N` covariance matrix of asset returns. Their test works well even with non-normal returns, time-varying volatility (like GARCH), and strong cross-asset correlations. They also provide a \"de-randomized\" procedure to ensure results are reproducible, a critical feature for practical application.\n",
        "\n",
        "--\n",
        "\n",
        "### **Step-by-Step Academic Breakdown**\n",
        "\n",
        "#### **Step 1: The Core Problem in Asset Pricing**\n",
        "\n",
        "The fundamental question is whether a linear factor model, like the CAPM or the Fama-French models, can fully explain the expected returns of a large panel of assets. The model is typically specified as:\n",
        "\n",
        "*   **Model:** `y_i,t = α_i + β_i' * f_t + u_i,t`\n",
        "    *   `y_i,t`: Excess return of asset `i` at time `t`.\n",
        "    *   `α_i`: The \"alpha\" or pricing error for asset `i`. This is the part of the return not explained by the factors.\n",
        "    *   `f_t`: A vector of common risk factors (e.g., market return, size, value).\n",
        "    *   `β_i`: The factor loadings (sensitivity) of asset `i` to the factors.\n",
        "    *   `u_i,t`: The idiosyncratic error term.\n",
        "\n",
        "*   **The Null Hypothesis (H₀):** The model is correctly specified. This means all pricing errors are jointly zero: `H₀: α₁ = α₂ = ... = α_N = 0`. Equivalently, `max|α_i| = 0`.\n",
        "\n",
        "*   **The Challenge:** Testing this hypothesis is notoriously difficult. Classic tests like the Gibbons-Ross-Shanken (GRS) test require `N < T` and make strong assumptions (e.g., Gaussian, i.i.d. errors) that are routinely violated by real financial data. More modern tests that allow for large `N` still struggle with estimating the `N x N` error covariance matrix and often impose restrictive assumptions on the dependence structure of the data.\n",
        "\n",
        "#### **Step 2: The Proposed Solution - A Randomized Maximum Test**\n",
        "\n",
        "The authors build their test in a clever, multi-stage process that avoids the covariance matrix problem entirely.\n",
        "\n",
        "1.  **Equation-by-Equation Estimation:** For each of the `N` assets, they run a simple time-series OLS regression to get an estimate of its alpha, `α̂_i`. This is computationally trivial and perfectly parallelizable.\n",
        "\n",
        "2.  **Transformation:** They create a new statistic, `ψ_i,NT`, by scaling the absolute value of the estimated alpha, `|α̂_i|`. This scaling is designed so that:\n",
        "    *   Under the null hypothesis (`α_i = 0`), `ψ_i,NT` converges to zero.\n",
        "    *   Under the alternative hypothesis (`α_i ≠ 0`), `ψ_i,NT` diverges to positive infinity.\n",
        "\n",
        "3.  **The \"Magic\" of Randomization:** They perturb each `ψ_i,NT` by adding an independent, standard normal random shock, `ω_i ~ N(0,1)`. This creates a new set of statistics: `z_i,NT = ψ_i,NT + ω_i`.\n",
        "    *   **Why?** This step is crucial. It effectively \"washes out\" the complex and unknown cross-sectional dependence structure of the original error terms. Under the null hypothesis, since `ψ_i,NT` goes to zero, the `z_i,NT` behave like an i.i.d. sequence of standard normal variables, regardless of the original correlation structure.\n",
        "\n",
        "4.  **The Test Statistic:** The final test statistic, `Z_N,T`, is the **maximum** of these randomized statistics: `Z_N,T = max(z_i,NT)`.\n",
        "\n",
        "#### **Step 3: The Asymptotic Theory**\n",
        "\n",
        "The choice of the maximum statistic leads to a result from **Extreme Value Theory**.\n",
        "\n",
        "*   **Under the Null (H₀):** The distribution of the (appropriately centered and scaled) `Z_N,T` converges to a **Gumbel distribution**. This is a standard, known distribution, so critical values can be easily calculated.\n",
        "*   **Under the Alternative (Hₐ):** At least one `α_i` is non-zero, causing the corresponding `ψ_i,NT` to diverge. This \"spike\" ensures that the maximum statistic `Z_N,T` also diverges to infinity, guaranteeing the test has power to detect mispricing.\n",
        "\n",
        "#### **Step 4: De-Randomization for Practical Use**\n",
        "\n",
        "A clear drawback of the test in Step 2 is that the result depends on the specific random shocks (`ω_i`) drawn. Two researchers with the same data would get different p-values. To solve this, the authors propose a de-randomization procedure:\n",
        "\n",
        "1.  **Replication:** Run the entire test `B` times (e.g., `B = 1000`), each time with a new set of random shocks `ω_i`.\n",
        "2.  **Aggregation:** Calculate the statistic `Q_N,T,B(τ)`, which is simply the *fraction of times* the null hypothesis was *not* rejected at a given significance level `τ` (e.g., 5%).\n",
        "3.  **Decision Rule:**\n",
        "    *   Under H₀, `Q` will converge to `1 - τ` (e.g., 0.95).\n",
        "    *   Under Hₐ, `Q` will converge to 0.\n",
        "    *   The researcher makes a final, deterministic decision by comparing `Q` to a threshold close to `1 - τ`. This result is stable and reproducible.\n",
        "\n",
        "#### **Step 5: Generality and Extensions**\n",
        "\n",
        "The authors demonstrate the flexibility of their framework by extending it beyond the simple case of observable, tradable factors. The core randomization machinery works as long as a consistent estimator for alpha exists. They show it can be adapted to:\n",
        "\n",
        "*   **Non-Tradable Factors:** Using the two-pass Fama-MacBeth estimation procedure.\n",
        "*   **Latent (Unobservable) Factors:** Using Principal Component Analysis (PCA) to first estimate the factors from the asset returns themselves.\n",
        "\n",
        "This significantly broadens the applicability of the test to a wider range of modern asset pricing models.\n",
        "\n",
        "#### **Step 6: Validation and Empirical Findings**\n",
        "\n",
        "*   **Monte Carlo Simulations:** The authors rigorously test their procedure against simulated data with known properties (Gaussian, fat-tailed Student's t, and GARCH errors). Their test consistently shows correct **size** (it doesn't reject the null too often when it's true), a property where many competing tests fail. It also exhibits excellent **power** (it correctly rejects the null when it's false).\n",
        "\n",
        "*   **Empirical Application (S&P 500):** They apply the test to constituents of the S&P 500 from 1985-2024, using 5-year rolling windows.\n",
        "    *   **Key Finding:** No single factor model (from CAPM to the Fama-French 6-factor model) is correctly specified across all time periods.\n",
        "    *   **Crisis Performance:** All models perform particularly poorly during periods of market turmoil (e.g., the 2008 Global Financial Crisis, the COVID-19 pandemic), consistently failing to price assets correctly.\n",
        "    *   **Post-2008:** The performance of all models seems to improve after the GFC, suggesting a possible structural shift in markets.\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "This paper makes a significant methodological contribution to the field of empirical asset pricing. It provides a powerful, robust, and computationally feasible tool for a foundational question in finance. By cleverly using randomization to bypass the estimation of the covariance matrix, the authors have developed a test that is well-suited for the high-dimensional and complex nature of modern financial data. The de-randomization step makes it a practical tool for academic and industry research."
      ],
      "metadata": {
        "id": "l4wmBGK6kp6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "b5Xcvl1HZ_0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  A General Randomized Test for Alpha in High-Dimensional Factor Models\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"A general randomized test for Alpha\"\n",
        "#  by Massacci, Sarno, Trapani, and Vallarino (2025). It delivers a robust\n",
        "#  and computationally sound \"truth detector\" for any proposed linear factor\n",
        "#  model, allowing practitioners and researchers to rigorously validate whether\n",
        "#  a given model fully captures all systematic drivers of return for a large\n",
        "#  universe of assets, even when returns violate classical statistical assumptions.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#   • Test for the joint null hypothesis of zero alphas (pricing errors).\n",
        "#   • A novel randomization procedure that avoids covariance matrix estimation.\n",
        "#   • A de-randomized decision rule for a deterministic, reproducible outcome.\n",
        "#   • Asymptotic theory based on Extreme Value Theory (Gumbel distribution).\n",
        "#   • Robustness to conditional heteroskedasticity, non-Gaussianity, and strong\n",
        "#     cross-sectional dependence in residuals.\n",
        "#   • Applicability to high-dimensional panels where N (assets) can be > T (time).\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#   • End-to-end pipeline from data cleansing to final report generation.\n",
        "#   • High-performance, vectorized OLS estimation for large panels.\n",
        "#   • Rolling-window analysis framework for empirical applications.\n",
        "#   • Comprehensive Monte Carlo simulation engine for evaluating test properties\n",
        "#     (size, power) under various data generating processes (DGP).\n",
        "#   • Modular and extensible design for both empirical and simulation studies.\n",
        "#   • Professional-grade visualization for time-series and cross-sectional results.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Massacci, D., Sarno, L., Trapani, L., & Vallarino, P. (2025). A general\n",
        "#  randomized test for Alpha. arXiv preprint arXiv:2507.17599v1.\n",
        "#  https://arxiv.org/abs/2507.17599v1\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# =============================================================================\n",
        "\n",
        "# =============================================================================\n",
        "# Standard Library Imports\n",
        "# =============================================================================\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "from typing import (\n",
        "    Any,\n",
        "    Callable,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    NamedTuple,\n",
        "    Optional,\n",
        "    Tuple,\n",
        "    Union\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Third-Party Library Imports\n",
        "# =============================================================================\n",
        "\n",
        "# Core numerical and data manipulation libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Statistical modeling and hypothesis testing\n",
        "from scipy.stats import chi2, cauchy, t\n",
        "from statsmodels.stats.proportion import proportion_confint\n",
        "\n",
        "# Parallel processing and progress tracking\n",
        "from joblib import Parallel, delayed\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# =============================================================================\n",
        "#  Logging Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# A single, consistent logging configuration for the entire project.\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# =============================================================================\n",
        "# Matplotlib Style Configuration\n",
        "# =============================================================================\n",
        "\n",
        "# Set a professional and consistent style for all plots.\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n"
      ],
      "metadata": {
        "id": "speqFpLVaCoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "a-7TLorsaGsi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draft 1\n",
        "\n",
        "### Analysis of the Inputs, Processes and Outputs (IPO) of Key Callables\n",
        "\n",
        "### **Analysis of All Final Callables**\n",
        "\n",
        "#### **Task 1: Data and Parameter Validation**\n",
        "\n",
        "*   **Callable:** `validate_inputs`\n",
        "    *   **Inputs:** `asset_returns` (raw `pd.DataFrame`), `factor_returns` (raw `pd.DataFrame`), `replication_config` (`Dict`).\n",
        "    *   **Process:** This function acts as a master gateway. It does not transform data but performs a series of checks by calling its private helpers. It verifies that the DataFrames have the correct structure (`DatetimeIndex`, monthly frequency), are not empty, and contain numeric data. It checks for excessive missing values and flags potential outliers. It recursively validates the structure and key parameter values of the `replication_config` dictionary against a predefined schema.\n",
        "    *   **Outputs:** A boolean `True` upon successful validation of all inputs. Its primary output is a side effect: it raises a specific `TypeError` or `ValueError` if any check fails, halting the pipeline before any computation occurs.\n",
        "    *   **Role in Research Pipeline:** This callable implements the foundational step of **input sanitation and verification**. It ensures that the raw data and parameters are suitable for the analysis, preventing a \"garbage in, garbage out\" scenario and ensuring the preconditions for the subsequent econometric procedures are met.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 2: Data Cleansing**\n",
        "\n",
        "*   **Callable:** `clean_and_align_data`\n",
        "    *   **Inputs:** `asset_returns` (raw `pd.DataFrame`), `factor_returns` (raw `pd.DataFrame`), and several scalar configuration parameters (`min_obs_frac_col`, `winsorize_quantiles`, etc.).\n",
        "    *   **Process:** This function executes a sequential data transformation pipeline.\n",
        "        1.  **Missing Value Handling:** It first filters assets and time periods with excessive `NaN`s. It then applies a constrained forward-fill to impute small, consecutive gaps in the asset returns. Finally, it prunes any assets that still contain `NaN`s.\n",
        "        2.  **Outlier Treatment:** It performs cross-sectional Winsorization on the imputed asset returns. For each time period, it calculates specified quantiles (e.g., 1st and 99th) and caps all returns in that period to fall within this range.\n",
        "        3.  **Alignment:** It determines the common `DatetimeIndex` between the treated asset returns and the factor returns. It slices both DataFrames to this common index and performs a final filter on the assets to ensure each has a sufficient number of observations for the subsequent rolling window analysis.\n",
        "    *   **Outputs:** A tuple containing:\n",
        "        1.  `final_assets` (`pd.DataFrame`): The cleansed, treated, and aligned asset returns.\n",
        "        2.  `aligned_factors` (`pd.DataFrame`): The aligned factor returns.\n",
        "        3.  `full_log` (`Dict`): A detailed dictionary auditing every action taken (assets dropped, values imputed, outliers winsorized).\n",
        "    *   **Role in Research Pipeline:** This callable implements the crucial **data preparation** step. High-quality empirical research requires clean, well-behaved data. This function ensures the dataset is dense, free of extreme outliers that could bias OLS, and perfectly aligned, making it ready for the econometric analysis.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 3: Empirical Analysis Setup**\n",
        "\n",
        "*   **Callable:** `setup_empirical_analysis_windows`\n",
        "    *   **Inputs:** `aligned_data_index` (`pd.DatetimeIndex`), `replication_config` (`Dict`).\n",
        "    *   **Process:** The function parses the `empirical:run_controls` section of the configuration to get the `window_size`, `start_date`, and `end_date`. It determines the *effective* date range by finding the intersection of the configured range and the actual range of the provided data index. It then programmatically generates a list of all possible full-sized rolling windows within this effective range. Each window is represented as a `(start_date, end_date)` tuple.\n",
        "    *   **Outputs:** A `List[Tuple[pd.Timestamp, pd.Timestamp]]` representing the complete schedule for the rolling window analysis.\n",
        "    *   **Role in Research Pipeline:** This callable implements the **experimental design** for the empirical study. It translates the high-level parameters of the study into a concrete, iterable plan of execution for the main analysis orchestrator.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 4: Model Estimation**\n",
        "\n",
        "*   **Callable:** `estimate_factor_model_params`\n",
        "    *   **Inputs:** `asset_returns_window` (`pd.DataFrame`), `factor_returns_window` (`pd.DataFrame`).\n",
        "    *   **Process:** This function performs a vectorized OLS estimation for all assets in the provided window simultaneously. It constructs the regressor matrix `X` (factors plus an intercept) and the dependent variable matrix `Y` (asset returns). It then solves the system of normal equations `(X'X)B = X'Y` for the `(K+1) x N` coefficient matrix `B` using the numerically stable `np.linalg.solve`. From `B`, it extracts the vector of alphas and the matrix of betas. Finally, it calculates the `T x N` matrix of residuals `U = Y - XB`.\n",
        "    *   **Outputs:** A `FactorModelEstimationResult` `NamedTuple` containing the estimated `alphas` (`pd.Series`), `betas` (`pd.DataFrame`), `residuals` (`pd.DataFrame`), and the `condition_number` of the regressor matrix.\n",
        "    *   **Role in Research Pipeline:** This callable is the workhorse of the empirical analysis, implementing the **parameter estimation** step. It directly estimates the key components of the linear factor model for each asset `i`:\n",
        "        $$ y_{i,t} = \\alpha_i + \\beta'_i f_t + u_{i,t} $$\n",
        "        The estimated `α̂_i` and `û_{i,t}` are the critical inputs for the subsequent test statistic calculation.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 5: Test Statistic Computation**\n",
        "\n",
        "*   **Callable:** `compute_psi_statistic`\n",
        "    *   **Inputs:** `estimation_result` (`FactorModelEstimationResult`), `nu` (`float`).\n",
        "    *   **Process:** The function first calls its helper, `_calculate_scaling_factor`, which computes the scaling factor `s_NT` from the input residuals. It then applies the main non-linear transformation to the estimated alphas.\n",
        "    *   **Outputs:** A `pd.Series` of the `ψ_i,NT` statistics, indexed by asset.\n",
        "    *   **Role in Research Pipeline:** This callable implements the **core transformation** of the test statistic. It takes the raw alpha estimates and converts them into a normalized, unit-free quantity that has the desired asymptotic properties under the null and alternative hypotheses. It implements:\n",
        "        *   **Equation (3.3):** $$ \\hat{s}_{NT} = \\sqrt{\\frac{1}{NT} \\sum_{i=1}^{N} \\sum_{t=1}^{T} \\hat{u}_{i,t}^2} $$\n",
        "        *   **Equation (3.2):** $$ \\psi_{i,NT} = \\left| \\frac{T^{1/\\nu} \\hat{\\alpha}_{i,T}}{\\hat{s}_{NT}} \\right|^{\\nu/2} $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 6: Randomization and Test Statistic**\n",
        "\n",
        "*   **Callable:** `compute_randomized_test_statistic`\n",
        "    *   **Inputs:** `psi_values` (`pd.Series`), `rng` (`np.random.Generator`).\n",
        "    *   **Process:** The function first generates `N` i.i.d. standard normal shocks, `ω_i`. It then adds these shocks element-wise to the input `ψ_i,NT` values to create the perturbed statistics `z_i,NT`. Finally, it finds the maximum value of this new series.\n",
        "    *   **Outputs:** A single scalar `float`, `Z_N,T`.\n",
        "    *   **Role in Research Pipeline:** This callable implements the **randomization procedure**, which is the central innovation of the paper. It transforms the deterministic (but with unknown distribution) `ψ` statistics into a new statistic `Z_N,T` whose asymptotic distribution under the null is a known Gumbel distribution. It implements:\n",
        "        *   $$ z_{i,NT} = \\psi_{i,NT} + \\omega_i, \\quad \\text{where } \\omega_i \\sim \\mathcal{N}(0, 1) $$\n",
        "        *   $$ Z_{N,T} = \\max_{1 \\le i \\le N} z_{i,NT} $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 7: De-randomization**\n",
        "\n",
        "*   **Callable:** `perform_de_randomization`\n",
        "    *   **Inputs:** `psi_values` (`pd.Series`), `tau` (`float`), `b_function` (`Callable`), `rng` (`np.random.Generator`).\n",
        "    *   **Process:** The function first computes the asymptotic Gumbel critical value, `c_τ`. It then enters a loop that runs `B` times (where `B` is determined by `b_function`). In each iteration, it calls `compute_randomized_test_statistic` to get one realization of `Z_N,T`. After the loop, it calculates the proportion of these realizations that were less than or equal to `c_τ`.\n",
        "    *   **Outputs:** A `DeRandomizationResult` `NamedTuple` containing the final `q_statistic`, the `critical_value` used, the number of `b_replications`, and `tau`.\n",
        "    *   **Role in Research Pipeline:** This callable implements the **de-randomization procedure** from Section 3.1. It removes the dependence of the final p-value on a single random draw by integrating over the distribution of the randomized statistic. It produces the key decision variable, `Q_N,T,B(τ)`. It implements:\n",
        "        *   **Equation (3.6):** $$ c_{\\tau} = b_N - a_N \\log(-\\log(1-\\tau)) $$\n",
        "        *   **Equation (3.7):** $$ Q_{N,T,B}(\\tau) = B^{-1} \\sum_{b=1}^{B} I(Z_{N,T}^{(b)} \\le c_{\\tau}) $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 8: Decision Rule**\n",
        "\n",
        "*   **Callable:** `make_test_decision`\n",
        "    *   **Inputs:** `de_randomization_result` (`DeRandomizationResult`), `f_b_function` (`Callable`).\n",
        "    *   **Process:** The function calculates the decision threshold by evaluating `(1 - tau) - f(B)`. It then compares the input `q_statistic` to this threshold.\n",
        "    *   **Outputs:** A `TestDecisionResult` `NamedTuple` containing the boolean decision (`fail_to_reject_h0`), a human-readable `outcome` string, and the inputs to the decision for auditability.\n",
        "    *   **Role in Research Pipeline:** This callable implements the final **hypothesis testing decision**. It applies the practical decision rule proposed in the paper to make a deterministic choice between the null and alternative hypotheses. It implements:\n",
        "        *   **Decision Rule from Equation (3.12):** Fail to Reject H₀ if $$ Q_{N,T,B}(\\tau) \\ge (1 - \\tau) - f(B) $$\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 9: Create Orchestrator Function**\n",
        "\n",
        "*   **Callable:** `EmpiricalAnalysisOrchestrator`\n",
        "    *   **Inputs:** `asset_returns` (`pd.DataFrame`), `factor_returns` (`pd.DataFrame`), `replication_config` (`Dict`), `seed` (`int`).\n",
        "    *   **Process:** This class acts as the master controller for the entire empirical study. Its `__init__` method orchestrates the calls to the validation, cleaning, and setup functions. Its `run_analysis` method implements the nested loops over models and rolling windows, calling the full testing pipeline (Tasks 4-8) for each combination and collecting the results.\n",
        "    *   **Outputs:** The `run_analysis` method returns a single, tidy, multi-indexed `pd.DataFrame` containing the detailed results of every test performed.\n",
        "    *   **Role in Research Pipeline:** This is the **empirical study orchestrator**. It automates the entire process of applying the test to real data, managing the complexity of the rolling window analysis and the comparison of multiple factor models.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 17 (Revised): Monte Carlo Robustness Analysis**\n",
        "\n",
        "*   **Callable:** `analyze_power_by_sparsity`\n",
        "    *   **Inputs:** `base_experiment` (`SimulationExperiment`), `n_jobs` (`int`).\n",
        "    *   **Process:** The function iterates through a list of sparsity levels defined in the configuration. For each level, it modifies a copy of the base experiment's configuration to set the new sparsity, then calls the full `run_monte_carlo_simulation` engine (Task 15) for the alternative hypothesis. It collects the resulting power and confidence interval for each level.\n",
        "    *   **Outputs:** A `pd.DataFrame` indexed by sparsity level, containing the empirical power and its confidence interval, suitable for plotting a power curve.\n",
        "    *   **Role in Research Pipeline:** This callable performs a **robustness check on the test's power**. It systematically investigates how the test's ability to detect false nulls changes as the signal (the pervasiveness of mispricing) weakens.\n",
        "\n",
        "*   **Callable:** `run_comparison_simulation`\n",
        "    *   **Inputs:** `experiment` (`SimulationExperiment`), `n_jobs` (`int`), `seed` (`int`).\n",
        "    *   **Process:** This function orchestrates a simulation for a single experimental cell, but for multiple statistical tests. Its helper, `_run_single_replication_for_comparison`, generates one dataset and then applies \"OurTest\", the \"FLY\" test, and the \"AS\" test to that same dataset, returning a dictionary of their decisions. The main function runs this `M` times (in parallel) for both the null and alternative hypotheses and aggregates the rejection rates for each test.\n",
        "    *   **Outputs:** A `pd.DataFrame` with rows 'size' and 'power' and columns for each test ('OurTest', 'FLY', 'AS'), allowing for direct comparison of their statistical properties.\n",
        "    *   **Role in Research Pipeline:** This callable performs a **comparative performance analysis**. It benchmarks the paper's proposed test against other methods from the literature under controlled, simulated conditions, which is a cornerstone of methodological research.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 18: Results Compilation and Visualization (Monte Carlo)**\n",
        "\n",
        "*   **Callable:** `create_mc_results_table`\n",
        "    *   **Inputs:** `mc_results_df` (`pd.DataFrame`), `scenario_filter` (`str`).\n",
        "    *   **Process:** The function takes the long-format \"tidy\" data from the Monte Carlo orchestrator. It filters for a specific scenario, then pivots the data twice: once for size and once for power, creating tables with `N` as the index and `T` as the columns. These two tables are then concatenated side-by-side and styled for publication.\n",
        "    *   **Outputs:** A `pandas.Styler` object representing a formatted, publication-quality table.\n",
        "    *   **Role in Research Pipeline:** This is a **reporting and summarization** tool. It transforms the raw simulation output into the standard tabular format used in econometrics papers (like Tables 5.1-5.6) for easy interpretation of size and power across different panel dimensions.\n",
        "\n",
        "*   **Callable:** `plot_power_curves`\n",
        "    *   **Inputs:** `power_curve_data` (`Dict[str, pd.DataFrame]`), `title` (`str`), `figsize` (`Tuple`).\n",
        "    *   **Process:** The function takes one or more power curve DataFrames (as generated by `analyze_power_by_sparsity`). It plots the empirical power against the sparsity level for each test, adding a shaded confidence interval around the primary test's curve.\n",
        "    *   **Outputs:** `matplotlib.figure.Figure` and `matplotlib.axes.Axes` objects.\n",
        "    *   **Role in Research Pipeline:** This is a **visualization and reporting** tool. It creates a plot analogous to Figure 5.1, providing a clear visual representation of the test's power and how it compares to other methods as the alternative hypothesis changes.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 19: Interpretation and Discussion**\n",
        "\n",
        "*   **Callable:** `generate_results_summary`\n",
        "    *   **Inputs:** `empirical_results` (`pd.DataFrame`), `mc_results` (`pd.DataFrame`), `replication_config` (`Dict`).\n",
        "    *   **Process:** This function acts as a final analytical layer. It takes the summary tables and raw results from the previous stages and performs a series of quantitative comparisons. It ranks models, identifies best/worst performers in different regimes, programmatically checks for size distortions in the simulations, and identifies areas of low power.\n",
        "    *   **Outputs:** A nested `Dict` containing a structured, quantitative summary of the key findings from the entire study.\n",
        "    *   **Role in Research Pipeline:** This callable automates the **first-pass interpretation of results**. It distills the vast amount of numerical output from the study into a set of key, data-driven statements, guiding the researcher's attention to the most significant findings and ensuring a consistent, objective initial analysis.\n",
        "\n",
        "--\n",
        "\n",
        "#### **Task 20: Master Orchestrator**\n",
        "\n",
        "*   **Callable:** `run_full_study`\n",
        "    *   **Inputs:** `asset_returns` (`pd.DataFrame`), `factor_returns` (`pd.DataFrame`), `replication_config` (`Dict`), and control flags.\n",
        "    *   **Process:** This is the top-level function. It sequentially calls the main orchestrators and compilation functions: `EmpiricalAnalysisOrchestrator`, `compile_empirical_results`, `plot_q_statistic_time_series`, `MonteCarloOrchestrator`, `create_mc_results_table`, and `generate_results_summary`. It manages the entire workflow from start to finish, with robust error handling for each major stage.\n",
        "    *   **Outputs:** A single, comprehensive `Dict` where keys are descriptive names (e.g., `'empirical_summary_table'`) and values are the major artifacts produced during the study (DataFrames, plots, summary dicts).\n",
        "    *   **Role in Research Pipeline:** This is the **master controller or main entry point**. It encapsulates the entire research project into a single, reproducible function call, ensuring that the study can be re-run from raw data to final interpretation with a single command.\n",
        "\n",
        "\n",
        "### Usage Example\n",
        "\n",
        "### **Example Usage: End-to-End Alpha Test Pipeline**\n",
        "\n",
        "This example demonstrates how a researcher would use the complete software suite to replicate the study from \"A general randomized test for Alpha\". It covers the setup of input data, the definition of the configuration, the execution of the main pipeline, and the interpretation of the structured output.\n",
        "\n",
        "#### **Step 1: Prepare Input Data**\n",
        "\n",
        "The first step is to load and prepare the two required datasets: asset returns and factor returns. For this example, we will generate synthetic data that mimics the structure of real-world financial data.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --- 1a: Generate Synthetic Asset Returns ---\n",
        "# In a real application, this data would be loaded from a source like CRSP or Bloomberg.\n",
        "# We simulate a panel of 500 assets over 40 years (480 months).\n",
        "N_assets = 500\n",
        "T_periods = 480\n",
        "# Create a realistic monthly DatetimeIndex ending in the present.\n",
        "dates = pd.to_datetime(pd.date_range(end='2024-12-31', periods=T_periods, freq='M'))\n",
        "# Generate random return data (e.g., from a normal distribution).\n",
        "asset_data = np.random.normal(loc=0.008, scale=0.05, size=(T_periods, N_assets))\n",
        "# Create the DataFrame with appropriate column names.\n",
        "asset_returns_df = pd.DataFrame(asset_data, index=dates, columns=[f'ASSET_{i+1}' for i in range(N_assets)])\n",
        "# Introduce some missing data to simulate real-world conditions.\n",
        "# This will be handled by our cleansing pipeline.\n",
        "for col in asset_returns_df.columns[:50]:\n",
        "    missing_indices = np.random.choice(T_periods, size=int(T_periods * 0.1), replace=False)\n",
        "    asset_returns_df.iloc[missing_indices, asset_returns_df.columns.get_loc(col)] = np.nan\n",
        "\n",
        "print(\"--- Sample of Asset Returns DataFrame ---\")\n",
        "print(asset_returns_df.head())\n",
        "print(\"\\n\")\n",
        "\n",
        "# --- 1b: Generate Synthetic Factor Returns ---\n",
        "# In a real application, this would be downloaded from the Kenneth French Data Library.\n",
        "# We need the 5 Fama-French factors plus the Momentum factor.\n",
        "factor_names = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'MOM']\n",
        "# Generate random factor data.\n",
        "factor_data = np.random.normal(loc=0.005, scale=0.03, size=(T_periods, len(factor_names)))\n",
        "# Create the DataFrame.\n",
        "factor_returns_df = pd.DataFrame(factor_data, index=dates, columns=factor_names)\n",
        "\n",
        "print(\"--- Sample of Factor Returns DataFrame ---\")\n",
        "print(factor_returns_df.head())\n",
        "print(\"\\n\")\n",
        "```\n",
        "\n",
        "#### **Step 2: Define the Study Configuration**\n",
        "\n",
        "The second step is to define the `replication_config` dictionary. This object controls every aspect of the study. We will use the default configuration provided in the original prompt, which is a complete specification for replicating the paper's empirical and simulation studies.\n",
        "\n",
        "```python\n",
        "# This single object contains all parameters for both the empirical analysis\n",
        "# and the Monte Carlo simulations.\n",
        "\n",
        "replication_config = {\n",
        "    # =========================================================================\n",
        "    # PART I: MONTE CARLO SIMULATION PARAMETERS (SECTION 5 & APPENDIX A)\n",
        "    # =========================================================================\n",
        "    'monte_carlo': {\n",
        "        'run_controls': {\n",
        "            'M_replications': 100, # Reduced for a quick example run\n",
        "            'N_grid': [100, 200],   # Reduced for a quick example run\n",
        "            'T_grid': [100, 200],   # Reduced for a quick example run\n",
        "            'scenarios': ['main', 'no_persistence'] # Reduced for a quick example\n",
        "        },\n",
        "        'dgp_base_params': {\n",
        "            'K_factors': 3,\n",
        "            'factor_mean': [0.53, 0.19, 0.19],\n",
        "            'factor_var_matrix': {'type': 'diag', 'values': [-0.1, 0.2, -0.2]},\n",
        "            'omitted_factor_loading_dist': {'type': 'uniform', 'args': (0.7, 0.9)},\n",
        "            'factor_loading_dist': [\n",
        "                {'type': 'uniform', 'args': (0.3, 1.8)},\n",
        "                {'type': 'uniform', 'args': (-1.0, 1.0)},\n",
        "                {'type': 'uniform', 'args': (-0.6, 0.9)},\n",
        "            ],\n",
        "            'error_dgp_types': {\n",
        "                'gaussian': {'type': 'normal', 'args': (0, 1)}\n",
        "            }\n",
        "        },\n",
        "        'hypothesis_params': {\n",
        "            'null': {'type': 'zero'},\n",
        "            'alternative': {\n",
        "                'type': 'sparse_normal',\n",
        "                'sparsity_levels': [0.05, 0.10, 0.15], # Reduced for example\n",
        "                'default_sparsity': 0.05,\n",
        "                'dist_args': (0, 1)\n",
        "            }\n",
        "        },\n",
        "        'test_config': {\n",
        "            'our_test': {\n",
        "                'nu': 5.0,\n",
        "                'tau': 0.05,\n",
        "                'de_randomization': {\n",
        "                    'B_function': lambda n: int(np.floor(np.log(n)**2)),\n",
        "                    'f_B_function': lambda b: b**(-0.25)\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    # =======================================================================\n",
        "    # PART II: EMPIRICAL APPLICATION PARAMETERS (SECTION 6)             \n",
        "    # =======================================================================\n",
        "    'empirical': {\n",
        "        'run_controls': {\n",
        "            'rolling_window_size_months': 60,\n",
        "            'start_date': '1985-01-01',\n",
        "            'end_date': '2024-12-31',\n",
        "        },\n",
        "        'model_specifications': {\n",
        "            'CAPM': ['Mkt-RF'],\n",
        "            'FF3':  ['Mkt-RF', 'SMB', 'HML'],\n",
        "            'FF5':  ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA'],\n",
        "            'FF6':  ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA', 'MOM']\n",
        "        },\n",
        "        'test_config': {\n",
        "            'our_test': {\n",
        "                'nu': 4.0,\n",
        "                'tau': 0.05,\n",
        "                'de_randomization': {\n",
        "                    'B_function': lambda n: int(np.floor(np.log(n)**2)),\n",
        "                    'f_B_function': lambda b: b**(-0.25)\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'reporting': {\n",
        "            'crisis_periods': {\n",
        "                'Global_Financial_Crisis':('2007-12-01', '2009-06-30'),\n",
        "                'COVID_19_Pandemic':      ('2020-01-01', '2021-05-31')\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"--- Replication Configuration Defined ---\")\n",
        "print(\"The study is now fully configured and ready to run.\")\n",
        "print(\"\\n\")\n",
        "```\n",
        "\n",
        "#### **Step 3: Execute the End-to-End Pipeline**\n",
        "\n",
        "With the data and configuration prepared, we execute the entire study with a single call to the `run_full_study` function. We can use the control flags to run only the empirical part for this example to ensure a timely result.\n",
        "\n",
        "```python\n",
        "# Assume all the previously defined functions (run_full_study, etc.) are imported\n",
        "# from the project's modules.\n",
        "\n",
        "# Execute the full study. For this example, we will only run the empirical\n",
        "# part to get a result quickly. Setting `run_monte_carlo=False` skips the\n",
        "# most time-consuming part.\n",
        "# `n_jobs=-1` would use all available CPU cores for parallelizable tasks.\n",
        "# `seed=42` ensures the entire study is reproducible.\n",
        "\n",
        "full_study_results = run_full_study(\n",
        "    asset_returns=asset_returns_df,\n",
        "    factor_returns=factor_returns_df,\n",
        "    replication_config=replication_config,\n",
        "    run_empirical=True,\n",
        "    run_monte_carlo=False, # Set to False for a quick demonstration\n",
        "    n_jobs=-1,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "print(\"--- End-to-End Pipeline Execution Complete ---\")\n",
        "print(\"The 'full_study_results' dictionary now contains all artifacts of the study.\")\n",
        "print(\"\\n\")\n",
        "```\n",
        "\n",
        "#### **Step 4: Inspect and Use the Outputs**\n",
        "\n",
        "The `run_full_study` function returns a dictionary containing all the major outputs. We can now easily access and analyze these artifacts.\n",
        "\n",
        "```python\n",
        "# --- 4a: Inspect the Empirical Results Summary Table ---\n",
        "# The summary table is a styled pandas DataFrame, ideal for display in notebooks.\n",
        "print(\"--- Empirical Results Summary Table ---\")\n",
        "# In a Jupyter environment, this would render a beautifully formatted table.\n",
        "# In a script, we access the underlying data.\n",
        "empirical_summary_table = full_study_results['empirical_summary_table']\n",
        "display(empirical_summary_table) # Use display() in a notebook\n",
        "# print(empirical_summary_table.data) # Or print the raw data in a script\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- 4b: Inspect the Time Series Plot ---\n",
        "# The output dictionary contains the matplotlib figure and axes objects.\n",
        "print(\"--- Visualizing the Q-Statistic Time Series ---\")\n",
        "fig, ax = full_study_results['empirical_q_statistic_plot']\n",
        "# This would display the plot in a Jupyter notebook or an interactive session.\n",
        "plt.show()\n",
        "# You can also save the figure to a file for a publication.\n",
        "# fig.savefig(\"q_statistic_time_series.pdf\", bbox_inches='tight')\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# --- 4c: Inspect the Quantitative Interpretation ---\n",
        "# The final summary provides a quantitative, text-based analysis.\n",
        "print(\"--- Quantitative Summary of Findings ---\")\n",
        "final_summary = full_study_results['final_quantitative_summary']\n",
        "\n",
        "# Print the empirical findings.\n",
        "print(\"Empirical Findings:\")\n",
        "empirical_findings = final_summary['empirical_findings']\n",
        "print(f\"  - Best Overall Model: {empirical_findings['full_sample']['best_performing_model']}\")\n",
        "print(f\"  - Worst Overall Model: {empirical_findings['full_sample']['worst_performing_model']}\")\n",
        "for period, finding in empirical_findings['crisis_analysis'].items():\n",
        "    if isinstance(finding, str):\n",
        "        print(f\"  - {period.replace('_', ' ')}: {finding}\")\n",
        "\n",
        "# If we had run the Monte Carlo part, we could inspect those results too:\n",
        "# if 'monte_carlo_findings' in final_summary:\n",
        "#     mc_findings = final_summary['monte_carlo_findings']\n",
        "#     print(\"\\nMonte Carlo Findings:\")\n",
        "#     print(f\"  - Size Control: {mc_findings['size_control']['summary']}\")\n",
        "#     print(f\"  - Average Power: {mc_findings['power_analysis']['average_power']}\")\n",
        "```\n",
        "\n",
        "This example provides a complete walkthrough of how to use the professional-grade pipeline. It demonstrates the simplicity of the final user interface (`run_full_study`), which abstracts away the immense complexity of the underlying econometric and computational machinery, delivering a robust, reproducible, and insightful analysis with a single function call.\n",
        "\n"
      ],
      "metadata": {
        "id": "dubCfB_iaIRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Data and Parameter Validation\n",
        "\n",
        "def _validate_dataframe_structure(\n",
        "    df: pd.DataFrame,\n",
        "    df_name: str,\n",
        "    min_numeric_cols: int = 1\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the fundamental structure of a pandas DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to validate.\n",
        "        df_name (str): The name of the DataFrame for logging and error messages.\n",
        "        min_numeric_cols (int): The minimum number of numeric columns required.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the input is not a pandas DataFrame.\n",
        "        ValueError: If the DataFrame is empty, lacks a DatetimeIndex, has an\n",
        "                    incorrect frequency, or contains insufficient numeric data.\n",
        "    \"\"\"\n",
        "    # Check if the input is a pandas DataFrame.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise TypeError(f\"Input '{df_name}' must be a pandas DataFrame, but got {type(df)}.\")\n",
        "\n",
        "    # Check if the DataFrame is empty.\n",
        "    if df.empty:\n",
        "        raise ValueError(f\"Input DataFrame '{df_name}' cannot be empty.\")\n",
        "\n",
        "    # Validate that the index is a DatetimeIndex.\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(f\"Index of '{df_name}' must be a DatetimeIndex.\")\n",
        "\n",
        "    # Infer the frequency of the index.\n",
        "    freq = pd.infer_freq(df.index)\n",
        "    # Check if the frequency is monthly (Month-End).\n",
        "    if freq not in ['M', 'ME']:\n",
        "        logging.warning(\n",
        "            f\"Could not infer monthly frequency for '{df_name}'. \"\n",
        "            f\"Inferred frequency: {freq}. Proceeding with caution.\"\n",
        "        )\n",
        "\n",
        "    # Ensure the DataFrame contains a sufficient number of numeric columns.\n",
        "    numeric_cols = df.select_dtypes(include=np.number).columns\n",
        "    if len(numeric_cols) < min_numeric_cols:\n",
        "        raise ValueError(\n",
        "            f\"'{df_name}' must contain at least {min_numeric_cols} numeric \"\n",
        "            f\"column(s), but found {len(numeric_cols)}.\"\n",
        "        )\n",
        "\n",
        "    logging.info(f\"Basic structure validation passed for '{df_name}'.\")\n",
        "\n",
        "\n",
        "def _validate_returns_data(\n",
        "    df: pd.DataFrame,\n",
        "    df_name: str,\n",
        "    max_nan_col_frac: float = 0.2,\n",
        "    max_nan_total_frac: float = 0.5,\n",
        "    outlier_z_threshold: float = 5.0\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs specific validation for a returns DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The returns DataFrame to validate.\n",
        "        df_name (str): The name of the DataFrame for logging.\n",
        "        max_nan_col_frac (float): Maximum allowable fraction of NaNs per column.\n",
        "        max_nan_total_frac (float): Maximum allowable fraction of NaNs in total.\n",
        "        outlier_z_threshold (float): Z-score threshold to flag potential outliers.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If NaN or outlier checks fail.\n",
        "    \"\"\"\n",
        "    # Calculate the total fraction of missing values.\n",
        "    total_nan_frac = df.isna().sum().sum() / df.size\n",
        "    # Check if the total missing value fraction exceeds the threshold.\n",
        "    if total_nan_frac > max_nan_total_frac:\n",
        "        raise ValueError(\n",
        "            f\"Total fraction of missing values in '{df_name}' is \"\n",
        "            f\"{total_nan_frac:.2%}, which exceeds the threshold of {max_nan_total_frac:.2%}.\"\n",
        "        )\n",
        "\n",
        "    # Calculate the fraction of missing values for each column.\n",
        "    nan_col_frac = df.isna().mean()\n",
        "    # Check if any column's missing value fraction exceeds the threshold.\n",
        "    if (nan_col_frac > max_nan_col_frac).any():\n",
        "        problematic_cols = nan_col_frac[nan_col_frac > max_nan_col_frac].index.tolist()\n",
        "        raise ValueError(\n",
        "            f\"Columns in '{df_name}' exceed the NaN threshold of {max_nan_col_frac:.2%}: \"\n",
        "            f\"{problematic_cols}\"\n",
        "        )\n",
        "\n",
        "    # Perform a preliminary check for extreme outliers using Z-scores.\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    # Calculate Z-scores, handling potential division by zero for constant columns.\n",
        "    z_scores = np.abs(\n",
        "        (numeric_df - numeric_df.mean()) / numeric_df.std().replace(0, np.nan)\n",
        "    )\n",
        "    # Count the number of outliers.\n",
        "    outlier_count = (z_scores > outlier_z_threshold).sum().sum()\n",
        "    # Issue a warning if a significant number of outliers are detected.\n",
        "    if outlier_count > 0:\n",
        "        logging.warning(\n",
        "            f\"Detected {outlier_count} potential outliers in '{df_name}' \"\n",
        "            f\"(Z-score > {outlier_z_threshold}). Consider reviewing the data.\"\n",
        "        )\n",
        "\n",
        "    logging.info(f\"Returns-specific data validation passed for '{df_name}'.\")\n",
        "\n",
        "\n",
        "def _validate_factor_data(\n",
        "    factor_df: pd.DataFrame,\n",
        "    required_cols: List[str]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs specific validation for a factor returns DataFrame.\n",
        "\n",
        "    Args:\n",
        "        factor_df (pd.DataFrame): The factor DataFrame to validate.\n",
        "        required_cols (List[str]): A list of column names required to be in the DataFrame.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If required columns are missing or if factors are near-constant.\n",
        "    \"\"\"\n",
        "    # Check for the presence of all required factor columns.\n",
        "    missing_cols = set(required_cols) - set(factor_df.columns)\n",
        "    if missing_cols:\n",
        "        raise ValueError(\n",
        "            f\"Factor DataFrame is missing required columns: {sorted(list(missing_cols))}\"\n",
        "        )\n",
        "\n",
        "    # Check for near-constant factors, which can cause issues in regression.\n",
        "    factor_variances = factor_df[required_cols].var()\n",
        "    # Identify columns with variance below a small threshold.\n",
        "    near_constant_factors = factor_variances[factor_variances < 1e-12].index.tolist()\n",
        "    if near_constant_factors:\n",
        "        raise ValueError(\n",
        "            f\"The following factors are near-constant (variance < 1e-12) and may \"\n",
        "            f\"cause numerical instability: {near_constant_factors}\"\n",
        "        )\n",
        "\n",
        "    logging.info(\"Factor data validation passed.\")\n",
        "\n",
        "\n",
        "def _validate_config_dict(\n",
        "    config: Dict[str, Any],\n",
        "    schema: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Recursively validates a nested dictionary against a schema.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The configuration dictionary to validate.\n",
        "        schema (Dict[str, Any]): The schema to validate against. The schema uses\n",
        "                                 types or nested dictionaries to define the expected structure.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If the config is not a dictionary or if a value has an incorrect type.\n",
        "        ValueError: If a required key is missing or a parameter value is out of bounds.\n",
        "    \"\"\"\n",
        "    # Check if the config object is a dictionary.\n",
        "    if not isinstance(config, dict):\n",
        "        raise TypeError(f\"Configuration must be a dict, but got {type(config)}.\")\n",
        "\n",
        "    # Iterate through the keys and values of the schema.\n",
        "    for key, expected_type in schema.items():\n",
        "        # Check if a required key is missing from the config.\n",
        "        if key not in config:\n",
        "            raise ValueError(f\"Missing required configuration key: '{key}'.\")\n",
        "\n",
        "        # Get the actual value from the config.\n",
        "        actual_value = config[key]\n",
        "\n",
        "        # If the expected type is a nested dictionary, recurse.\n",
        "        if isinstance(expected_type, dict):\n",
        "            _validate_config_dict(actual_value, expected_type)\n",
        "        # If the expected type is a list, validate the type of its elements.\n",
        "        elif isinstance(expected_type, list):\n",
        "            if not isinstance(actual_value, list):\n",
        "                raise TypeError(f\"Expected list for key '{key}', but got {type(actual_value)}.\")\n",
        "            # This is a simplified check; a more robust implementation might check each element.\n",
        "        # If the expected type is a callable (e.g., a lambda function).\n",
        "        elif expected_type is Callable:\n",
        "            if not callable(actual_value):\n",
        "                raise TypeError(f\"Expected a callable function for key '{key}', but got {type(actual_value)}.\")\n",
        "        # Otherwise, check the type directly.\n",
        "        elif not isinstance(actual_value, expected_type):\n",
        "            raise TypeError(\n",
        "                f\"Invalid type for key '{key}'. Expected {expected_type}, but got {type(actual_value)}.\"\n",
        "            )\n",
        "\n",
        "    # Perform specific value-based checks after structural validation.\n",
        "    # This part can be expanded based on the full schema.\n",
        "    if 'empirical' in config and 'test_config' in config['empirical']:\n",
        "        nu = config['empirical']['test_config']['our_test']['nu']\n",
        "        if not (isinstance(nu, (int, float)) and nu > 4):\n",
        "            raise ValueError(f\"Parameter 'nu' must be a number > 4, but got {nu}.\")\n",
        "\n",
        "        tau = config['empirical']['test_config']['our_test']['tau']\n",
        "        if not (isinstance(tau, float) and 0 < tau < 1):\n",
        "            raise ValueError(f\"Parameter 'tau' must be a float between 0 and 1, but got {tau}.\")\n",
        "\n",
        "    logging.info(\"Configuration dictionary validation passed.\")\n",
        "\n",
        "\n",
        "def validate_inputs(\n",
        "    asset_returns: pd.DataFrame,\n",
        "    factor_returns: pd.DataFrame,\n",
        "    replication_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of all inputs for the Alpha Test Framework.\n",
        "\n",
        "    This function serves as the main entry point for input validation. It checks\n",
        "    the structural and statistical integrity of the asset and factor returns\n",
        "    DataFrames, and validates the structure and parameter values of the\n",
        "    configuration dictionary.\n",
        "\n",
        "    Args:\n",
        "        asset_returns (pd.DataFrame): A datetime-indexed DataFrame of monthly\n",
        "                                      asset returns. Columns are asset tickers.\n",
        "        factor_returns (pd.DataFrame): A datetime-indexed DataFrame of monthly\n",
        "                                       factor returns (e.g., Fama-French).\n",
        "        replication_config (Dict[str, Any]): A nested dictionary containing all\n",
        "                                             parameters for the study.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if all validations pass successfully.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If inputs are of the wrong type.\n",
        "        ValueError: If inputs fail structural, content, or parameter checks.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # --- Step 1: Validate Asset Returns DataFrame ---\n",
        "        logging.info(\"--- Starting Asset Returns Validation ---\")\n",
        "        # Perform basic structural checks on the asset returns DataFrame.\n",
        "        _validate_dataframe_structure(asset_returns, 'asset_returns')\n",
        "        # Perform data-specific checks for returns data.\n",
        "        _validate_returns_data(asset_returns, 'asset_returns')\n",
        "        logging.info(\"--- Asset Returns Validation Complete ---\")\n",
        "\n",
        "        # --- Step 2: Validate Factor Returns DataFrame ---\n",
        "        logging.info(\"--- Starting Factor Returns Validation ---\")\n",
        "        # Perform basic structural checks on the factor returns DataFrame.\n",
        "        _validate_dataframe_structure(factor_returns, 'factor_returns')\n",
        "        # Define the list of required columns for the Fama-French 5-factor model.\n",
        "        required_factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "        # Perform factor-specific data checks.\n",
        "        _validate_factor_data(factor_returns, required_factors)\n",
        "        logging.info(\"--- Factor Returns Validation Complete ---\")\n",
        "\n",
        "        # --- Step 3: Validate Configuration Dictionary ---\n",
        "        logging.info(\"--- Starting Configuration Dictionary Validation ---\")\n",
        "        # Define a simplified schema for the configuration dictionary.\n",
        "        # A full implementation would have a much more detailed schema.\n",
        "        config_schema = {\n",
        "            'monte_carlo': dict,\n",
        "            'empirical': {\n",
        "                'run_controls': dict,\n",
        "                'model_specifications': dict,\n",
        "                'test_config': {\n",
        "                    'our_test': {\n",
        "                        'nu': (int, float),\n",
        "                        'tau': float,\n",
        "                        'de_randomization': {\n",
        "                            'B_function': Callable,\n",
        "                            'f_B_function': Callable\n",
        "                        }\n",
        "                    }\n",
        "                },\n",
        "                'reporting': dict\n",
        "            }\n",
        "        }\n",
        "        # Validate the configuration dictionary against the schema.\n",
        "        _validate_config_dict(replication_config, config_schema)\n",
        "        logging.info(\"--- Configuration Dictionary Validation Complete ---\")\n",
        "\n",
        "    except (TypeError, ValueError) as e:\n",
        "        # Log the specific error that occurred during validation.\n",
        "        logging.error(f\"Input validation failed: {e}\")\n",
        "        # Re-raise the exception to halt execution.\n",
        "        raise\n",
        "\n",
        "    # If all checks pass, log a success message and return True.\n",
        "    logging.info(\"All input validations passed successfully.\")\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "tbx_oKb2aMHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Data Cleansing\n",
        "\n",
        "def _handle_missing_returns(\n",
        "    returns_df: pd.DataFrame,\n",
        "    min_obs_frac_col: float,\n",
        "    min_obs_frac_row: float,\n",
        "    max_consecutive_fill: int\n",
        ") -> Tuple[pd.DataFrame, Dict[str, List]]:\n",
        "    \"\"\"\n",
        "    Handles missing values in the asset returns DataFrame with high rigor.\n",
        "\n",
        "    The process involves:\n",
        "    1. Filtering out assets with excessive missing data.\n",
        "    2. Filtering out time periods with insufficient asset data.\n",
        "    3. Applying a constrained forward-fill for small, isolated gaps.\n",
        "    4. A final pruning of any assets that still contain NaNs.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The input DataFrame of asset returns.\n",
        "        min_obs_frac_col (float): The minimum fraction of non-NaN observations\n",
        "                                  required for an asset (column) to be kept.\n",
        "        min_obs_frac_row (float): The minimum fraction of non-NaN assets\n",
        "                                  (columns) required for a time period (row)\n",
        "                                  to be kept.\n",
        "        max_consecutive_fill (int): The maximum number of consecutive NaNs to\n",
        "                                    forward-fill.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, List]]: A tuple containing:\n",
        "            - The cleansed DataFrame.\n",
        "            - A log dictionary detailing the cleansing actions.\n",
        "    \"\"\"\n",
        "    # Work on a deep copy to avoid modifying the original DataFrame.\n",
        "    df = returns_df.copy(deep=True)\n",
        "    original_assets = set(df.columns)\n",
        "    original_dates = set(df.index)\n",
        "\n",
        "    # Initialize the log.\n",
        "    log = {\n",
        "        'assets_dropped_initial_filter': [],\n",
        "        'dates_dropped_initial_filter': [],\n",
        "        'imputations': []\n",
        "    }\n",
        "\n",
        "    # 1. Filter columns (assets) with too many missing values.\n",
        "    min_obs_col = int(min_obs_frac_col * len(df))\n",
        "    df.dropna(axis=1, thresh=min_obs_col, inplace=True)\n",
        "    assets_after_col_filter = set(df.columns)\n",
        "    log['assets_dropped_initial_filter'] = sorted(list(original_assets - assets_after_col_filter))\n",
        "    logging.info(f\"Dropped {len(log['assets_dropped_initial_filter'])} assets due to excessive NaNs.\")\n",
        "\n",
        "    # 2. Filter rows (dates) with too many missing values.\n",
        "    min_obs_row = int(min_obs_frac_row * len(df.columns))\n",
        "    df.dropna(axis=0, thresh=min_obs_row, inplace=True)\n",
        "    dates_after_row_filter = set(df.index)\n",
        "    log['dates_dropped_initial_filter'] = sorted(list(original_dates - dates_after_row_filter))\n",
        "    logging.info(f\"Dropped {len(log['dates_dropped_initial_filter'])} dates due to insufficient asset data.\")\n",
        "\n",
        "    # 3. Apply constrained forward-fill.\n",
        "    for col in df.columns:\n",
        "        # Identify consecutive NaN blocks.\n",
        "        nan_blocks = df[col].isna().astype(int).groupby(df[col].notna().astype(int).cumsum()).sum()\n",
        "        # Iterate through blocks that are small enough to fill.\n",
        "        for block_start_idx, block_size in nan_blocks[nan_blocks <= max_consecutive_fill].items():\n",
        "            if block_size > 0:\n",
        "                # Find the actual start and end index of the NaN block in the DataFrame.\n",
        "                start_loc = df.index.get_loc(df[col].iloc[block_start_idx - 1:block_start_idx + block_size].index[0])\n",
        "                # Apply ffill only to this specific slice.\n",
        "                df.iloc[start_loc:start_loc + block_size, df.columns.get_loc(col)] = df.iloc[start_loc-1:start_loc, df.columns.get_loc(col)].values[0]\n",
        "                # Log the imputation.\n",
        "                for i in range(int(block_size)):\n",
        "                    log['imputations'].append({\n",
        "                        'asset': col,\n",
        "                        'date': df.index[start_loc + i],\n",
        "                        'imputed_value': df.iloc[start_loc + i, df.columns.get_loc(col)]\n",
        "                    })\n",
        "\n",
        "    logging.info(f\"Performed {len(log['imputations'])} constrained forward-fill imputations.\")\n",
        "\n",
        "    # 4. Final pruning: remove any columns that still have NaNs.\n",
        "    assets_before_final_prune = set(df.columns)\n",
        "    df.dropna(axis=1, how='any', inplace=True)\n",
        "    assets_after_final_prune = set(df.columns)\n",
        "    log['assets_dropped_final_prune'] = sorted(list(assets_before_final_prune - assets_after_final_prune))\n",
        "    logging.info(f\"Dropped {len(log['assets_dropped_final_prune'])} assets with remaining NaNs after imputation.\")\n",
        "\n",
        "    return df, log\n",
        "\n",
        "\n",
        "def _treat_outliers_by_winsorization(\n",
        "    returns_df: pd.DataFrame,\n",
        "    lower_quantile: float,\n",
        "    upper_quantile: float\n",
        ") -> Tuple[pd.DataFrame, Dict[str, List]]:\n",
        "    \"\"\"\n",
        "    Treats outliers by cross-sectional Winsorization for each time period.\n",
        "\n",
        "    Args:\n",
        "        returns_df (pd.DataFrame): The input DataFrame of asset returns.\n",
        "        lower_quantile (float): The lower percentile for capping (e.g., 0.01).\n",
        "        upper_quantile (float): The upper percentile for capping (e.g., 0.99).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, Dict[str, List]]: A tuple containing:\n",
        "            - The DataFrame with outliers treated.\n",
        "            - A log of all modifications.\n",
        "    \"\"\"\n",
        "    # Work on a deep copy.\n",
        "    df = returns_df.copy(deep=True)\n",
        "    log = {'winsorizations': []}\n",
        "\n",
        "    # Calculate cross-sectional quantiles for each time period (row).\n",
        "    lower_bounds = df.quantile(q=lower_quantile, axis=1)\n",
        "    upper_bounds = df.quantile(q=upper_quantile, axis=1)\n",
        "\n",
        "    # Create a boolean DataFrame indicating where clipping will occur.\n",
        "    is_clipped = (df.lt(lower_bounds, axis=0)) | (df.gt(upper_bounds, axis=0))\n",
        "\n",
        "    # Log the changes before applying them.\n",
        "    for date, row in is_clipped.iterrows():\n",
        "        clipped_assets = row[row].index\n",
        "        for asset in clipped_assets:\n",
        "            original_value = df.loc[date, asset]\n",
        "            # Determine the new value after clipping.\n",
        "            new_value = np.clip(original_value, lower_bounds[date], upper_bounds[date])\n",
        "            log['winsorizations'].append({\n",
        "                'asset': asset,\n",
        "                'date': date,\n",
        "                'original_value': original_value,\n",
        "                'new_value': new_value\n",
        "            })\n",
        "\n",
        "    # Apply the clipping (Winsorization) efficiently.\n",
        "    df_winsorized = df.clip(lower=lower_bounds, upper=upper_bounds, axis=0)\n",
        "\n",
        "    logging.info(f\"Performed {len(log['winsorizations'])} outlier treatments via Winsorization.\")\n",
        "\n",
        "    return df_winsorized, log\n",
        "\n",
        "\n",
        "def clean_and_align_data(\n",
        "    asset_returns: pd.DataFrame,\n",
        "    factor_returns: pd.DataFrame,\n",
        "    min_obs_frac_col: float = 0.8,\n",
        "    min_obs_frac_row: float = 0.5,\n",
        "    max_consecutive_fill: int = 2,\n",
        "    winsorize_quantiles: Tuple[float, float] = (0.01, 0.99),\n",
        "    min_obs_for_analysis: int = 60\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Orchestrates the data cleansing and alignment pipeline.\n",
        "\n",
        "    This function takes raw asset and factor returns, handles missing values,\n",
        "    treats outliers, and produces two perfectly aligned DataFrames ready for\n",
        "    econometric analysis, along with a comprehensive log of all actions taken.\n",
        "\n",
        "    Args:\n",
        "        asset_returns (pd.DataFrame): Raw DataFrame of asset returns.\n",
        "        factor_returns (pd.DataFrame): Raw DataFrame of factor returns.\n",
        "        min_obs_frac_col (float): Min fraction of data for an asset to be kept.\n",
        "        min_obs_frac_row (float): Min fraction of assets for a date to be kept.\n",
        "        max_consecutive_fill (int): Max number of consecutive NaNs to fill.\n",
        "        winsorize_quantiles (Tuple[float, float]): Lower and upper quantiles for\n",
        "                                                   outlier treatment.\n",
        "        min_obs_for_analysis (int): Minimum number of observations required for\n",
        "                                    an asset to be included in the final universe.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame, Dict[str, Any]]: A tuple containing:\n",
        "            - The clean, aligned asset returns DataFrame.\n",
        "            - The clean, aligned factor returns DataFrame.\n",
        "            - A comprehensive log dictionary.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Handle Missing Values in Asset Returns ---\n",
        "    logging.info(\"--- Starting Missing Value Handling ---\")\n",
        "    # Apply the rigorous missing value handling process.\n",
        "    clean_returns, missing_log = _handle_missing_returns(\n",
        "        asset_returns,\n",
        "        min_obs_frac_col,\n",
        "        min_obs_frac_row,\n",
        "        max_consecutive_fill\n",
        "    )\n",
        "\n",
        "    # --- Step 2: Treat Outliers in Asset Returns ---\n",
        "    logging.info(\"--- Starting Outlier Treatment ---\")\n",
        "    # Apply cross-sectional Winsorization to the cleaned returns.\n",
        "    winsorized_returns, outlier_log = _treat_outliers_by_winsorization(\n",
        "        clean_returns,\n",
        "        winsorize_quantiles[0],\n",
        "        winsorize_quantiles[1]\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Align DataFrames and Final Filtering ---\n",
        "    logging.info(\"--- Starting DataFrame Alignment and Final Filtering ---\")\n",
        "    # Determine the common date range by intersecting the indices.\n",
        "    common_index = winsorized_returns.index.intersection(factor_returns.index)\n",
        "    if common_index.empty:\n",
        "        raise ValueError(\"No common dates found between asset and factor returns.\")\n",
        "\n",
        "    # Slice both DataFrames to the common date range.\n",
        "    aligned_assets = winsorized_returns.loc[common_index].copy()\n",
        "    aligned_factors = factor_returns.loc[common_index].copy()\n",
        "\n",
        "    # Final asset filtering: ensure each asset has enough data for rolling analysis.\n",
        "    obs_counts = aligned_assets.notna().sum()\n",
        "    # Identify assets that meet the minimum observation requirement.\n",
        "    assets_to_keep = obs_counts[obs_counts >= min_obs_for_analysis].index\n",
        "    # Filter the asset returns DataFrame to keep only these viable assets.\n",
        "    final_assets = aligned_assets[assets_to_keep]\n",
        "\n",
        "    # Log the final filtering step.\n",
        "    assets_dropped_final = sorted(list(set(aligned_assets.columns) - set(final_assets.columns)))\n",
        "    logging.info(\n",
        "        f\"Dropped {len(assets_dropped_final)} assets with insufficient observations \"\n",
        "        f\"for rolling analysis (min_obs_for_analysis={min_obs_for_analysis}).\"\n",
        "    )\n",
        "    logging.info(\n",
        "        f\"Final aligned dataset contains {len(final_assets.columns)} assets \"\n",
        "        f\"and {len(final_assets)} time periods.\"\n",
        "    )\n",
        "\n",
        "    # Compile the comprehensive log.\n",
        "    full_log = {\n",
        "        'missing_value_handling': missing_log,\n",
        "        'outlier_treatment': outlier_log,\n",
        "        'alignment_and_filtering': {\n",
        "            'common_start_date': common_index.min(),\n",
        "            'common_end_date': common_index.max(),\n",
        "            'n_common_periods': len(common_index),\n",
        "            'assets_dropped_insufficient_history': assets_dropped_final,\n",
        "            'final_n_assets': final_assets.shape[1],\n",
        "            'final_n_periods': final_assets.shape[0]\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return final_assets, aligned_factors, full_log\n"
      ],
      "metadata": {
        "id": "s348kXyDaegf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Empirical Analysis Setup\n",
        "\n",
        "def setup_empirical_analysis_windows(\n",
        "    aligned_data_index: pd.DatetimeIndex,\n",
        "    replication_config: Dict[str, Any]\n",
        ") -> List[Tuple[pd.Timestamp, pd.Timestamp]]:\n",
        "    \"\"\"\n",
        "    Sets up the rolling window schedule for the empirical analysis.\n",
        "\n",
        "    This function reads the empirical analysis parameters from the configuration,\n",
        "    validates them, determines the effective date range based on available data,\n",
        "    and generates a list of (start_date, end_date) tuples for each rolling\n",
        "    window.\n",
        "\n",
        "    Args:\n",
        "        aligned_data_index (pd.DatetimeIndex): The datetime index from the\n",
        "                                               aligned and cleansed data, which\n",
        "                                               defines the universe of available dates.\n",
        "        replication_config (Dict[str, Any]): The study's configuration dictionary.\n",
        "\n",
        "    Returns:\n",
        "        List[Tuple[pd.Timestamp, pd.Timestamp]]: A list where each tuple contains\n",
        "                                                 the start and end Timestamps for\n",
        "                                                 one rolling window.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If configuration parameters are invalid, if the specified\n",
        "                    date range has no overlap with the available data, or if\n",
        "                    no full windows can be constructed.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Define and Validate Rolling Window Parameters ---\n",
        "    try:\n",
        "        # Extract the empirical run controls from the configuration.\n",
        "        controls = replication_config['empirical']['run_controls']\n",
        "        # Extract the window size in months.\n",
        "        window_size = controls['rolling_window_size_months']\n",
        "        # Extract the user-specified start date string.\n",
        "        config_start_str = controls['start_date']\n",
        "        # Extract the user-specified end date string.\n",
        "        config_end_str = controls['end_date']\n",
        "    except KeyError as e:\n",
        "        # Handle cases where essential keys are missing in the config.\n",
        "        logging.error(f\"Missing key in replication_config['empirical']['run_controls']: {e}\")\n",
        "        raise ValueError(f\"Configuration is missing required key for empirical setup: {e}\")\n",
        "\n",
        "    # Validate the type and value of the window size.\n",
        "    if not isinstance(window_size, int) or window_size < 36:\n",
        "        raise ValueError(\n",
        "            f\"rolling_window_size_months must be an integer >= 36, but got {window_size}.\"\n",
        "        )\n",
        "\n",
        "    # Parse date strings into pandas Timestamp objects for robust handling.\n",
        "    try:\n",
        "        config_start_date = pd.to_datetime(config_start_str)\n",
        "        config_end_date = pd.to_datetime(config_end_str)\n",
        "    except Exception as e:\n",
        "        # Handle invalid date formats.\n",
        "        logging.error(f\"Could not parse dates from configuration: {e}\")\n",
        "        raise ValueError(\"Invalid date format in configuration. Please use 'YYYY-MM-DD'.\")\n",
        "\n",
        "    # Validate the chronological order of the configured dates.\n",
        "    if config_start_date >= config_end_date:\n",
        "        raise ValueError(\"Configuration 'start_date' must be before 'end_date'.\")\n",
        "\n",
        "    # Determine the effective date range by intersecting config range with available data.\n",
        "    data_start_date = aligned_data_index.min()\n",
        "    data_end_date = aligned_data_index.max()\n",
        "\n",
        "    # The effective start is the later of the two start dates.\n",
        "    effective_start_date = max(config_start_date, data_start_date)\n",
        "    # The effective end is the earlier of the two end dates.\n",
        "    effective_end_date = min(config_end_date, data_end_date)\n",
        "\n",
        "    # Check if there is a valid overlapping period for the analysis.\n",
        "    if effective_start_date >= effective_end_date:\n",
        "        raise ValueError(\n",
        "            \"No overlapping date range between configured dates and available data. \"\n",
        "            f\"Config Range: {config_start_date.date()} to {config_end_date.date()}. \"\n",
        "            f\"Data Range: {data_start_date.date()} to {data_end_date.date()}.\"\n",
        "        )\n",
        "\n",
        "    logging.info(f\"Empirical analysis parameters validated. Effective date range: \"\n",
        "                 f\"{effective_start_date.date()} to {effective_end_date.date()}.\")\n",
        "\n",
        "    # --- Step 2: Generate the List of Rolling Windows ---\n",
        "\n",
        "    # The first possible end date for a full window.\n",
        "    first_window_end = effective_start_date + pd.DateOffset(months=window_size - 1)\n",
        "\n",
        "    # Check if the data range is long enough to form at least one window.\n",
        "    if first_window_end > effective_end_date:\n",
        "        raise ValueError(\n",
        "            f\"The effective data range ({effective_start_date.date()} to \"\n",
        "            f\"{effective_end_date.date()}) is shorter than the window size \"\n",
        "            f\"of {window_size} months. No full windows can be constructed.\"\n",
        "        )\n",
        "\n",
        "    # Generate all possible month-end dates that can serve as the end of a window.\n",
        "    # We use the aligned data index to ensure our window dates are actual data points.\n",
        "    possible_end_dates = aligned_data_index[\n",
        "        (aligned_data_index >= first_window_end) &\n",
        "        (aligned_data_index <= effective_end_date)\n",
        "    ]\n",
        "\n",
        "    # If no valid end dates are found, no windows can be created.\n",
        "    if possible_end_dates.empty:\n",
        "        raise ValueError(\n",
        "            \"Could not form any valid rolling windows. Check alignment of data \"\n",
        "            \"and configured date range.\"\n",
        "        )\n",
        "\n",
        "    # Construct the list of (start_date, end_date) tuples.\n",
        "    windows = []\n",
        "    for end_date in possible_end_dates:\n",
        "        # Calculate the start date for the current window.\n",
        "        # Note: This start date might not be an exact data point, but defines the slice.\n",
        "        start_date = end_date - pd.DateOffset(months=window_size - 1)\n",
        "        # Append the window tuple to the list.\n",
        "        windows.append((start_date, end_date))\n",
        "\n",
        "    logging.info(f\"Successfully generated {len(windows)} rolling windows of \"\n",
        "                 f\"{window_size} months each.\")\n",
        "\n",
        "    return windows\n"
      ],
      "metadata": {
        "id": "ksAZElxkbTEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Model Estimation\n",
        "\n",
        "class FactorModelEstimationResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured container for the results of factor model estimation.\n",
        "\n",
        "    Attributes:\n",
        "        alphas (pd.Series): A series of estimated alphas (α̂), indexed by asset.\n",
        "        betas (pd.DataFrame): A DataFrame of estimated betas (β̂), with assets\n",
        "                              as the index and factors as columns.\n",
        "        residuals (pd.DataFrame): A DataFrame of residuals (û), with the same\n",
        "                                  dimensions and indices as the input asset returns.\n",
        "        condition_number (float): The condition number of the regressor matrix,\n",
        "                                  used to diagnose multicollinearity.\n",
        "    \"\"\"\n",
        "    alphas: pd.Series\n",
        "    betas: pd.DataFrame\n",
        "    residuals: pd.DataFrame\n",
        "    condition_number: float\n",
        "\n",
        "def estimate_factor_model_params(\n",
        "    asset_returns_window: pd.DataFrame,\n",
        "    factor_returns_window: pd.DataFrame\n",
        ") -> FactorModelEstimationResult:\n",
        "    \"\"\"\n",
        "    Estimates alpha, beta, and residuals for a panel of assets using OLS.\n",
        "\n",
        "    This function performs a time-series regression for each asset against the\n",
        "    provided factors for a specific time window. It is highly optimized to\n",
        "    estimate parameters for all assets simultaneously using matrix algebra.\n",
        "\n",
        "    The regression model for each asset `i` is:\n",
        "    y_i,t = α_i + β'_i * f_t + u_i,t\n",
        "\n",
        "    Args:\n",
        "        asset_returns_window (pd.DataFrame): A (T x N) DataFrame of asset\n",
        "                                             returns for the estimation window.\n",
        "                                             Index is datetime, columns are assets.\n",
        "        factor_returns_window (pd.DataFrame): A (T x K) DataFrame of factor\n",
        "                                              returns for the same window.\n",
        "                                              Index is datetime, columns are factors.\n",
        "\n",
        "    Returns:\n",
        "        FactorModelEstimationResult: A NamedTuple containing the estimated alphas,\n",
        "                                     betas, residuals, and the regressor matrix\n",
        "                                     condition number.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If input DataFrames are not aligned, empty, or contain NaNs.\n",
        "        np.linalg.LinAlgError: If the regression fails due to a singular matrix.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check for empty inputs.\n",
        "    if asset_returns_window.empty or factor_returns_window.empty:\n",
        "        raise ValueError(\"Input DataFrames for estimation cannot be empty.\")\n",
        "\n",
        "    # Check for perfect temporal alignment.\n",
        "    if not asset_returns_window.index.equals(factor_returns_window.index):\n",
        "        raise ValueError(\"Asset and factor returns indices are not aligned.\")\n",
        "\n",
        "    # Ensure no missing values are present in the estimation window.\n",
        "    if asset_returns_window.isna().any().any() or factor_returns_window.isna().any().any():\n",
        "        raise ValueError(\"Input DataFrames for estimation must not contain NaN values.\")\n",
        "\n",
        "    # --- Step 1: Data Preparation and Matrix Construction ---\n",
        "\n",
        "    # Extract dimensions for clarity and validation.\n",
        "    T, N = asset_returns_window.shape\n",
        "    _T, K = factor_returns_window.shape\n",
        "\n",
        "    # Assert that the time dimension T is consistent.\n",
        "    assert T == _T, \"Mismatch in number of time periods (rows).\"\n",
        "\n",
        "    # Convert pandas DataFrames to NumPy arrays for performance, ensuring float64.\n",
        "    Y = asset_returns_window.to_numpy(dtype=np.float64)\n",
        "    F = factor_returns_window.to_numpy(dtype=np.float64)\n",
        "\n",
        "    # Create the regressor matrix X by adding an intercept column of ones.\n",
        "    # X will have dimensions T x (K+1).\n",
        "    X = np.hstack([np.ones((T, 1), dtype=np.float64), F])\n",
        "\n",
        "    # --- Step 2: OLS Estimation using Numerically Stable Solver ---\n",
        "\n",
        "    # Check for potential multicollinearity before solving.\n",
        "    # This is a crucial step for numerical stability.\n",
        "    xtx = X.T @ X\n",
        "    condition_number = np.linalg.cond(xtx)\n",
        "\n",
        "    # Log a warning if the matrix is ill-conditioned.\n",
        "    if condition_number > 1e3: # A common threshold for moderate multicollinearity\n",
        "        logging.warning(\n",
        "            f\"High condition number detected: {condition_number:.2e}. \"\n",
        "            \"OLS estimates may be unstable due to multicollinearity.\"\n",
        "        )\n",
        "\n",
        "    try:\n",
        "        # Solve the normal equations (X'X)B = X'Y for B.\n",
        "        # This is numerically superior to computing the inverse of X'X.\n",
        "        # B is the (K+1) x N matrix of coefficients.\n",
        "        # Equation: B̂ = (X'X)^-1 * X'Y\n",
        "        coefficients = np.linalg.solve(xtx, X.T @ Y)\n",
        "    except np.linalg.LinAlgError as e:\n",
        "        # Handle cases where the matrix is singular and cannot be solved.\n",
        "        logging.error(f\"Linear algebra error during OLS estimation: {e}\")\n",
        "        raise\n",
        "\n",
        "    # --- Step 3: Extract Parameters and Calculate Residuals ---\n",
        "\n",
        "    # The first row of the coefficient matrix contains the alphas.\n",
        "    # Equation: α̂_i = ȳ_i - β̂'_i * f̄ (This is implicitly calculated by the regression)\n",
        "    alphas_arr = coefficients[0, :]\n",
        "\n",
        "    # The remaining rows contain the betas.\n",
        "    betas_arr = coefficients[1:, :]\n",
        "\n",
        "    # Calculate the predicted values (Y_hat) for all assets.\n",
        "    # Equation: Ŷ = X * B̂\n",
        "    Y_hat = X @ coefficients\n",
        "\n",
        "    # Calculate the residuals (û).\n",
        "    # Equation: û = Y - Ŷ\n",
        "    residuals_arr = Y - Y_hat\n",
        "\n",
        "    # --- Step 4: Format and Return Results ---\n",
        "\n",
        "    # Convert the NumPy array results back to labeled pandas objects.\n",
        "    alphas_series = pd.Series(\n",
        "        alphas_arr,\n",
        "        index=asset_returns_window.columns,\n",
        "        name='alpha'\n",
        "    )\n",
        "    betas_df = pd.DataFrame(\n",
        "        betas_arr.T, # Transpose to get N x K shape\n",
        "        index=asset_returns_window.columns,\n",
        "        columns=factor_returns_window.columns\n",
        "    )\n",
        "    residuals_df = pd.DataFrame(\n",
        "        residuals_arr,\n",
        "        index=asset_returns_window.index,\n",
        "        columns=asset_returns_window.columns\n",
        "    )\n",
        "\n",
        "    # Package the results into the structured NamedTuple.\n",
        "    result = FactorModelEstimationResult(\n",
        "        alphas=alphas_series,\n",
        "        betas=betas_df,\n",
        "        residuals=residuals_df,\n",
        "        condition_number=condition_number\n",
        "    )\n",
        "\n",
        "    logging.info(f\"Successfully estimated parameters for {N} assets and {K} factors over {T} periods.\")\n",
        "\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "tNg4Pavxb-SV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Test Statistic Computation\n",
        "\n",
        "def _calculate_scaling_factor(\n",
        "    residuals: pd.DataFrame\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the scaling factor s_NT based on squared OLS residuals.\n",
        "\n",
        "    This function implements Equation (3.3) from the paper.\n",
        "    Equation (3.3): s_NT = sqrt((1/NT) * Σ_{i=1 to N} Σ_{t=1 to T} û²_i,t)\n",
        "\n",
        "    Args:\n",
        "        residuals (pd.DataFrame): A (T x N) DataFrame of model residuals (û).\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar scaling factor s_NT.\n",
        "    \"\"\"\n",
        "    # Convert the residuals DataFrame to a NumPy array for efficient computation.\n",
        "    residuals_arr = residuals.to_numpy(dtype=np.float64)\n",
        "\n",
        "    # Square all residual values element-wise.\n",
        "    # Corresponds to: û²_i,t\n",
        "    squared_residuals = np.square(residuals_arr)\n",
        "\n",
        "    # Calculate the mean of all squared residuals.\n",
        "    # This is equivalent to (1/NT) * ΣΣ û²_i,t\n",
        "    mean_squared_residuals = np.mean(squared_residuals)\n",
        "\n",
        "    # Take the square root to get the final scaling factor.\n",
        "    s_nt = np.sqrt(mean_squared_residuals)\n",
        "\n",
        "    # Ensure the scaling factor is strictly positive to prevent division by zero.\n",
        "    # This adds a \"nugget\" for numerical stability.\n",
        "    s_nt_stable = np.fmax(s_nt, 1e-12)\n",
        "\n",
        "    return float(s_nt_stable)\n",
        "\n",
        "\n",
        "def compute_psi_statistic(\n",
        "    estimation_result: FactorModelEstimationResult,\n",
        "    nu: float\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Computes the transformed psi (ψ) statistic for each asset.\n",
        "\n",
        "    This function implements the core transformation of the test, as specified\n",
        "    in Equation (3.2) of the paper. It takes the estimated alphas and residuals,\n",
        "    calculates the necessary scaling factors, and applies the non-linear\n",
        "    transformation.\n",
        "\n",
        "    Equation (3.2): ψ_i,NT = (|T^(1/ν) * α̂_i,T| / s_NT)^(ν/2)\n",
        "\n",
        "    Args:\n",
        "        estimation_result (FactorModelEstimationResult): A NamedTuple containing\n",
        "            the outputs from the `estimate_factor_model_params` function.\n",
        "        nu (float): The moment parameter ν, which must be greater than 4 as per\n",
        "                    the paper's theoretical requirements.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series containing the calculated ψ_i,NT value for\n",
        "                   each asset, indexed by the asset identifiers.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the parameter nu is not > 4.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Validate the moment parameter nu, a critical assumption of the test.\n",
        "    if not (isinstance(nu, (int, float)) and nu > 4):\n",
        "        raise ValueError(f\"The moment parameter 'nu' (ν) must be a number strictly greater than 4, but got {nu}.\")\n",
        "\n",
        "    # Extract necessary components from the estimation result.\n",
        "    alphas = estimation_result.alphas\n",
        "    residuals = estimation_result.residuals\n",
        "\n",
        "    # Get the number of time periods (T) from the residuals DataFrame.\n",
        "    T = residuals.shape[0]\n",
        "\n",
        "    # --- Step 1: Calculate the Scaling Factor s_NT ---\n",
        "    # This helper function implements Equation (3.3).\n",
        "    s_nt = _calculate_scaling_factor(residuals)\n",
        "\n",
        "    # --- Step 2: Apply the Psi Transformation ---\n",
        "\n",
        "    # Calculate the time-scaling component: T^(1/ν)\n",
        "    time_scaler = np.power(T, 1.0 / nu, dtype=np.float64)\n",
        "\n",
        "    # Calculate the numerator of the base: |T^(1/ν) * α̂_i,T|\n",
        "    # We use .abs() for the absolute value of the alphas.\n",
        "    numerator = time_scaler * alphas.abs()\n",
        "\n",
        "    # Calculate the base of the exponentiation: numerator / s_NT\n",
        "    base = numerator / s_nt\n",
        "\n",
        "    # Check for potential overflow before final exponentiation.\n",
        "    if base.max() > 1e10:\n",
        "        logging.warning(\n",
        "            f\"Large base value ({base.max():.2e}) detected before final \"\n",
        "            \"exponentiation. Results for psi may be inf.\"\n",
        "        )\n",
        "\n",
        "    # Calculate the final psi statistic by raising the base to the power of ν/2.\n",
        "    # This implements the full Equation (3.2).\n",
        "    psi_values = np.power(base, nu / 2.0, dtype=np.float64)\n",
        "\n",
        "    # Ensure the final result is a pandas Series with the correct name and index.\n",
        "    psi_values.name = 'psi_statistic'\n",
        "\n",
        "    logging.info(f\"Successfully computed psi statistics for {len(psi_values)} assets.\")\n",
        "\n",
        "    return psi_values\n"
      ],
      "metadata": {
        "id": "DgWGq7y1ct0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Randomization and Test Statistic\n",
        "\n",
        "def _generate_omega_shocks(\n",
        "    n: int,\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Generates N i.i.d. standard normal random shocks.\n",
        "\n",
        "    Args:\n",
        "        n (int): The number of shocks to generate (should equal the number of assets, N).\n",
        "        rng (np.random.Generator): The random number generator instance to ensure\n",
        "                                   reproducibility and control over the random state.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of shape (n,) containing the random shocks.\n",
        "    \"\"\"\n",
        "    # Generate n random variates from the standard normal distribution.\n",
        "    # Using the Generator instance is the modern, preferred NumPy API.\n",
        "    return rng.standard_normal(size=n, dtype=np.float64)\n",
        "\n",
        "\n",
        "def compute_randomized_test_statistic(\n",
        "    psi_values: pd.Series,\n",
        "    rng: np.random.Generator\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes one realization of the randomized test statistic, Z_N,T.\n",
        "\n",
        "    This function executes the core randomization step of the testing procedure.\n",
        "    It takes the deterministic ψ statistics, adds a random standard normal shock\n",
        "    to each, and then finds the maximum value, which is the test statistic.\n",
        "\n",
        "    The procedure implements the following equations from the paper:\n",
        "    1. z_i,NT = ψ_i,NT + ω_i, where ω_i ~ i.i.d. N(0, 1)\n",
        "    2. Z_N,T = max_{1≤i≤N} z_i,NT\n",
        "\n",
        "    Args:\n",
        "        psi_values (pd.Series): A pandas Series of the calculated ψ_i,NT values,\n",
        "                                indexed by asset identifiers.\n",
        "        rng (np.random.Generator): The random number generator instance to use for\n",
        "                                   generating the ω shocks. This is critical for\n",
        "                                   reproducibility, especially within the\n",
        "                                   de-randomization loop.\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar value of the final test statistic, Z_N,T.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input psi_values Series is empty or contains NaNs.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input Series is empty.\n",
        "    if psi_values.empty:\n",
        "        raise ValueError(\"Input 'psi_values' Series cannot be empty.\")\n",
        "\n",
        "    # The psi statistics must be finite and non-negative. NaNs are not allowed.\n",
        "    if not np.all(np.isfinite(psi_values)) or (psi_values < 0).any():\n",
        "        raise ValueError(\"Input 'psi_values' must contain only finite, non-negative numbers.\")\n",
        "\n",
        "    # Get the number of assets (N) from the length of the Series.\n",
        "    N = len(psi_values)\n",
        "\n",
        "    # --- Step 1: Generate Random Shocks (ω_i) ---\n",
        "    # Generate N i.i.d. standard normal shocks using the provided generator.\n",
        "    omega_shocks = _generate_omega_shocks(n=N, rng=rng)\n",
        "\n",
        "    # --- Step 2: Compute Perturbed Statistics (z_i,NT) ---\n",
        "    # Convert psi Series to NumPy array for efficient, non-indexed addition.\n",
        "    psi_array = psi_values.to_numpy(dtype=np.float64)\n",
        "\n",
        "    # Add the random shocks to the psi statistics element-wise.\n",
        "    # Equation: z_i,NT = ψ_i,NT + ω_i\n",
        "    z_array = psi_array + omega_shocks\n",
        "\n",
        "    # --- Step 3: Compute Final Test Statistic (Z_N,T) ---\n",
        "    # Find the maximum value among all z statistics.\n",
        "    # Equation: Z_N,T = max_{1≤i≤N} z_i,NT\n",
        "    z_nt_statistic = np.max(z_array)\n",
        "\n",
        "    # The result must be a standard Python float.\n",
        "    return float(z_nt_statistic)\n"
      ],
      "metadata": {
        "id": "UVs_RQu9dUtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: De-randomization\n",
        "\n",
        "class DeRandomizationResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Structured container for the results of the de-randomization procedure.\n",
        "\n",
        "    Attributes:\n",
        "        q_statistic (float): The final Q_N,T,B(τ) statistic.\n",
        "        critical_value (float): The asymptotic critical value c_τ used.\n",
        "        b_replications (int): The number of replications (B) performed.\n",
        "        tau (float): The significance level (τ) of the test.\n",
        "    \"\"\"\n",
        "    q_statistic: float\n",
        "    critical_value: float\n",
        "    b_replications: int\n",
        "    tau: float\n",
        "\n",
        "# Configure a basic logger for the de-randomization module\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "\n",
        "def _compute_asymptotic_critical_value(N: int, tau: float) -> float:\n",
        "    \"\"\"\n",
        "    Computes the asymptotic critical value c_τ from the Gumbel distribution.\n",
        "\n",
        "    This function implements the calculation of normalization constants a_N and b_N,\n",
        "    and the final critical value c_τ as specified in Section 3 of the paper,\n",
        "    leading up to Equation (3.6).\n",
        "\n",
        "    Equation for b_N: b_N = sqrt(2logN) - (log(logN) + log(4π)) / (2*sqrt(2logN))\n",
        "    Equation for a_N: a_N = b_N / (1 + b_N^2)\n",
        "    Equation (3.6): c_τ = b_N - a_N * log(-log(1 - τ))\n",
        "\n",
        "    Args:\n",
        "        N (int): The number of assets (cross-sectional dimension).\n",
        "        tau (float): The significance level of the test (e.g., 0.05).\n",
        "\n",
        "    Returns:\n",
        "        float: The scalar asymptotic critical value, c_τ.\n",
        "    \"\"\"\n",
        "    # Validate inputs for theoretical soundness.\n",
        "    if N < 3:\n",
        "        raise ValueError(f\"N must be >= 3 for asymptotic constants to be well-defined, but got {N}.\")\n",
        "    if not (0 < tau < 1):\n",
        "        raise ValueError(f\"Significance level tau must be in (0, 1), but got {tau}.\")\n",
        "\n",
        "    # Use float64 for all intermediate calculations to maintain precision.\n",
        "    N_f = float(N)\n",
        "\n",
        "    # Calculate log(N) and log(log(N)).\n",
        "    log_N = np.log(N_f)\n",
        "    log_log_N = np.log(log_N)\n",
        "    sqrt_2_log_N = np.sqrt(2 * log_N)\n",
        "\n",
        "    # Calculate the b_N normalization constant.\n",
        "    b_N = sqrt_2_log_N - (log_log_N + np.log(4 * math.pi)) / (2 * sqrt_2_log_N)\n",
        "\n",
        "    # Calculate the a_N normalization constant.\n",
        "    a_N = b_N / (1 + np.square(b_N))\n",
        "\n",
        "    # Calculate the final critical value c_τ using the Gumbel quantile function.\n",
        "    c_tau = b_N - a_N * np.log(-np.log(1 - tau))\n",
        "\n",
        "    return float(c_tau)\n",
        "\n",
        "\n",
        "def perform_de_randomization(\n",
        "    psi_values: pd.Series,\n",
        "    tau: float,\n",
        "    b_function: Callable[[int], int],\n",
        "    seed: Optional[int] = None\n",
        ") -> DeRandomizationResult:\n",
        "    \"\"\"\n",
        "    Performs the de-randomization procedure to obtain a deterministic test outcome.\n",
        "\n",
        "    This function orchestrates the entire de-randomization process:\n",
        "    1. Calculates the number of replications, B.\n",
        "    2. Computes the asymptotic critical value, c_τ.\n",
        "    3. Runs a loop B times, each time computing a new randomized test statistic Z_N,T.\n",
        "    4. Computes the Q statistic, which is the fraction of times Z_N,T <= c_τ.\n",
        "\n",
        "    Args:\n",
        "        psi_values (pd.Series): A pandas Series of the calculated ψ_i,NT values.\n",
        "        tau (float): The significance level of the test (e.g., 0.05).\n",
        "        b_function (Callable[[int], int]): A function that takes N and returns B,\n",
        "                                           the number of replications.\n",
        "        seed (Optional[int]): A seed for the random number generator to ensure\n",
        "                              reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        DeRandomizationResult: A NamedTuple containing the Q statistic and other\n",
        "                               relevant parameters of the procedure.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(psi_values, pd.Series):\n",
        "        raise TypeError(f\"psi_values must be a pandas Series, but got {type(psi_values)}.\")\n",
        "    if psi_values.empty:\n",
        "        raise ValueError(\"psi_values cannot be empty.\")\n",
        "\n",
        "    # Get the number of assets, N.\n",
        "    N = len(psi_values)\n",
        "\n",
        "    # --- Step 1: Calculate B and the Critical Value c_τ ---\n",
        "\n",
        "    # Calculate the number of replications B using the provided function.\n",
        "    # As per Theorem 3.2, B should be at least O(log^2 N).\n",
        "    B = b_function(N)\n",
        "    if not isinstance(B, int) or B <= 0:\n",
        "        raise ValueError(\"b_function must return a positive integer.\")\n",
        "\n",
        "    # Compute the asymptotic critical value.\n",
        "    critical_value = _compute_asymptotic_critical_value(N=N, tau=tau)\n",
        "\n",
        "    logging.info(f\"Starting de-randomization with N={N}, B={B}, τ={tau}, c_τ={critical_value:.4f}.\")\n",
        "\n",
        "    # --- Step 2: B-Replication Loop ---\n",
        "\n",
        "    # Initialize a modern random number generator with the provided seed.\n",
        "    # This is crucial for reproducibility.\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Pre-allocate a NumPy array to store the B realizations of the test statistic.\n",
        "    z_statistics = np.empty(B, dtype=np.float64)\n",
        "\n",
        "    # Loop B times to generate the distribution of the randomized statistic.\n",
        "    for b in range(B):\n",
        "        # In each iteration, compute one realization of Z_N,T.\n",
        "        # The same rng object is used, ensuring its state evolves and\n",
        "        # produces independent shocks for each replication.\n",
        "        z_statistics[b] = compute_randomized_test_statistic(psi_values, rng)\n",
        "\n",
        "    # --- Step 3: Compute the Q Statistic ---\n",
        "\n",
        "    # Compare each Z statistic to the critical value to get a boolean array.\n",
        "    # This implements the indicator function I(Z_N,T^(b) <= c_τ).\n",
        "    indicator_results = (z_statistics <= critical_value)\n",
        "\n",
        "    # Calculate the mean of the boolean array to get the Q statistic.\n",
        "    # This implements Equation (3.7): Q_N,T,B(τ) = (1/B) * Σ I(...)\n",
        "    q_statistic = np.mean(indicator_results)\n",
        "\n",
        "    logging.info(f\"De-randomization complete. Q-statistic: {q_statistic:.4f}.\")\n",
        "\n",
        "    # Package and return the results in a structured format.\n",
        "    return DeRandomizationResult(\n",
        "        q_statistic=float(q_statistic),\n",
        "        critical_value=critical_value,\n",
        "        b_replications=B,\n",
        "        tau=tau\n",
        "    )\n"
      ],
      "metadata": {
        "id": "rw0Wpp5ZefLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Decision Rule\n",
        "\n",
        "class TestDecisionResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Structured container for the final hypothesis test decision.\n",
        "\n",
        "    Attributes:\n",
        "        q_statistic (float): The calculated Q_N,T,B(τ) statistic.\n",
        "        decision_threshold (float): The threshold against which the Q statistic\n",
        "                                    was compared. Calculated as (1-τ) - f(B).\n",
        "        fail_to_reject_h0 (bool): The outcome of the test. True if the null\n",
        "                                  hypothesis is not rejected, False if it is.\n",
        "        outcome (str): A human-readable string describing the test result.\n",
        "        de_randomization_result (DeRandomizationResult): The full result object\n",
        "            from the de-randomization step for complete auditability.\n",
        "    \"\"\"\n",
        "    q_statistic: float\n",
        "    decision_threshold: float\n",
        "    fail_to_reject_h0: bool\n",
        "    outcome: str\n",
        "    de_randomization_result: DeRandomizationResult\n",
        "\n",
        "def make_test_decision(\n",
        "    de_randomization_result: DeRandomizationResult,\n",
        "    f_b_function: Callable[[int], float]\n",
        ") -> TestDecisionResult:\n",
        "    \"\"\"\n",
        "    Makes a final, deterministic decision on the null hypothesis of zero alpha.\n",
        "\n",
        "    This function applies the decision rule from Section 3.1 of the paper,\n",
        "    specifically the practical rule suggested in Equation (3.12). It compares\n",
        "    the de-randomized Q statistic against a threshold to determine whether to\n",
        "    reject or fail to reject the null hypothesis.\n",
        "\n",
        "    Decision Rule (from Eq. 3.12):\n",
        "    Fail to Reject H₀ if Q_N,T,B(τ) ≥ (1 - τ) - f(B)\n",
        "\n",
        "    Args:\n",
        "        de_randomization_result (DeRandomizationResult): The output from the\n",
        "            `perform_de_randomization` function.\n",
        "        f_b_function (Callable[[int], float]): A user-specified, non-increasing\n",
        "            function of B that defines the tolerance for the decision rule.\n",
        "            The paper suggests f(B) = B**(-0.25).\n",
        "\n",
        "    Returns:\n",
        "        TestDecisionResult: A NamedTuple containing the binary decision, the\n",
        "                            threshold used, and other relevant statistics for\n",
        "                            a complete and auditable result.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If f_b_function is not a callable or returns a non-numeric type.\n",
        "        ValueError: If the result from f_b_function is not a small positive number.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Validate that the f_b_function is a valid callable.\n",
        "    if not callable(f_b_function):\n",
        "        raise TypeError(f\"f_b_function must be a callable, but got {type(f_b_function)}.\")\n",
        "\n",
        "    # Extract necessary values from the input result object for clarity.\n",
        "    q_statistic = de_randomization_result.q_statistic\n",
        "    B = de_randomization_result.b_replications\n",
        "    tau = de_randomization_result.tau\n",
        "\n",
        "    # --- Step 1: Evaluate the f(B) Function ---\n",
        "    try:\n",
        "        # Calculate the tolerance factor f(B).\n",
        "        f_b_value = f_b_function(B)\n",
        "    except Exception as e:\n",
        "        # Catch potential errors during the execution of the provided lambda.\n",
        "        logging.error(f\"The provided f_b_function failed with error: {e}\")\n",
        "        raise TypeError(\"f_b_function could not be executed.\")\n",
        "\n",
        "    # Validate the output of the f(B) function.\n",
        "    if not isinstance(f_b_value, (int, float)) or not (0 < f_b_value < 1):\n",
        "        raise ValueError(\n",
        "            f\"f_b_function must return a small positive float, but returned {f_b_value}.\"\n",
        "        )\n",
        "\n",
        "    # --- Step 2: Calculate Threshold and Make Decision ---\n",
        "\n",
        "    # Calculate the decision threshold based on the paper's rule.\n",
        "    # Equation: threshold = (1 - τ) - f(B)\n",
        "    decision_threshold = (1 - tau) - f_b_value\n",
        "\n",
        "    # Apply the decision rule by comparing the Q statistic to the threshold.\n",
        "    # Equation: Q_N,T,B(τ) ≥ (1 - τ) - f(B)\n",
        "    fail_to_reject_h0 = (q_statistic >= decision_threshold)\n",
        "\n",
        "    # Create a human-readable outcome string for clarity in reporting.\n",
        "    outcome = \"Fail to Reject H₀\" if fail_to_reject_h0 else \"Reject H₀\"\n",
        "\n",
        "    logging.info(\n",
        "        f\"Test Decision: Q-stat={q_statistic:.4f}, Threshold={decision_threshold:.4f} -> {outcome}\"\n",
        "    )\n",
        "\n",
        "    # Package the final, comprehensive result into the TestDecisionResult NamedTuple.\n",
        "    return TestDecisionResult(\n",
        "        q_statistic=q_statistic,\n",
        "        decision_threshold=decision_threshold,\n",
        "        fail_to_reject_h0=fail_to_reject_h0,\n",
        "        outcome=outcome,\n",
        "        de_randomization_result=de_randomization_result\n",
        "    )\n"
      ],
      "metadata": {
        "id": "zrg3FunnfO-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: Orchestrator Function\n",
        "\n",
        "class EmpiricalAnalysisOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the end-to-end empirical analysis for the randomized alpha test.\n",
        "\n",
        "    This class provides a professional-grade, robust pipeline for executing the\n",
        "    entire testing procedure as described in Massacci, Sarno, Trapani, and\n",
        "    Vallarino. It manages the workflow from raw data ingestion to the final\n",
        "    results compilation, ensuring methodological rigor, reproducibility, and\n",
        "    transparent logging at every step.\n",
        "\n",
        "    The workflow is as follows:\n",
        "    1. Initialization: Validates all inputs, cleans and aligns data, and\n",
        "       pre-computes the rolling window schedule.\n",
        "    2. Execution: The `run_analysis` method iterates through all specified\n",
        "       factor models and rolling windows, applying the full test procedure.\n",
        "    3. Output: Produces a clean, multi-indexed pandas DataFrame containing\n",
        "       the detailed results for every test performed.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        asset_returns: pd.DataFrame,\n",
        "        factor_returns: pd.DataFrame,\n",
        "        replication_config: Dict[str, Any],\n",
        "        seed: Optional[int] = 42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the orchestrator, validates, cleans, and prepares data.\n",
        "\n",
        "        Args:\n",
        "            asset_returns (pd.DataFrame): Raw DataFrame of asset returns.\n",
        "            factor_returns (pd.DataFrame): Raw DataFrame of factor returns.\n",
        "            replication_config (Dict[str, Any]): The study's configuration dict.\n",
        "            seed (Optional[int]): A global seed for all random operations to\n",
        "                                  ensure full reproducibility of the analysis.\n",
        "        \"\"\"\n",
        "        # --- Initialization and State Setup ---\n",
        "        logger.info(\"Initializing EmpiricalAnalysisOrchestrator...\")\n",
        "\n",
        "        # Create a master random number generator from the seed and store it.\n",
        "        # This ensures that the entire analysis, including all de-randomization\n",
        "        # loops, is fully reproducible.\n",
        "        self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Store a deep copy of the configuration to prevent external modifications.\n",
        "        self.config = copy.deepcopy(replication_config)\n",
        "\n",
        "        # --- Data Validation, Cleansing, and Setup ---\n",
        "        # Note: In a real package, the functions would be imported. Here we assume\n",
        "        # they are available in the execution context.\n",
        "\n",
        "        # Step 1: Validate raw inputs. This is a critical first gate.\n",
        "        logger.info(\"Validating raw input data and configuration...\")\n",
        "        validate_inputs(asset_returns, factor_returns, self.config)\n",
        "\n",
        "        # Step 2: Clean and align the data.\n",
        "        logger.info(\"Cleaning and aligning input data...\")\n",
        "        # The clean_and_align_data function handles missing values, outliers,\n",
        "        # and temporal alignment, returning the analysis-ready datasets.\n",
        "        self.asset_returns, self.factor_returns, self.cleansing_log = \\\n",
        "            clean_and_align_data(\n",
        "                asset_returns,\n",
        "                factor_returns,\n",
        "                # These parameters would ideally be in the config, but we use\n",
        "                # robust defaults as specified in the task description.\n",
        "                min_obs_frac_col=0.8,\n",
        "                min_obs_frac_row=0.5,\n",
        "                max_consecutive_fill=2,\n",
        "                winsorize_quantiles=(0.01, 0.99),\n",
        "                min_obs_for_analysis=self.config['empirical']['run_controls']['rolling_window_size_months']\n",
        "            )\n",
        "\n",
        "        # Step 3: Set up the rolling window schedule.\n",
        "        logger.info(\"Setting up rolling window analysis schedule...\")\n",
        "        # This function generates the list of (start, end) date tuples.\n",
        "        self.windows = setup_empirical_analysis_windows(\n",
        "            self.asset_returns.index,\n",
        "            self.config\n",
        "        )\n",
        "\n",
        "        logger.info(\n",
        "            f\"Orchestrator initialized successfully. \"\n",
        "            f\"Data shape: {self.asset_returns.shape}. \"\n",
        "            f\"Number of windows: {len(self.windows)}.\"\n",
        "        )\n",
        "\n",
        "    def _run_test_for_single_window(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        start_date: pd.Timestamp,\n",
        "        end_date: pd.Timestamp\n",
        "    ) -> Optional[Tuple[TestDecisionResult, int, int]]:\n",
        "        \"\"\"\n",
        "        Executes the full test procedure for a single model in a single window.\n",
        "\n",
        "        This is the core computational unit, performing estimation, statistic\n",
        "        computation, de-randomization, and decision making.\n",
        "\n",
        "        Args:\n",
        "            model_name (str): The name of the factor model to test.\n",
        "            start_date (pd.Timestamp): The start date of the window.\n",
        "            end_date (pd.Timestamp): The end date of the window.\n",
        "\n",
        "        Returns:\n",
        "            An optional tuple containing the TestDecisionResult and the\n",
        "            dimensions (N, T) used in the test. Returns None if the window\n",
        "            data is insufficient for testing.\n",
        "        \"\"\"\n",
        "        # Extract the list of factor names for the specified model.\n",
        "        factor_names = self.config['empirical']['model_specifications'][model_name]\n",
        "\n",
        "        # Slice the master dataframes for the current window.\n",
        "        asset_window_raw = self.asset_returns.loc[start_date:end_date]\n",
        "        factor_window = self.factor_returns.loc[start_date:end_date, factor_names]\n",
        "\n",
        "        # CRITICAL STEP: Drop assets with any missing data *within this specific window*.\n",
        "        # This ensures a balanced panel for the regression.\n",
        "        asset_window = asset_window_raw.dropna(axis=1, how='any')\n",
        "\n",
        "        # Get the final dimensions (T, N, K) for this specific test.\n",
        "        T, N = asset_window.shape\n",
        "        _T, K = factor_window.shape\n",
        "\n",
        "        # Check if the remaining data is sufficient for OLS.\n",
        "        # We need more observations (T) than regressors (K+1) and at least one asset.\n",
        "        if N == 0 or T < K + 2:\n",
        "            logger.warning(\n",
        "                f\"Skipping window {end_date.date()} for model {model_name} due to \"\n",
        "                f\"insufficient data after window-specific cleaning (N={N}, T={T}, K={K}).\"\n",
        "            )\n",
        "            return None\n",
        "\n",
        "        # Extract the specific test parameters from the configuration.\n",
        "        test_params = self.config['empirical']['test_config']['our_test']\n",
        "        nu = test_params['nu']\n",
        "        tau = test_params['de_randomization']['tau']\n",
        "        b_function = test_params['de_randomization']['B_function']\n",
        "        f_b_function = test_params['de_randomization']['f_B_function']\n",
        "\n",
        "        # --- Execute the Full Testing Pipeline (Tasks 4-8) ---\n",
        "\n",
        "        # Task 4: Estimate model parameters.\n",
        "        estimation_result = estimate_factor_model_params(asset_window, factor_window)\n",
        "\n",
        "        # Task 5: Compute the psi statistic.\n",
        "        psi_values = compute_psi_statistic(estimation_result, nu=nu)\n",
        "\n",
        "        # Task 7: Perform the de-randomization procedure.\n",
        "        de_rand_result = perform_de_randomization(\n",
        "            psi_values, tau=tau, b_function=b_function, rng=self.rng\n",
        "        )\n",
        "\n",
        "        # Task 8: Make the final test decision.\n",
        "        decision = make_test_decision(de_rand_result, f_b_function)\n",
        "\n",
        "        return decision, N, T\n",
        "\n",
        "    def run_analysis(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Executes the full empirical analysis across all models and windows.\n",
        "\n",
        "        This method iterates through all model specifications and all rolling\n",
        "        windows, calling the testing procedure for each combination and\n",
        "        compiling the results into a final, structured DataFrame.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A multi-indexed DataFrame containing the detailed\n",
        "                          results of every test performed. The index levels\n",
        "                          are 'model' and 'window_end_date'.\n",
        "        \"\"\"\n",
        "        # Retrieve model specifications from the configuration.\n",
        "        model_specs = self.config['empirical']['model_specifications']\n",
        "        # Initialize an empty list to store the results dictionaries.\n",
        "        results_list = []\n",
        "\n",
        "        logger.info(\"--- Starting Full Empirical Analysis ---\")\n",
        "        # Outer loop: Iterate through each factor model specification with a progress bar.\n",
        "        for model_name in tqdm(model_specs.keys(), desc=\"Processing Models\"):\n",
        "\n",
        "            # Inner loop: Iterate through each rolling window with a nested progress bar.\n",
        "            for start_date, end_date in tqdm(self.windows, desc=f\"  - Windows for {model_name}\", leave=False):\n",
        "                try:\n",
        "                    # Run the full test procedure for this specific model and window.\n",
        "                    result_tuple = self._run_test_for_single_window(\n",
        "                        model_name, start_date, end_date\n",
        "                    )\n",
        "\n",
        "                    # If the window was skipped due to insufficient data, continue.\n",
        "                    if result_tuple is None:\n",
        "                        continue\n",
        "\n",
        "                    # Unpack the results.\n",
        "                    decision_result, N, T = result_tuple\n",
        "\n",
        "                    # Compile a dictionary of results for this run.\n",
        "                    results_list.append({\n",
        "                        'model': model_name,\n",
        "                        'window_end_date': end_date,\n",
        "                        'N': N,\n",
        "                        'T': T,\n",
        "                        'q_statistic': decision_result.q_statistic,\n",
        "                        'decision_threshold': decision_result.decision_threshold,\n",
        "                        'reject_h0': not decision_result.fail_to_reject_h0,\n",
        "                        'outcome': decision_result.outcome,\n",
        "                        'critical_value': decision_result.de_randomization_result.critical_value,\n",
        "                        'B': decision_result.de_randomization_result.b_replications\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    # Log any unexpected errors that occur for a specific window and continue.\n",
        "                    # This makes the entire analysis robust to failures in isolated windows.\n",
        "                    logger.error(\n",
        "                        f\"Failed processing window {end_date.date()} for model {model_name}: {e}\",\n",
        "                        exc_info=True # Include traceback for debugging\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "        # Check if any results were successfully generated.\n",
        "        if not results_list:\n",
        "            logger.warning(\"Analysis completed but produced no results. Check data and configuration.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Convert the list of dictionaries into a structured DataFrame.\n",
        "        results_df = pd.DataFrame(results_list)\n",
        "        # Set a multi-index for easy slicing and analysis.\n",
        "        results_df.set_index(['model', 'window_end_date'], inplace=True)\n",
        "\n",
        "        logger.info(f\"--- Full Empirical Analysis Complete. Generated {len(results_df)} results. ---\")\n",
        "\n",
        "        # Return the final, sorted results DataFrame.\n",
        "        return results_df.sort_index()\n"
      ],
      "metadata": {
        "id": "VA5Syrp2gFyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: Robustness Analysis\n",
        "\n",
        "def run_robustness_analysis(\n",
        "    asset_returns: pd.DataFrame,\n",
        "    factor_returns: pd.DataFrame,\n",
        "    base_config: Dict[str, Any],\n",
        "    parameter_grid: Dict[str, List[Any]]\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Performs a robustness analysis by re-running the empirical test with\n",
        "    different parameter configurations.\n",
        "\n",
        "    This function systematically varies key test parameters (like nu and f(B)),\n",
        "    runs the full analysis for each combination, and then computes metrics to\n",
        "    assess the stability of the results.\n",
        "\n",
        "    Args:\n",
        "        asset_returns (pd.DataFrame): Raw DataFrame of asset returns.\n",
        "        factor_returns (pd.DataFrame): Raw DataFrame of factor returns.\n",
        "        base_config (Dict[str, Any]): The baseline configuration dictionary.\n",
        "        parameter_grid (Dict[str, List[Any]]): A dictionary defining the\n",
        "            parameter variations to test. Keys are parameter names (e.g., 'nu')\n",
        "            and values are lists of alternative values.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing:\n",
        "            'rejection_rates_df': A DataFrame comparing model rejection rates\n",
        "                                  across all tested parameter sets.\n",
        "            'correlation_matrix': A DataFrame showing the Pearson correlation\n",
        "                                  of rejection rates between parameter sets.\n",
        "    \"\"\"\n",
        "    # --- Step 1: Parameter Sensitivity Testing ---\n",
        "    logger.info(\"--- Starting Robustness Analysis ---\")\n",
        "\n",
        "    # Generate all combinations of parameters from the provided grid.\n",
        "    param_keys = parameter_grid.keys()\n",
        "    param_values = parameter_grid.values()\n",
        "    param_combinations = list(itertools.product(*param_values))\n",
        "\n",
        "    # Dictionary to store the results DataFrame from each run.\n",
        "    all_results = {}\n",
        "\n",
        "    # Loop through each unique combination of parameters.\n",
        "    for combo in param_combinations:\n",
        "        # Create a descriptive name for this parameter set.\n",
        "        param_set_name = \", \".join([f\"{key}={val}\" for key, val in zip(param_keys, combo)])\n",
        "\n",
        "        # For callables, use the function's name or a description.\n",
        "        param_set_name = param_set_name.replace('<lambda>', 'lambda')\n",
        "        logger.info(f\"Running analysis for parameter set: {param_set_name}\")\n",
        "\n",
        "        # Create a deep copy of the base configuration to modify.\n",
        "        run_config = copy.deepcopy(base_config)\n",
        "\n",
        "        # Update the configuration with the current parameter combination.\n",
        "        # This uses a simplified path update; a real implementation might need\n",
        "        # a more robust nested dictionary update function.\n",
        "        test_config_path = run_config['empirical']['test_config']['our_test']\n",
        "        for key, value in zip(param_keys, combo):\n",
        "            if key == 'f_b_function_str': # Special handling for function strings\n",
        "                test_config_path['de_randomization']['f_B_function'] = eval(value)\n",
        "            else:\n",
        "                test_config_path[key] = value\n",
        "\n",
        "        try:\n",
        "            # Instantiate and run the orchestrator with the modified config.\n",
        "            # A unique seed is used for each run to ensure that while the overall\n",
        "            # analysis is reproducible, the robustness check isn't biased by\n",
        "            # using the exact same random numbers for different parameters.\n",
        "            orchestrator = EmpiricalAnalysisOrchestrator(\n",
        "                asset_returns=asset_returns,\n",
        "                factor_returns=factor_returns,\n",
        "                replication_config=run_config,\n",
        "                seed=hash(param_set_name) % (2**32) # Create a deterministic seed\n",
        "            )\n",
        "            results_df = orchestrator.run_analysis()\n",
        "\n",
        "            # Store the results if the analysis was successful.\n",
        "            if not results_df.empty:\n",
        "                all_results[param_set_name] = results_df\n",
        "            else:\n",
        "                logger.warning(f\"Analysis for '{param_set_name}' produced no results.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(\n",
        "                f\"Robustness run failed for parameter set '{param_set_name}': {e}\",\n",
        "                exc_info=True\n",
        "            )\n",
        "            continue\n",
        "\n",
        "    if not all_results:\n",
        "        raise RuntimeError(\"Robustness analysis failed to produce any valid results.\")\n",
        "\n",
        "    # --- Step 2: Robustness Metric Calculation ---\n",
        "    logger.info(\"--- Calculating Robustness Metrics ---\")\n",
        "\n",
        "    # Calculate the overall rejection rate for each model under each param set.\n",
        "    rejection_rates = {}\n",
        "    for param_set_name, results_df in all_results.items():\n",
        "        # Group by model and calculate the mean of the boolean 'reject_h0' column.\n",
        "        rejection_rates[param_set_name] = results_df.groupby('model')['reject_h0'].mean()\n",
        "\n",
        "    # Combine all rejection rate Series into a single DataFrame.\n",
        "    rejection_rates_df = pd.DataFrame(rejection_rates)\n",
        "\n",
        "    # Calculate the Pearson correlation matrix of the rejection rates.\n",
        "    # High correlation indicates robust conclusions.\n",
        "    correlation_matrix = rejection_rates_df.corr(method='pearson')\n",
        "\n",
        "    # Calculate the standard deviation of rejection rates across parameter sets for each model.\n",
        "    # Low std dev indicates stable absolute rejection rates.\n",
        "    rejection_rates_df['std_dev'] = rejection_rates_df.std(axis=1)\n",
        "\n",
        "    logger.info(\"--- Robustness Analysis Complete ---\")\n",
        "\n",
        "    return {\n",
        "        'rejection_rates_df': rejection_rates_df,\n",
        "        'correlation_matrix': correlation_matrix\n",
        "    }\n"
      ],
      "metadata": {
        "id": "hTykEWIZj62E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: Results Compilation\n",
        "\n",
        "def compile_empirical_results(\n",
        "    results_df: pd.DataFrame,\n",
        "    replication_config: Dict[str, Any]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compiles the raw, window-by-window results into a summary table.\n",
        "\n",
        "    This function calculates the rejection frequencies for the null hypothesis\n",
        "    of zero alpha for each factor model. It computes the overall rejection\n",
        "    rate across the full sample period and also provides a breakdown for\n",
        "    specific, economically significant subperiods (e.g., financial crises)\n",
        "    as defined in the configuration. The output format is designed to mirror\n",
        "    Table 6.1 in the source paper.\n",
        "\n",
        "    Args:\n",
        "        results_df (pd.DataFrame): The multi-indexed DataFrame of detailed\n",
        "            results produced by the EmpiricalAnalysisOrchestrator.\n",
        "        replication_config (Dict[str, Any]): The study's configuration\n",
        "            dictionary, used to retrieve the crisis period definitions.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A summary DataFrame where rows represent different sample\n",
        "                      periods (Full Sample and crises) and columns represent\n",
        "                      the factor models. Cell values are the rejection rates.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the results_df is empty or lacks the required structure.\n",
        "        KeyError: If the configuration dictionary is missing the required\n",
        "                  'crisis_periods' section.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input DataFrame is empty.\n",
        "    if results_df.empty:\n",
        "        logger.warning(\"Input results_df is empty. Returning an empty DataFrame.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Verify the required index levels and columns are present.\n",
        "    required_index = ['model', 'window_end_date']\n",
        "    required_cols = ['reject_h0']\n",
        "    if not all(level in results_df.index.names for level in required_index):\n",
        "        raise ValueError(f\"results_df must have index levels: {required_index}\")\n",
        "    if not all(col in results_df.columns for col in required_cols):\n",
        "        raise ValueError(f\"results_df must have columns: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Calculate Overall Rejection Rates ---\n",
        "    logger.info(\"Calculating overall rejection rates for the full sample.\")\n",
        "\n",
        "    # Group by the 'model' index level and calculate the mean of the boolean\n",
        "    # 'reject_h0' column. This gives the rejection frequency.\n",
        "    full_sample_rates = results_df.groupby(level='model')['reject_h0'].mean()\n",
        "\n",
        "    # Store the full sample results in a dictionary to build the final table.\n",
        "    summary_results = {'Full Sample': full_sample_rates}\n",
        "\n",
        "    # --- Step 2: Calculate Rejection Rates for Crisis Periods ---\n",
        "    try:\n",
        "        # Extract the crisis period definitions from the configuration.\n",
        "        crisis_periods = replication_config['empirical']['reporting']['crisis_periods']\n",
        "    except KeyError:\n",
        "        logger.error(\"Missing 'crisis_periods' in replication_config['empirical']['reporting'].\")\n",
        "        raise KeyError(\"Configuration must contain crisis period definitions.\")\n",
        "\n",
        "    logger.info(f\"Calculating rejection rates for {len(crisis_periods)} defined subperiods.\")\n",
        "\n",
        "    # Get the 'window_end_date' from the index for efficient slicing.\n",
        "    window_end_dates = results_df.index.get_level_values('window_end_date')\n",
        "\n",
        "    # Iterate through each defined crisis period.\n",
        "    for period_name, (start_str, end_str) in crisis_periods.items():\n",
        "        # Parse the date strings into Timestamp objects.\n",
        "        start_date = pd.to_datetime(start_str)\n",
        "        end_date = pd.to_datetime(end_str)\n",
        "\n",
        "        # Create a boolean mask to select windows ending within this period.\n",
        "        period_mask = (window_end_dates >= start_date) & (window_end_dates <= end_date)\n",
        "\n",
        "        # Slice the results DataFrame using the mask.\n",
        "        period_results_df = results_df[period_mask]\n",
        "\n",
        "        # Check if any windows fall within this period.\n",
        "        if period_results_df.empty:\n",
        "            logger.warning(f\"No windows found for the period '{period_name}'. \"\n",
        "                           f\"Results for this period will be NaN.\")\n",
        "            # Create a Series of NaNs to ensure consistent table structure.\n",
        "            period_rates = pd.Series(np.nan, index=full_sample_rates.index)\n",
        "        else:\n",
        "            # If data exists, calculate rejection rates for this subperiod.\n",
        "            period_rates = period_results_df.groupby(level='model')['reject_h0'].mean()\n",
        "\n",
        "        # Add the results for this period to the summary dictionary.\n",
        "        summary_results[period_name] = period_rates\n",
        "\n",
        "    # --- Step 3: Assemble the Final Summary DataFrame ---\n",
        "\n",
        "    # Convert the dictionary of results into a single DataFrame.\n",
        "    # The `orient='index'` argument makes the dictionary keys the rows.\n",
        "    summary_df = pd.DataFrame(summary_results).T\n",
        "\n",
        "    # Ensure the column order matches the model specifications in the config.\n",
        "    model_order = list(replication_config['empirical']['model_specifications'].keys())\n",
        "    # Reorder columns, dropping any that might not be in the results (if a model failed completely).\n",
        "    summary_df = summary_df.reindex(columns=[m for m in model_order if m in summary_df.columns])\n",
        "\n",
        "    logger.info(\"Successfully compiled summary results table.\")\n",
        "\n",
        "    return summary_df\n"
      ],
      "metadata": {
        "id": "9dBKheEXk_sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: Visualization\n",
        "\n",
        "def plot_q_statistic_time_series(\n",
        "    results_df: pd.DataFrame,\n",
        "    replication_config: Dict[str, Any],\n",
        "    title: str = \"Time Series of De-Randomized Q-Statistics\",\n",
        "    figsize: Tuple[int, int] = (15, 8)\n",
        ") -> Tuple[plt.Figure, plt.Axes]:\n",
        "    \"\"\"\n",
        "    Creates a publication-quality plot of the Q-statistic time series.\n",
        "\n",
        "    This function visualizes the results of the empirical analysis, replicating\n",
        "    the style of Figure 6.1 from the source paper. It plots the time series\n",
        "    of the Q-statistic for each factor model, includes the decision threshold\n",
        "    as a horizontal line, and shades the specified crisis periods.\n",
        "\n",
        "    Args:\n",
        "        results_df (pd.DataFrame): The multi-indexed DataFrame of detailed\n",
        "            results from the EmpiricalAnalysisOrchestrator.\n",
        "        replication_config (Dict[str, Any]): The study's configuration\n",
        "            dictionary, used for model ordering and crisis period definitions.\n",
        "        title (str): The title for the plot.\n",
        "        figsize (Tuple[int, int]): The size of the figure in inches.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the matplotlib Figure and Axes objects, allowing for\n",
        "        further customization or saving.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the results_df is empty or lacks the required structure.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    # Check if the input DataFrame is empty.\n",
        "    if results_df.empty:\n",
        "        raise ValueError(\"Cannot generate plot from an empty results DataFrame.\")\n",
        "    # Verify the required index levels and columns are present.\n",
        "    required_index = ['model', 'window_end_date']\n",
        "    required_cols = ['q_statistic', 'decision_threshold']\n",
        "    if not all(level in results_df.index.names for level in required_index):\n",
        "        raise ValueError(f\"results_df must have index levels: {required_index}\")\n",
        "    if not all(col in results_df.columns for col in required_cols):\n",
        "        raise ValueError(f\"results_df must have columns: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Reshape Data for Plotting ---\n",
        "    logger.info(\"Reshaping results data for visualization.\")\n",
        "\n",
        "    # Select the q_statistic column for plotting.\n",
        "    q_stats = results_df['q_statistic']\n",
        "\n",
        "    # Unstack the 'model' level to create a wide-format DataFrame where\n",
        "    # columns are models and the index is the date.\n",
        "    q_stats_wide = q_stats.unstack(level='model')\n",
        "\n",
        "    # Ensure the plotting order of models is consistent with the configuration.\n",
        "    model_order = list(replication_config['empirical']['model_specifications'].keys())\n",
        "    q_stats_wide = q_stats_wide.reindex(columns=[m for m in model_order if m in q_stats_wide.columns])\n",
        "\n",
        "    # --- Step 2: Create the Plot ---\n",
        "    logger.info(\"Generating the Q-statistic time series plot.\")\n",
        "\n",
        "    # Initialize the matplotlib figure and axes with a professional style.\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot the time series of the Q-statistic for each model.\n",
        "    q_stats_wide.plot(ax=ax, lw=1.5)\n",
        "\n",
        "    # Calculate the average decision threshold to plot as a single line.\n",
        "    # This is a reasonable simplification as N varies only slightly across windows.\n",
        "    avg_threshold = results_df['decision_threshold'].mean()\n",
        "    # Plot the decision threshold as a horizontal dashed line.\n",
        "    ax.axhline(\n",
        "        y=avg_threshold,\n",
        "        color='black',\n",
        "        linestyle='--',\n",
        "        linewidth=1.0,\n",
        "        label=f'Decision Threshold (Avg ≈ {avg_threshold:.3f})'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Add Contextual Shading for Crisis Periods ---\n",
        "    # Extract crisis period definitions from the configuration.\n",
        "    crisis_periods = replication_config['empirical']['reporting']['crisis_periods']\n",
        "    # Use a semi-transparent grey for shading.\n",
        "    shade_color = 'grey'\n",
        "    shade_alpha = 0.2\n",
        "\n",
        "    # Iterate through each crisis period and add a shaded region to the plot.\n",
        "    for period_name, (start_str, end_str) in crisis_periods.items():\n",
        "        # Convert date strings to Timestamp objects.\n",
        "        start_date = pd.to_datetime(start_str)\n",
        "        end_date = pd.to_datetime(end_str)\n",
        "        # Add the vertical shaded span to the axes.\n",
        "        ax.axvspan(start_date, end_date, color=shade_color, alpha=shade_alpha, zorder=0)\n",
        "\n",
        "    # --- Step 4: Final Formatting for Publication Quality ---\n",
        "\n",
        "    # Set the title and axis labels.\n",
        "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
        "    ax.set_ylabel(\"Q-Statistic\", fontsize=12)\n",
        "    ax.set_xlabel(None) # The date axis is self-explanatory.\n",
        "\n",
        "    # Set the y-axis limits to be between 0 and 1, as Q is a proportion.\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "    # Format the x-axis to display years clearly.\n",
        "    ax.xaxis.set_major_locator(mdates.YearLocator(5)) # Ticks every 5 years\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.setp(ax.get_xticklabels(), rotation=0, ha='center')\n",
        "\n",
        "    # Configure the legend.\n",
        "    ax.legend(title='Factor Model', loc='upper left', bbox_to_anchor=(1.02, 1))\n",
        "\n",
        "    # Adjust layout to prevent the legend from being cut off.\n",
        "    fig.tight_layout(rect=[0, 0, 0.9, 1]) # Make space for legend on the right\n",
        "\n",
        "    logger.info(\"Plot generation complete.\")\n",
        "\n",
        "    return fig, ax\n"
      ],
      "metadata": {
        "id": "hLWtuW2qlnHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Monte Carlo Simulation Setup\n",
        "\n",
        "class SimulationExperiment(NamedTuple):\n",
        "    \"\"\"\n",
        "    A structured container for a single Monte Carlo experiment's configuration.\n",
        "\n",
        "    This object holds all necessary parameters to run one specific simulation\n",
        "    cell (e.g., a given scenario with a specific N and T). Using a NamedTuple\n",
        "    ensures immutability and provides clear attribute access.\n",
        "\n",
        "    Attributes:\n",
        "        experiment_id (str): A unique identifier for the experiment.\n",
        "        scenario (str): The name of the simulation scenario.\n",
        "        N (int): The number of assets (cross-sectional dimension).\n",
        "        T (int): The number of time periods.\n",
        "        M_replications (int): The number of Monte Carlo replications to run.\n",
        "        dgp_params (Dict[str, Any]): The fully specified parameters for the\n",
        "                                     Data Generation Process.\n",
        "        hypothesis_params (Dict[str, Any]): Parameters defining the null and\n",
        "                                             alternative hypotheses.\n",
        "        test_config (Dict[str, Any]): Configuration for the statistical tests.\n",
        "    \"\"\"\n",
        "    experiment_id: str\n",
        "    scenario: str\n",
        "    N: int\n",
        "    T: int\n",
        "    M_replications: int\n",
        "    dgp_params: Dict[str, Any]\n",
        "    hypothesis_params: Dict[str, Any]\n",
        "    test_config: Dict[str, Any]\n",
        "\n",
        "\n",
        "def _configure_scenario(\n",
        "    base_dgp_params: Dict[str, Any],\n",
        "    scenario: str,\n",
        "    N: int\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Applies scenario-specific modifications to the base DGP parameters.\n",
        "\n",
        "    Args:\n",
        "        base_dgp_params (Dict[str, Any]): The base DGP parameters.\n",
        "        scenario (str): The name of the scenario to configure.\n",
        "        N (int): The number of assets for this experiment.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: The modified DGP parameters for the specified scenario.\n",
        "    \"\"\"\n",
        "    # Start with a deep copy to avoid modifying the base configuration.\n",
        "    dgp_params = copy.deepcopy(base_dgp_params)\n",
        "\n",
        "    # The 'main' scenario includes persistence in the omitted factor.\n",
        "    dgp_params['omitted_factor_persistence'] = 0.4\n",
        "\n",
        "    # Apply modifications based on the scenario name.\n",
        "    if scenario == 'no_persistence':\n",
        "        # As per Appendix A.1, set the omitted factor persistence (phi_g) to 0.\n",
        "        dgp_params['omitted_factor_persistence'] = 0.0\n",
        "\n",
        "    elif scenario == 'weak_error_factor':\n",
        "        # As per Appendix A.2, a fraction of assets have zero loading on the omitted factor.\n",
        "        # The factor is \"weak\" as it affects only floor(N^0.4) assets.\n",
        "        num_affected = int(np.floor(N**0.4))\n",
        "        dgp_params['omitted_factor_num_affected'] = num_affected\n",
        "\n",
        "    elif scenario == 'semi_strong_error_factor':\n",
        "        # As per Appendix A.2, the factor is \"semi-strong\" (affects floor(N^0.8) assets).\n",
        "        num_affected = int(np.floor(N**0.8))\n",
        "        dgp_params['omitted_factor_num_affected'] = num_affected\n",
        "\n",
        "    elif scenario == 'semi_strong_pricing_factor':\n",
        "        # As per Appendix A.3, some pricing factors are semi-strong.\n",
        "        # Here, we specify that factors 2 and 3 are zero for a fraction of assets.\n",
        "        num_affected = int(np.floor(N**0.8))\n",
        "        dgp_params['pricing_factor_num_affected'] = {\n",
        "            'indices': [1, 2], # Corresponds to Beta_2 and Beta_3\n",
        "            'count': num_affected\n",
        "        }\n",
        "\n",
        "    # The 'main' scenario implies all assets are affected by the omitted factor.\n",
        "    if 'omitted_factor_num_affected' not in dgp_params:\n",
        "        dgp_params['omitted_factor_num_affected'] = N\n",
        "\n",
        "    return dgp_params\n",
        "\n",
        "\n",
        "def setup_monte_carlo_experiments(\n",
        "    replication_config: Dict[str, Any]\n",
        ") -> List[SimulationExperiment]:\n",
        "    \"\"\"\n",
        "    Generates the full list of Monte Carlo experiments to be executed.\n",
        "\n",
        "    This function parses the 'monte_carlo' section of the configuration,\n",
        "    creates the Cartesian product of all parameter grids (N, T, scenarios),\n",
        "    and constructs a list of fully-specified, immutable SimulationExperiment\n",
        "    objects, one for each unique experimental cell.\n",
        "\n",
        "    Args:\n",
        "        replication_config (Dict[str, Any]): The complete study configuration.\n",
        "\n",
        "    Returns:\n",
        "        List[SimulationExperiment]: A list of all simulation experiments to run.\n",
        "\n",
        "    Raises:\n",
        "        KeyError: If the configuration is missing required sections.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- Setting up Monte Carlo Simulation Experiments ---\")\n",
        "\n",
        "    try:\n",
        "        # Extract the main Monte Carlo configuration sections.\n",
        "        mc_config = replication_config['monte_carlo']\n",
        "        controls = mc_config['run_controls']\n",
        "        base_dgp = mc_config['dgp_base_params']\n",
        "        hypotheses = mc_config['hypothesis_params']\n",
        "        tests = mc_config['test_config']\n",
        "\n",
        "        # Extract the parameter grids.\n",
        "        N_grid = controls['N_grid']\n",
        "        T_grid = controls['T_grid']\n",
        "        scenarios = controls['scenarios']\n",
        "        M_replications = controls['M_replications']\n",
        "\n",
        "    except KeyError as e:\n",
        "        logger.error(f\"Configuration missing required Monte Carlo key: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Create the list to hold all experiment configurations.\n",
        "    experiments = []\n",
        "\n",
        "    # Generate the Cartesian product of all experimental parameters.\n",
        "    experiment_grid = list(itertools.product(scenarios, N_grid, T_grid))\n",
        "    logger.info(f\"Generated {len(experiment_grid)} unique experimental cells.\")\n",
        "\n",
        "    # Process each experimental cell to create a full configuration.\n",
        "    for scenario, N, T in experiment_grid:\n",
        "\n",
        "        # --- Theoretical Constraint Check (Assumption 3.1) ---\n",
        "        nu = tests['our_test']['nu']\n",
        "        # The exponent in the theoretical bound N = O(T^exponent)\n",
        "        theoretical_exponent = (nu / 2 - 1) / 2\n",
        "        # We check this for a small epsilon, e.g., 0.01\n",
        "        if (np.log(N) / np.log(T)) > (theoretical_exponent - 0.01):\n",
        "            logger.warning(\n",
        "                f\"Experiment (N={N}, T={T}, nu={nu}) may violate Assumption 3.1. \"\n",
        "                f\"log(N)/log(T)={np.log(N)/np.log(T):.2f} vs. bound≈{theoretical_exponent:.2f}. \"\n",
        "                \"Asymptotic results may not hold.\"\n",
        "            )\n",
        "\n",
        "        # --- Configure Scenario-Specific DGP ---\n",
        "        # This helper function applies the required modifications for the scenario.\n",
        "        dgp_params = _configure_scenario(base_dgp, scenario, N)\n",
        "\n",
        "        # Create a unique ID for this experiment for logging and result tracking.\n",
        "        exp_id = f\"scenario={scenario}_N={N}_T={T}\"\n",
        "\n",
        "        # Create the immutable experiment configuration object.\n",
        "        experiment = SimulationExperiment(\n",
        "            experiment_id=exp_id,\n",
        "            scenario=scenario,\n",
        "            N=N,\n",
        "            T=T,\n",
        "            M_replications=M_replications,\n",
        "            dgp_params=dgp_params,\n",
        "            hypothesis_params=hypotheses,\n",
        "            test_config=tests\n",
        "        )\n",
        "        experiments.append(experiment)\n",
        "\n",
        "    logger.info(f\"Successfully created {len(experiments)} simulation experiment configurations.\")\n",
        "\n",
        "    return experiments\n"
      ],
      "metadata": {
        "id": "ff1nD4TzmdoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Data Generation Process (DGP)\n",
        "\n",
        "class DGPData(NamedTuple):\n",
        "    \"\"\"\n",
        "    Container for a single realization of the Data Generation Process.\n",
        "\n",
        "    Attributes:\n",
        "        asset_returns (np.ndarray): (T x N) array of simulated asset returns.\n",
        "        factor_returns (np.ndarray): (T x K) array of simulated factor returns.\n",
        "    \"\"\"\n",
        "    asset_returns: np.ndarray\n",
        "    factor_returns: np.ndarray\n",
        "\n",
        "def _draw_from_dist(\n",
        "    dist_config: Dict[str, Any],\n",
        "    size: Tuple[int, ...],\n",
        "    rng: np.random.Generator\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Draws random samples from a distribution specified in a configuration dictionary.\n",
        "\n",
        "    This helper function provides a flexible way to generate random numbers from\n",
        "    various distributions (e.g., uniform, normal) based on string identifiers\n",
        "    and parameters provided in the main configuration object.\n",
        "\n",
        "    Args:\n",
        "        dist_config (Dict[str, Any]): A dictionary containing 'type' (e.g., 'uniform')\n",
        "                                     and 'args' (e.g., (low, high)).\n",
        "        size (Tuple[int, ...]): The desired output shape of the random array.\n",
        "        rng (np.random.Generator): The random number generator instance to use.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: An array of random samples with the specified shape.\n",
        "\n",
        "    Raises:\n",
        "        NotImplementedError: If the specified distribution 'type' is not supported.\n",
        "        KeyError: If the dist_config dictionary is missing 'type' or 'args'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract the distribution type (e.g., 'uniform', 'normal').\n",
        "        dist_type = dist_config['type']\n",
        "\n",
        "        # Extract the arguments for the distribution (e.g., (low, high) or (mean, std)).\n",
        "        args = dist_config['args']\n",
        "    except KeyError as e:\n",
        "        # Raise an error if the configuration is malformed.\n",
        "        logger.error(f\"Distribution configuration is missing key: {e}\")\n",
        "        raise KeyError(f\"Invalid distribution configuration: {dist_config}\")\n",
        "\n",
        "    # Generate random numbers based on the specified distribution type.\n",
        "    if dist_type == 'uniform':\n",
        "        # Draw from a uniform distribution.\n",
        "        return rng.uniform(low=args[0], high=args[1], size=size)\n",
        "    elif dist_type == 'normal':\n",
        "        # Draw from a normal (Gaussian) distribution.\n",
        "        return rng.normal(loc=args[0], scale=args[1], size=size)\n",
        "\n",
        "    # Raise an error for any unsupported distribution type.\n",
        "    raise NotImplementedError(f\"Distribution type '{dist_type}' not implemented.\")\n",
        "\n",
        "\n",
        "def generate_dgp_data(\n",
        "    experiment: SimulationExperiment,\n",
        "    is_null_hypothesis: bool,\n",
        "    rng: np.random.Generator\n",
        ") -> DGPData:\n",
        "    \"\"\"\n",
        "    Generates a single, complete dataset according to the Data Generation Process (DGP).\n",
        "\n",
        "    This function is the core of the Monte Carlo simulation. It synthesizes a\n",
        "    dataset of asset and factor returns based on the detailed specifications\n",
        "    provided in the SimulationExperiment object, meticulously following the\n",
        "    stochastic processes outlined in Section 5 and Appendix A of the source paper.\n",
        "\n",
        "    Args:\n",
        "        experiment (SimulationExperiment): The configuration object for this DGP run.\n",
        "        is_null_hypothesis (bool): If True, generate data under the null (all alphas=0).\n",
        "                                   If False, generate under the alternative.\n",
        "        rng (np.random.Generator): The random number generator to use for all stochastic\n",
        "                                   components, ensuring reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        DGPData: A NamedTuple containing the (T x N) asset returns and (T x K)\n",
        "                 factor returns as NumPy arrays.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If DGP parameters are invalid (e.g., non-stationary VAR).\n",
        "        NotImplementedError: If an unsupported error DGP type is specified.\n",
        "    \"\"\"\n",
        "    # --- Step 0: Extract Parameters and Validate ---\n",
        "\n",
        "    # Unpack parameters from the experiment object for clarity and frequent use.\n",
        "    N, T = experiment.N, experiment.T\n",
        "    dgp = experiment.dgp_params\n",
        "    K = dgp['K_factors']\n",
        "\n",
        "    # Define a burn-in period to allow stochastic processes to reach stationarity.\n",
        "    burn_in = 100\n",
        "\n",
        "    # --- Step 1: Generate Factor Process (f_t) ---\n",
        "    logger.debug(f\"Generating {K} factors for T={T} periods...\")\n",
        "\n",
        "    # Equation (5.2): f_t = f̄ + Φ(f_{t-1} - f̄) + ζ_t\n",
        "\n",
        "    # Extract the unconditional mean of the factors.\n",
        "    factor_mean = np.array(dgp['factor_mean'], dtype=np.float64)\n",
        "\n",
        "    # Extract the VAR(1) coefficient matrix (assumed diagonal as per the paper).\n",
        "    phi_matrix = np.diag(dgp['factor_var_matrix']['values'])\n",
        "\n",
        "    # Check for stationarity of the VAR(1) process.\n",
        "    if np.any(np.abs(np.linalg.eigvals(phi_matrix)) >= 1):\n",
        "        raise ValueError(\"Factor VAR(1) process is not stationary (eigenvalue >= 1).\")\n",
        "\n",
        "    # Pre-allocate array for the factor time series, including the burn-in period.\n",
        "    factors = np.zeros((T + burn_in, K), dtype=np.float64)\n",
        "\n",
        "    # Initialize the process at its unconditional mean.\n",
        "    factors[0, :] = factor_mean\n",
        "\n",
        "    # Generate the i.i.d. standard normal innovations for the factor process.\n",
        "    innovations_f = rng.standard_normal(size=(T + burn_in, K))\n",
        "\n",
        "    # Iterate through time to generate the VAR(1) series.\n",
        "    for t in range(1, T + burn_in):\n",
        "        # Apply the VAR(1) update equation.\n",
        "        factors[t, :] = factor_mean + phi_matrix @ (factors[t-1, :] - factor_mean) + innovations_f[t, :]\n",
        "\n",
        "    # Discard the burn-in period to get the final stationary time series.\n",
        "    factor_returns = factors[burn_in:, :]\n",
        "\n",
        "    # --- Step 2: Generate Factor Loadings (β_i for pricing, γ_i for error) ---\n",
        "    logger.debug(f\"Generating factor loadings for N={N} assets...\")\n",
        "\n",
        "    # Pre-allocate the matrix for pricing factor loadings (betas).\n",
        "    betas = np.zeros((N, K), dtype=np.float64)\n",
        "\n",
        "    # Generate loadings for each of the K pricing factors from its specified distribution.\n",
        "    for k in range(K):\n",
        "        betas[:, k] = _draw_from_dist(dgp['factor_loading_dist'][k], (N,), rng)\n",
        "\n",
        "    # Apply scenario modification for semi-strong pricing factors if specified.\n",
        "    if 'pricing_factor_num_affected' in dgp:\n",
        "        # Extract the specification for which factors are semi-strong.\n",
        "        spec = dgp['pricing_factor_num_affected']\n",
        "        # Determine the number of assets that will have zero loadings.\n",
        "        num_zero_loadings = N - spec['count']\n",
        "        # Randomly select the assets that will have zero loadings.\n",
        "        indices_to_zero = rng.choice(N, size=num_zero_loadings, replace=False)\n",
        "        # Set the loadings for the specified factors and assets to zero.\n",
        "        for k in spec['indices']:\n",
        "            betas[indices_to_zero, k] = 0.0\n",
        "\n",
        "    # Generate loadings (gammas) for the common component in the error term.\n",
        "    gammas = _draw_from_dist(dgp['omitted_factor_loading_dist'], (N, 1), rng)\n",
        "\n",
        "    # Apply scenario modification for weak or semi-strong error factors.\n",
        "    num_affected_error = dgp['omitted_factor_num_affected']\n",
        "    if num_affected_error < N:\n",
        "        # Determine the number of assets with zero loadings on the error factor.\n",
        "        num_zero_loadings_error = N - num_affected_error\n",
        "        # Randomly select the assets to have zero loadings.\n",
        "        indices_to_zero_error = rng.choice(N, size=num_zero_loadings_error, replace=False)\n",
        "        # Set their gamma loadings to zero.\n",
        "        gammas[indices_to_zero_error, 0] = 0.0\n",
        "\n",
        "    # --- Step 3: Generate Error Terms (u_i,t) ---\n",
        "    logger.debug(\"Generating error terms...\")\n",
        "\n",
        "    # Equation (5.3): u_t = γ * g_t + ξ_t\n",
        "\n",
        "    # Generate the common error factor (g_t) as an AR(1) process.\n",
        "    phi_g = dgp['omitted_factor_persistence']\n",
        "    g_t = np.zeros(T + burn_in, dtype=np.float64)\n",
        "    innovations_g = rng.standard_normal(size=T + burn_in)\n",
        "    for t in range(1, T + burn_in):\n",
        "        g_t[t] = phi_g * g_t[t-1] + innovations_g[t]\n",
        "    # Reshape after discarding burn-in to be a (T x 1) column vector.\n",
        "    g_t_final = g_t[burn_in:].reshape(-1, 1)\n",
        "\n",
        "    # Generate the T x N matrix of idiosyncratic shocks (ξ_i,t).\n",
        "    # This example uses the first defined error type; a full implementation could loop.\n",
        "    error_spec_key = list(dgp['error_dgp_types'].keys())[0]\n",
        "    error_spec = dgp['error_dgp_types'][error_spec_key]\n",
        "\n",
        "    if error_spec['type'] == 'gaussian':\n",
        "        # Generate standard normal idiosyncratic shocks.\n",
        "        xi = rng.standard_normal(size=(T, N))\n",
        "    elif error_spec['type'] == 'student_t':\n",
        "        # Generate shocks from a Student's t-distribution.\n",
        "        df = error_spec['args']['df']\n",
        "        if df <= 2:\n",
        "            raise ValueError(\"Degrees of freedom for Student's t must be > 2 for finite variance.\")\n",
        "        xi = rng.standard_t(df=df, size=(T, N))\n",
        "        # Standardize the shocks to have unit variance.\n",
        "        xi /= np.sqrt(df / (df - 2))\n",
        "    elif error_spec['type'] == 'garch':\n",
        "        # Generate shocks from a GARCH(1,1) process for each asset.\n",
        "        garch_params = error_spec['args']\n",
        "        omegas = _draw_from_dist(garch_params['omega_dist'], (N,), rng)\n",
        "        alphas_g = _draw_from_dist(garch_params['alpha_dist'], (N,), rng)\n",
        "        betas_g = _draw_from_dist(garch_params['beta_dist'], (N,), rng)\n",
        "\n",
        "        # Check GARCH stationarity condition.\n",
        "        if np.any(alphas_g + betas_g >= 1):\n",
        "            logger.warning(\"Some GARCH processes may be non-stationary (alpha+beta >= 1).\")\n",
        "\n",
        "        xi = np.zeros((T, N), dtype=np.float64)\n",
        "        h_sq = np.ones((T, N), dtype=np.float64) # Initialize conditional variance\n",
        "        innovations_xi = rng.standard_normal(size=(T, N))\n",
        "\n",
        "        # Iterate over time to generate the GARCH series for all assets.\n",
        "        for t in range(1, T):\n",
        "            # Equation: h_i,t^2 = ω_i + α_i * ξ_{i,t-1}^2 + β_i * h_{i,t-1}^2\n",
        "            h_sq[t, :] = omegas + alphas_g * np.square(xi[t-1, :]) + betas_g * h_sq[t-1, :]\n",
        "        # Calculate the shocks: ξ_i,t = h_i,t * z_i,t\n",
        "        xi = np.sqrt(h_sq) * innovations_xi\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Error DGP type '{error_spec['type']}' not implemented.\")\n",
        "\n",
        "    # Combine the common factor and idiosyncratic shocks to form the final errors.\n",
        "    errors = g_t_final @ gammas.T + xi\n",
        "\n",
        "    # --- Step 4: Generate Alphas and Synthesize Asset Returns ---\n",
        "    logger.debug(\"Generating alphas and synthesizing final asset returns...\")\n",
        "\n",
        "    # Initialize alphas as a vector of zeros (for the null hypothesis).\n",
        "    alphas = np.zeros((N, 1), dtype=np.float64)\n",
        "\n",
        "    # If generating under the alternative, create sparse non-zero alphas.\n",
        "    if not is_null_hypothesis:\n",
        "        alt_params = experiment.hypothesis_params['alternative']\n",
        "        sparsity = alt_params['default_sparsity']\n",
        "        num_nonzero = int(np.floor(N * sparsity))\n",
        "\n",
        "        if num_nonzero > 0:\n",
        "            # Randomly select which assets will have non-zero alpha.\n",
        "            nonzero_indices = rng.choice(N, size=num_nonzero, replace=False)\n",
        "            # Draw the alpha values from the specified distribution.\n",
        "            dist_args = alt_params['dist_args']\n",
        "            alphas[nonzero_indices, 0] = rng.normal(loc=dist_args[0], scale=dist_args[1], size=num_nonzero)\n",
        "\n",
        "    # Synthesize the final T x N asset returns matrix.\n",
        "    # Equation (5.1): y_i,t = α_i + β'_i * f_t + u_i,t\n",
        "    # In matrix form: Y = F @ B.T + U + α.T\n",
        "    asset_returns = factor_returns @ betas.T + errors + alphas.T\n",
        "\n",
        "    # Return the final generated data in the structured container.\n",
        "    return DGPData(\n",
        "        asset_returns=asset_returns,\n",
        "        factor_returns=factor_returns\n",
        "    )\n"
      ],
      "metadata": {
        "id": "PhZk-9MhobSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Monte Carlo Simulation\n",
        "\n",
        "class SimulationResult(NamedTuple):\n",
        "    \"\"\"\n",
        "    Structured container for the results of a full Monte Carlo simulation.\n",
        "\n",
        "    Attributes:\n",
        "        experiment_id (str): The unique identifier for the experiment.\n",
        "        rejection_rate (float): The empirical rejection frequency (size or power).\n",
        "        conf_interval (Tuple[float, float]): The Wilson score confidence\n",
        "                                             interval for the rejection rate.\n",
        "        successful_replications (int): The number of replications that\n",
        "                                       completed without errors.\n",
        "        total_replications (int): The total number of replications attempted.\n",
        "    \"\"\"\n",
        "    experiment_id: str\n",
        "    rejection_rate: float\n",
        "    conf_interval: Tuple[float, float]\n",
        "    successful_replications: int\n",
        "    total_replications: int\n",
        "\n",
        "def _run_single_replication(\n",
        "    experiment: SimulationExperiment,\n",
        "    is_null_hypothesis: bool,\n",
        "    seed: int\n",
        ") -> Optional[bool]:\n",
        "    \"\"\"\n",
        "    Executes a single replication of the Monte Carlo simulation.\n",
        "\n",
        "    This involves generating one dataset and running the full testing pipeline on it.\n",
        "\n",
        "    Args:\n",
        "        experiment (SimulationExperiment): The configuration for the experiment.\n",
        "        is_null_hypothesis (bool): Flag to generate data under H0 or HA.\n",
        "        seed (int): A unique seed for this specific replication.\n",
        "\n",
        "    Returns:\n",
        "        Optional[bool]: True if H0 was rejected, False otherwise. None on failure.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a dedicated RNG for this replication to ensure independence.\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Task 14: Generate one dataset.\n",
        "        dgp_data = generate_dgp_data(experiment, is_null_hypothesis, rng)\n",
        "\n",
        "        # Convert numpy arrays to the pandas DataFrames required by the test pipeline.\n",
        "        # A dummy DatetimeIndex is sufficient for the simulation.\n",
        "        date_index = pd.to_datetime(pd.date_range(start='2000-01-01', periods=experiment.T, freq='M'))\n",
        "        asset_returns_df = pd.DataFrame(dgp_data.asset_returns, index=date_index)\n",
        "        factor_returns_df = pd.DataFrame(dgp_data.factor_returns, index=date_index)\n",
        "\n",
        "        # --- Execute Full Test Pipeline (Tasks 4-8) ---\n",
        "        test_config = experiment.test_config['our_test']\n",
        "\n",
        "        estimation_res = estimate_factor_model_params(asset_returns_df, factor_returns_df)\n",
        "        psi_vals = compute_psi_statistic(estimation_res, nu=test_config['nu'])\n",
        "        de_rand_res = perform_de_randomization(\n",
        "            psi_vals,\n",
        "            tau=test_config['de_randomization']['tau'],\n",
        "            b_function=test_config['de_randomization']['B_function'],\n",
        "            rng=rng # Pass the same RNG to maintain state\n",
        "        )\n",
        "        decision = make_test_decision(\n",
        "            de_rand_res,\n",
        "            f_b_function=test_config['de_randomization']['f_B_function']\n",
        "        )\n",
        "\n",
        "        # Return the rejection decision (True if rejected, False if not).\n",
        "        return not decision.fail_to_reject_h0\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any step fails for this replication, log the error and return None.\n",
        "        logger.error(f\"Replication failed for experiment {experiment.experiment_id}: {e}\", exc_info=False)\n",
        "        return None\n",
        "\n",
        "\n",
        "def run_monte_carlo_simulation(\n",
        "    experiment: SimulationExperiment,\n",
        "    is_null_hypothesis: bool,\n",
        "    n_jobs: int = -1,\n",
        "    seed: Optional[int] = None\n",
        ") -> SimulationResult:\n",
        "    \"\"\"\n",
        "    Runs a full Monte Carlo simulation for a single experimental cell.\n",
        "\n",
        "    This function manages the execution of M replications, distributing the\n",
        "    workload across multiple CPU cores if available. It then aggregates the\n",
        "    results to compute the empirical rejection rate (size or power) and its\n",
        "    confidence interval.\n",
        "\n",
        "    Args:\n",
        "        experiment (SimulationExperiment): The configuration for the experiment.\n",
        "        is_null_hypothesis (bool): If True, calculates empirical size. If False,\n",
        "                                   calculates empirical power.\n",
        "        n_jobs (int): The number of CPU cores to use. -1 means use all available.\n",
        "        seed (Optional[int]): A seed to initialize the random number generator\n",
        "                              for the entire set of replications.\n",
        "\n",
        "    Returns:\n",
        "        SimulationResult: A NamedTuple containing the final simulation results.\n",
        "    \"\"\"\n",
        "    # Extract the number of replications from the experiment config.\n",
        "    M = experiment.M_replications\n",
        "\n",
        "    # Create a master RNG for this experiment.\n",
        "    # It will be used to generate unique, deterministic seeds for each replication.\n",
        "    master_rng = np.random.default_rng(seed)\n",
        "\n",
        "    # Generate M unique seeds for the M replications. This is crucial for\n",
        "    # ensuring that parallel jobs are independently and reproducibly seeded.\n",
        "    replication_seeds = master_rng.integers(low=0, high=2**32 - 1, size=M)\n",
        "\n",
        "    logger.info(\n",
        "        f\"Starting Monte Carlo simulation for '{experiment.experiment_id}' \"\n",
        "        f\"under H{int(not is_null_hypothesis)} with {M} replications on {n_jobs} core(s).\"\n",
        "    )\n",
        "\n",
        "    # Use joblib.Parallel for efficient, parallel execution of the replications.\n",
        "    # The `delayed` function wraps the function and its arguments for lazy evaluation.\n",
        "    results = Parallel(n_jobs=n_jobs)(\n",
        "        delayed(_run_single_replication)(experiment, is_null_hypothesis, seed)\n",
        "        for seed in tqdm(replication_seeds, desc=f\"Simulating {experiment.experiment_id}\")\n",
        "    )\n",
        "\n",
        "    # Filter out any replications that failed (returned None).\n",
        "    successful_results = [res for res in results if res is not None]\n",
        "\n",
        "    # Get the number of successful replications.\n",
        "    num_success = len(successful_results)\n",
        "\n",
        "    # Check if a significant number of replications failed.\n",
        "    if num_success < 0.95 * M:\n",
        "        logger.warning(\n",
        "            f\"High failure rate for experiment '{experiment.experiment_id}': \"\n",
        "            f\"{M - num_success}/{M} replications failed. Results may be unreliable.\"\n",
        "        )\n",
        "\n",
        "    # If no replications succeeded, return a failure result.\n",
        "    if num_success == 0:\n",
        "        logger.error(f\"All {M} replications failed for '{experiment.experiment_id}'.\")\n",
        "        return SimulationResult(experiment.experiment_id, np.nan, (np.nan, np.nan), 0, M)\n",
        "\n",
        "    # Calculate the number of rejections.\n",
        "    num_rejections = sum(successful_results)\n",
        "\n",
        "    # Calculate the empirical rejection rate (size or power).\n",
        "    rejection_rate = num_rejections / num_success\n",
        "\n",
        "    # Calculate the Wilson score confidence interval for the rejection rate.\n",
        "    conf_interval = proportion_confint(\n",
        "        count=num_rejections, nobs=num_success, method='wilson'\n",
        "    )\n",
        "\n",
        "    logger.info(\n",
        "        f\"Simulation for '{experiment.experiment_id}' complete. \"\n",
        "        f\"Rejection Rate: {rejection_rate:.4f} \"\n",
        "        f\"(95% CI: [{conf_interval[0]:.4f}, {conf_interval[1]:.4f}])\"\n",
        "    )\n",
        "\n",
        "    # Return the final, structured result.\n",
        "    return SimulationResult(\n",
        "        experiment_id=experiment.experiment_id,\n",
        "        rejection_rate=rejection_rate,\n",
        "        conf_interval=conf_interval,\n",
        "        successful_replications=num_success,\n",
        "        total_replications=M\n",
        "    )\n"
      ],
      "metadata": {
        "id": "QrO7UPOzrTLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: Monte Carlo Orchestrator Function\n",
        "\n",
        "class MonteCarloOrchestrator:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the entire Monte Carlo simulation study.\n",
        "\n",
        "    This class manages the full lifecycle of the simulation:\n",
        "    1. Initialization: Sets up the complete grid of experiments based on the\n",
        "       provided configuration.\n",
        "    2. Execution: Runs the simulation for each experimental cell, for both the\n",
        "       null and alternative hypotheses, to calculate empirical size and power.\n",
        "    3. Compilation: Gathers all results into a single, tidy DataFrame.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        replication_config: Dict[str, Any],\n",
        "        seed: Optional[int] = 42\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the orchestrator and sets up the experimental plan.\n",
        "\n",
        "        Args:\n",
        "            replication_config (Dict[str, Any]): The complete study configuration.\n",
        "            seed (Optional[int]): A master seed for the random number generator\n",
        "                                  to ensure the entire study is reproducible.\n",
        "        \"\"\"\n",
        "        # Log the start of the initialization process.\n",
        "        logger.info(\"Initializing MonteCarloOrchestrator...\")\n",
        "\n",
        "        # Store a deep copy of the configuration to prevent external modifications.\n",
        "        self.config = copy.deepcopy(replication_config)\n",
        "\n",
        "        # Store the master seed.\n",
        "        self.seed = seed\n",
        "\n",
        "        # Initialize the results attribute to None. It will be populated by the run method.\n",
        "        self.results_df: Optional[pd.DataFrame] = None\n",
        "\n",
        "        try:\n",
        "            # Use the setup function from Task 13 to generate the experimental plan.\n",
        "            self.experiments: List[SimulationExperiment] = setup_monte_carlo_experiments(\n",
        "                self.config\n",
        "            )\n",
        "        except (KeyError, ValueError) as e:\n",
        "            # Catch errors during setup and provide a clear message.\n",
        "            logger.error(f\"Failed to initialize orchestrator during experiment setup: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Log the successful completion of the setup.\n",
        "        logger.info(\n",
        "            f\"Orchestrator initialized with {len(self.experiments)} unique experiments to run.\"\n",
        "        )\n",
        "\n",
        "    def run(self, n_jobs: int = -1) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Executes the full grid of Monte Carlo simulations.\n",
        "\n",
        "        This method iterates through every experiment defined during initialization,\n",
        "        running simulations for both the null (size) and alternative (power)\n",
        "        hypotheses. It leverages parallel processing to speed up execution.\n",
        "\n",
        "        Args:\n",
        "            n_jobs (int): The number of CPU cores to use for parallel execution.\n",
        "                          -1 means use all available cores. 1 means run serially.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: A tidy DataFrame containing the results (size and power)\n",
        "                          for every experimental cell.\n",
        "        \"\"\"\n",
        "        # Log the start of the main execution phase.\n",
        "        logger.info(f\"--- Starting Monte Carlo Simulation Run ({len(self.experiments)} experiments) ---\")\n",
        "\n",
        "        # Initialize an empty list to store the result dictionaries.\n",
        "        results_list = []\n",
        "\n",
        "        # Create a master RNG to generate unique seeds for each experimental cell.\n",
        "        master_rng = np.random.default_rng(self.seed)\n",
        "\n",
        "        # Iterate through the pre-defined list of experiments with a progress bar.\n",
        "        for experiment in tqdm(self.experiments, desc=\"Overall Simulation Progress\"):\n",
        "            try:\n",
        "                # Generate a unique, deterministic seed for this specific experiment cell.\n",
        "                experiment_seed = master_rng.integers(0, 2**32 - 1)\n",
        "\n",
        "                # --- Run simulation under the Null Hypothesis (for Size) ---\n",
        "                size_result = run_monte_carlo_simulation(\n",
        "                    experiment=experiment,\n",
        "                    is_null_hypothesis=True,\n",
        "                    n_jobs=n_jobs,\n",
        "                    seed=experiment_seed\n",
        "                )\n",
        "\n",
        "                # Convert the result to a dictionary and add experiment details.\n",
        "                size_dict = {\n",
        "                    'scenario': experiment.scenario, 'N': experiment.N, 'T': experiment.T,\n",
        "                    'hypothesis': 'size', **size_result._asdict()\n",
        "                }\n",
        "                results_list.append(size_dict)\n",
        "\n",
        "                # --- Run simulation under the Alternative Hypothesis (for Power) ---\n",
        "                power_result = run_monte_carlo_simulation(\n",
        "                    experiment=experiment,\n",
        "                    is_null_hypothesis=False,\n",
        "                    n_jobs=n_jobs,\n",
        "                    seed=experiment_seed + 1 # Use a different seed for the power run\n",
        "                )\n",
        "\n",
        "                # Convert the result to a dictionary and add experiment details.\n",
        "                power_dict = {\n",
        "                    'scenario': experiment.scenario, 'N': experiment.N, 'T': experiment.T,\n",
        "                    'hypothesis': 'power', **power_result._asdict()\n",
        "                }\n",
        "                results_list.append(power_dict)\n",
        "\n",
        "            except Exception as e:\n",
        "                # Log any critical failure for an entire experimental cell and continue.\n",
        "                logger.error(\n",
        "                    f\"A critical error occurred during simulation for experiment \"\n",
        "                    f\"'{experiment.experiment_id}': {e}\",\n",
        "                    exc_info=True\n",
        "                )\n",
        "                continue\n",
        "\n",
        "        # Check if the simulation produced any results.\n",
        "        if not results_list:\n",
        "            logger.error(\"Monte Carlo simulation run completed with no results.\")\n",
        "            self.results_df = pd.DataFrame()\n",
        "            return self.results_df\n",
        "\n",
        "        # Convert the list of dictionaries into a final, tidy DataFrame.\n",
        "        self.results_df = pd.DataFrame(results_list)\n",
        "\n",
        "        # Log the successful completion of the entire simulation study.\n",
        "        logger.info(\"--- Monte Carlo Simulation Run Complete ---\")\n",
        "\n",
        "        return self.results_df\n"
      ],
      "metadata": {
        "id": "uPpG8ITysckd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Monte Carlo Robustness Analysis\n",
        "\n",
        "def _run_fly_test(\n",
        "    asset_returns_df: pd.DataFrame,\n",
        "    factor_returns_df: pd.DataFrame,\n",
        "    tau: float\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Implements a high-dimensional GRS-style test inspired by Fan, Liao, Yao (2015).\n",
        "\n",
        "    This function provides a functional implementation of a test for the null\n",
        "    hypothesis that all alphas are jointly zero, designed for high-dimensional\n",
        "    settings (N > T). It uses a regularized (pseudo-inverse) estimate of the\n",
        "    residual covariance matrix to ensure stability.\n",
        "\n",
        "    Args:\n",
        "        asset_returns_df (pd.DataFrame): A (T x N) DataFrame of asset returns.\n",
        "        factor_returns_df (pd.DataFrame): A (T x K) DataFrame of factor returns.\n",
        "        tau (float): The significance level for the hypothesis test.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the null hypothesis is rejected, False otherwise.\n",
        "    \"\"\"\n",
        "    # Step 1: Perform OLS to obtain estimated alphas and residuals.\n",
        "    estimation_result = estimate_factor_model_params(asset_returns_df, factor_returns_df)\n",
        "\n",
        "    # Extract estimated alphas as a NumPy array.\n",
        "    alphas = estimation_result.alphas.to_numpy()\n",
        "\n",
        "    # Extract residuals as a NumPy array.\n",
        "    residuals = estimation_result.residuals.to_numpy()\n",
        "\n",
        "    # Get the dimensions of the problem.\n",
        "    T, N = residuals.shape\n",
        "\n",
        "    # Step 2: Estimate the residual covariance matrix.\n",
        "    sigma_u = np.cov(residuals, rowvar=False)\n",
        "\n",
        "    # Use the Moore-Penrose pseudo-inverse for numerical stability, which is crucial when N > T.\n",
        "    sigma_u_inv = np.linalg.pinv(sigma_u)\n",
        "\n",
        "    # Step 3: Calculate the GRS-style test statistic (Wald test for alphas).\n",
        "    # Equation: J = T * α' * Σ_u^(-1) * α\n",
        "    test_statistic = T * (alphas.T @ sigma_u_inv @ alphas)\n",
        "\n",
        "    # Step 4: Determine the critical value from the asymptotic Chi-squared distribution.\n",
        "    # Under H0, the statistic is asymptotically χ² with N degrees of freedom.\n",
        "    critical_value = chi2.ppf(1 - tau, df=N)\n",
        "\n",
        "    # The null is rejected if the test statistic exceeds the critical value.\n",
        "    return test_statistic > critical_value\n",
        "\n",
        "def _run_as_test(\n",
        "    asset_returns_df: pd.DataFrame,\n",
        "    factor_returns_df: pd.DataFrame,\n",
        "    tau: float\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Implements the Ardia and Sessinou (2024) Cauchy combination test.\n",
        "\n",
        "    This method tests the joint null of zero alphas by running individual t-tests\n",
        "    for each asset's alpha and then combining the p-values using the robust\n",
        "    Cauchy combination method.\n",
        "\n",
        "    Args:\n",
        "        asset_returns_df (pd.DataFrame): A (T x N) DataFrame of asset returns.\n",
        "        factor_returns_df (pd.DataFrame): A (T x K) DataFrame of factor returns.\n",
        "        tau (float): The significance level for the hypothesis test.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the null hypothesis is rejected, False otherwise.\n",
        "    \"\"\"\n",
        "    # Get the dimensions of the problem.\n",
        "    T, N = asset_returns_df.shape\n",
        "    K = factor_returns_df.shape[1]\n",
        "\n",
        "    # Initialize a list to store the p-value from each individual alpha test.\n",
        "    p_values = []\n",
        "\n",
        "    # --- Step 1 & 2: Asset-by-Asset t-tests and p-value collection ---\n",
        "\n",
        "    # Prepare the regressor matrix X by adding an intercept.\n",
        "    X = np.hstack([np.ones((T, 1)), factor_returns_df.to_numpy()])\n",
        "\n",
        "    # Pre-compute (X'X)^-1 for efficiency.\n",
        "    try:\n",
        "        xtx_inv = np.linalg.inv(X.T @ X)\n",
        "    except np.linalg.LinAlgError:\n",
        "        logger.warning(\"Factor matrix is singular in AS test; cannot proceed.\")\n",
        "        return False\n",
        "\n",
        "    # Loop through each asset to perform an individual t-test.\n",
        "    for i in range(N):\n",
        "        # Extract the return vector for the current asset.\n",
        "        y = asset_returns_df.iloc[:, i].to_numpy()\n",
        "\n",
        "        # Calculate OLS coefficients: β̂ = (X'X)^-1 * X'y\n",
        "        coeffs = xtx_inv @ X.T @ y\n",
        "\n",
        "        # Calculate residuals: û = y - Xβ̂\n",
        "        residuals = y - X @ coeffs\n",
        "\n",
        "        # Estimate residual variance: σ̂^2 = û'û / (T - K - 1)\n",
        "        sigma_sq = np.sum(np.square(residuals)) / (T - K - 1)\n",
        "\n",
        "        # Calculate the standard error of alpha (the intercept, which is the first coefficient).\n",
        "        se_alpha = np.sqrt(sigma_sq * xtx_inv[0, 0])\n",
        "\n",
        "        # If standard error is near zero, the test is degenerate; skip this asset.\n",
        "        if se_alpha < 1e-9:\n",
        "            continue\n",
        "\n",
        "        # Calculate the t-statistic for the alpha.\n",
        "        t_stat = coeffs[0] / se_alpha\n",
        "\n",
        "        # Calculate the two-tailed p-value from the t-distribution's survival function.\n",
        "        p_val = 2 * t.sf(np.abs(t_stat), df=T - K - 1)\n",
        "        p_values.append(p_val)\n",
        "\n",
        "    # If no valid p-values were generated, the test cannot be completed.\n",
        "    if not p_values:\n",
        "        logger.warning(\"AS test could not be performed; no valid p-values were generated.\")\n",
        "        return False\n",
        "\n",
        "    # --- Step 3: Cauchy Combination ---\n",
        "\n",
        "    # Transform p-values using the inverse CDF of the Cauchy distribution.\n",
        "    # Equation: T_C = (1/N_valid) * Σ tan((0.5 - p_i) * π)\n",
        "    cauchy_terms = np.tan((0.5 - np.array(p_values)) * np.pi)\n",
        "\n",
        "    # The test statistic is the mean of these transformed values.\n",
        "    test_statistic = np.mean(cauchy_terms)\n",
        "\n",
        "    # --- Step 4: Final p-value and Decision ---\n",
        "\n",
        "    # The combined p-value is derived from the CDF of the standard Cauchy distribution.\n",
        "    # Equation: p_combined = 0.5 - arctan(T_C) / π\n",
        "    final_p_value = 0.5 - np.arctan(test_statistic) / np.pi\n",
        "\n",
        "    # Reject the null hypothesis if the combined p-value is less than the significance level.\n",
        "    return final_p_value < tau\n",
        "\n",
        "def _run_single_replication_for_comparison(\n",
        "    experiment: SimulationExperiment,\n",
        "    is_null_hypothesis: bool,\n",
        "    seed: int\n",
        ") -> Optional[Dict[str, bool]]:\n",
        "    \"\"\"\n",
        "    Executes a single replication for multiple tests and returns their decisions.\n",
        "\n",
        "    This function serves as the core computational unit for the comparison\n",
        "    simulation. For a single set of DGP parameters, it generates one dataset\n",
        "    and applies the main test (\"OurTest\") as well as a suite of competing\n",
        "    tests (FLY, AS). It is designed to be robust, handling potential failures\n",
        "    in any individual test without halting the entire replication.\n",
        "\n",
        "    Args:\n",
        "        experiment (SimulationExperiment): The configuration object containing all\n",
        "            parameters for the DGP and the tests.\n",
        "        is_null_hypothesis (bool): A flag indicating whether to generate data\n",
        "            under the null (alphas=0) or the alternative hypothesis.\n",
        "        seed (int): A unique seed for the random number generator to ensure\n",
        "            this specific replication is reproducible.\n",
        "\n",
        "    Returns:\n",
        "        Optional[Dict[str, bool]]: A dictionary where keys are test names\n",
        "            (e.g., 'OurTest', 'FLY', 'AS') and values are booleans indicating\n",
        "            the rejection decision (True for reject, False for fail to reject).\n",
        "            Returns None if the data generation or a critical part of the\n",
        "            setup fails, allowing the parent process to discard the replication.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize a dedicated RNG for this replication to ensure independence.\n",
        "        rng = np.random.default_rng(seed)\n",
        "\n",
        "        # Generate one dataset using the DGP from Task 14.\n",
        "        dgp_data = generate_dgp_data(experiment, is_null_hypothesis, rng)\n",
        "\n",
        "        # Format the generated data into pandas DataFrames.\n",
        "        date_index = pd.to_datetime(pd.date_range(start='2000-01-01', periods=experiment.T, freq='M'))\n",
        "        asset_returns_df = pd.DataFrame(dgp_data.asset_returns, index=date_index, columns=[f'Asset_{i+1}' for i in range(experiment.N)])\n",
        "        factor_returns_df = pd.DataFrame(dgp_data.factor_returns, index=date_index, columns=[f'Factor_{i+1}' for i in range(experiment.dgp_params['K_factors'])])\n",
        "\n",
        "        # --- Run Our Test (Massacci, Sarno, Trapani, Vallarino) ---\n",
        "        test_config = experiment.test_config['our_test']\n",
        "        est_res = estimate_factor_model_params(asset_returns_df, factor_returns_df)\n",
        "        psi_vals = compute_psi_statistic(est_res, nu=test_config['nu'])\n",
        "        de_rand_res = perform_de_randomization(\n",
        "            psi_vals, tau=test_config['de_randomization']['tau'],\n",
        "            b_function=test_config['de_randomization']['B_function'], rng=rng\n",
        "        )\n",
        "        decision = make_test_decision(\n",
        "            de_rand_res, f_b_function=test_config['de_randomization']['f_B_function']\n",
        "        )\n",
        "        our_test_rejection = not decision.fail_to_reject_h0\n",
        "\n",
        "        # --- Run Competing Tests ---\n",
        "        fly_rejection = _run_fly_test(asset_returns_df, factor_returns_df, tau=test_config['de_randomization']['tau'])\n",
        "        as_rejection = _run_as_test(asset_returns_df, factor_returns_df, tau=test_config['de_randomization']['tau'])\n",
        "\n",
        "        # Return a dictionary of boolean rejection decisions.\n",
        "        return {'OurTest': our_test_rejection, 'FLY': fly_rejection, 'AS': as_rejection}\n",
        "\n",
        "    except Exception as e:\n",
        "        # Gracefully handle any failure during a single replication.\n",
        "        logger.error(f\"Comparison replication failed for exp {experiment.experiment_id}: {e}\", exc_info=False)\n",
        "        return None\n",
        "\n",
        "def run_comparison_simulation(\n",
        "    experiment: SimulationExperiment,\n",
        "    n_jobs: int = -1,\n",
        "    seed: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Runs a full Monte Carlo simulation comparing multiple alpha tests.\n",
        "\n",
        "    This function orchestrates a complete simulation experiment to compare the\n",
        "    performance (both empirical size and power) of the paper's proposed test\n",
        "    against specified competitors. It manages the parallel execution of\n",
        "    M replications, aggregates the results, and provides a final summary\n",
        "    DataFrame.\n",
        "\n",
        "    Args:\n",
        "        experiment (SimulationExperiment): The configuration object for the\n",
        "            experimental cell to be simulated.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution of\n",
        "            the M replications. A value of -1 uses all available cores. A value\n",
        "            of 1 runs the simulation serially.\n",
        "        seed (Optional[int]): A master seed to initialize the random number\n",
        "            generator for the entire set of replications. This ensures that\n",
        "            the full simulation run is reproducible.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with two rows ('size', 'power') and one\n",
        "            column for each test being compared (e.g., 'OurTest', 'FLY', 'AS').\n",
        "            The values are the empirical rejection rates.\n",
        "    \"\"\"\n",
        "    # Extract the number of replications for this experiment.\n",
        "    M = experiment.M_replications\n",
        "\n",
        "    # Create a master RNG to generate unique seeds for each replication.\n",
        "    master_rng = np.random.default_rng(seed)\n",
        "    replication_seeds = master_rng.integers(low=0, high=2**32 - 1, size=M)\n",
        "\n",
        "    logger.info(f\"Starting comparison simulation for '{experiment.experiment_id}'...\")\n",
        "\n",
        "    # List to store the results DataFrames for size and power.\n",
        "    results_data = []\n",
        "\n",
        "    # Run the simulation for both the null (is_null=True) and alternative (is_null=False).\n",
        "    for is_null in [True, False]:\n",
        "        # Set a descriptive name for the current run.\n",
        "        hypothesis_name = 'size' if is_null else 'power'\n",
        "\n",
        "        # Execute the M replications in parallel.\n",
        "        results = Parallel(n_jobs=n_jobs)(\n",
        "            delayed(_run_single_replication_for_comparison)(experiment, is_null, seed)\n",
        "            for seed in tqdm(replication_seeds, desc=f\"Simulating {hypothesis_name} for {experiment.experiment_id}\")\n",
        "        )\n",
        "\n",
        "        # Filter out any replications that failed.\n",
        "        successful_results = [res for res in results if res is not None]\n",
        "\n",
        "        # If no replications were successful, log a warning and continue.\n",
        "        if not successful_results:\n",
        "            logger.warning(f\"No successful replications for {hypothesis_name} in {experiment.experiment_id}.\")\n",
        "            continue\n",
        "\n",
        "        # Convert the list of result dictionaries into a DataFrame.\n",
        "        results_df = pd.DataFrame(successful_results)\n",
        "\n",
        "        # Calculate the rejection rate for each test.\n",
        "        rejection_rates = results_df.mean().rename(hypothesis_name)\n",
        "        results_data.append(rejection_rates)\n",
        "\n",
        "    # If both runs failed, return an empty DataFrame.\n",
        "    if not results_data:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Concatenate the size and power Series into a single summary DataFrame.\n",
        "    comparison_df = pd.concat(results_data, axis=1).T\n",
        "    logger.info(f\"Comparison simulation for '{experiment.experiment_id}' complete.\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "def analyze_power_by_sparsity(\n",
        "    base_experiment: SimulationExperiment,\n",
        "    n_jobs: int = -1\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analyzes and computes the empirical power curve of the test.\n",
        "\n",
        "    This function systematically evaluates the power of the proposed test by\n",
        "    running a series of full Monte Carlo simulations across a grid of\n",
        "    \"sparsity levels\". Sparsity here refers to the fraction of assets with\n",
        "    non-zero alphas under the alternative hypothesis. This analysis is crucial\n",
        "    for understanding how the test's ability to detect mispricing changes as\n",
        "    the mispricing becomes less pervasive.\n",
        "\n",
        "    Args:\n",
        "        base_experiment (SimulationExperiment): A fully configured experiment\n",
        "            object that serves as the template. The sparsity level within its\n",
        "            hypothesis_params will be programmatically overridden for each run.\n",
        "        n_jobs (int): The number of CPU cores to use for parallel execution\n",
        "            of the simulations at each sparsity level.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame indexed by the sparsity level. Columns include\n",
        "            'power' (the empirical rejection rate), and 'ci_lower' and 'ci_upper'\n",
        "            for the confidence interval of the power estimate.\n",
        "    \"\"\"\n",
        "    # Extract the grid of sparsity levels to test from the configuration.\n",
        "    sparsity_levels = base_experiment.hypothesis_params['alternative']['sparsity_levels']\n",
        "    logger.info(f\"Starting power analysis across {len(sparsity_levels)} sparsity levels for {base_experiment.experiment_id}.\")\n",
        "\n",
        "    # List to store the results for each sparsity level.\n",
        "    power_results = []\n",
        "\n",
        "    # Iterate through each sparsity level with a progress bar.\n",
        "    for sparsity in tqdm(sparsity_levels, desc=f\"Analyzing Power vs. Sparsity for {base_experiment.experiment_id}\"):\n",
        "        # Create a mutable copy of the hypothesis params to modify.\n",
        "        new_hypothesis_params = copy.deepcopy(base_experiment.hypothesis_params)\n",
        "\n",
        "        # Surgically update the sparsity level for this specific run.\n",
        "        new_hypothesis_params['alternative']['default_sparsity'] = sparsity\n",
        "\n",
        "        # Create a new, immutable experiment tuple with the modified hypothesis.\n",
        "        run_experiment = base_experiment._replace(hypothesis_params=new_hypothesis_params)\n",
        "\n",
        "        # Run the simulation for the alternative hypothesis only.\n",
        "        power_result = run_monte_carlo_simulation(\n",
        "            experiment=run_experiment,\n",
        "            is_null_hypothesis=False,\n",
        "            n_jobs=n_jobs,\n",
        "            seed=int(sparsity * 1e6) # Use a deterministic seed based on sparsity.\n",
        "        )\n",
        "\n",
        "        # Append the structured results to the list.\n",
        "        power_results.append({\n",
        "            'sparsity_level': sparsity,\n",
        "            'power': power_result.rejection_rate,\n",
        "            'ci_lower': power_result.conf_interval[0],\n",
        "            'ci_upper': power_result.conf_interval[1],\n",
        "        })\n",
        "\n",
        "    # Convert the list of results into a final, indexed DataFrame.\n",
        "    power_curve_df = pd.DataFrame(power_results).set_index('sparsity_level')\n",
        "\n",
        "    logger.info(f\"Power analysis by sparsity for {base_experiment.experiment_id} complete.\")\n",
        "    return power_curve_df\n"
      ],
      "metadata": {
        "id": "cSCpKFl7zvgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 18: Results Compilation and Visualization\n",
        "\n",
        "def create_mc_results_table(\n",
        "    mc_results_df: pd.DataFrame,\n",
        "    scenario_filter: str\n",
        ") -> pd.io.formats.style.Styler:\n",
        "    \"\"\"\n",
        "    Creates a publication-quality summary table of Monte Carlo results.\n",
        "\n",
        "    This function takes the tidy DataFrame of all simulation results, filters it\n",
        "    for a specific scenario, and pivots it into a wide-format table showing\n",
        "    empirical size and power across the grid of N and T values, similar to\n",
        "    the tables in Section 5 of the source paper.\n",
        "\n",
        "    Args:\n",
        "        mc_results_df (pd.DataFrame): The complete, tidy DataFrame of results\n",
        "            from the MonteCarloOrchestrator.\n",
        "        scenario_filter (str): The name of the scenario to create the table for.\n",
        "\n",
        "    Returns:\n",
        "        pd.io.formats.style.Styler: A pandas Styler object, which can be\n",
        "            rendered in a Jupyter notebook or exported to HTML/LaTeX.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not isinstance(mc_results_df, pd.DataFrame) or mc_results_df.empty:\n",
        "        raise ValueError(\"mc_results_df must be a non-empty pandas DataFrame.\")\n",
        "    required_cols = {'scenario', 'N', 'T', 'hypothesis', 'rejection_rate'}\n",
        "    if not required_cols.issubset(mc_results_df.columns):\n",
        "        raise ValueError(f\"mc_results_df is missing required columns. Need: {required_cols}\")\n",
        "\n",
        "    # --- Step 1: Filter and Prepare Data ---\n",
        "\n",
        "    # Filter the results for the specified scenario.\n",
        "    scenario_df = mc_results_df[mc_results_df['scenario'] == scenario_filter].copy()\n",
        "\n",
        "    if scenario_df.empty:\n",
        "        logger.warning(f\"No results found for scenario '{scenario_filter}'. Returning empty table.\")\n",
        "        return pd.DataFrame().style\n",
        "\n",
        "    # --- Step 2: Pivot Data for Size and Power ---\n",
        "\n",
        "    # Create the pivot table for empirical size.\n",
        "    size_pivot = scenario_df[scenario_df['hypothesis'] == 'size'].pivot_table(\n",
        "        index='N', columns='T', values='rejection_rate'\n",
        "    )\n",
        "\n",
        "    # Create the pivot table for empirical power.\n",
        "    power_pivot = scenario_df[scenario_df['hypothesis'] == 'power'].pivot_table(\n",
        "        index='N', columns='T', values='rejection_rate'\n",
        "    )\n",
        "\n",
        "    # --- Step 3: Combine and Format the Table ---\n",
        "\n",
        "    # Create a multi-level column index for clarity.\n",
        "    size_pivot.columns = pd.MultiIndex.from_product([['Empirical Size (H₀)'], size_pivot.columns])\n",
        "    power_pivot.columns = pd.MultiIndex.from_product([['Empirical Power (Hₐ)'], power_pivot.columns])\n",
        "\n",
        "    # Concatenate the size and power tables side-by-side.\n",
        "    final_table = pd.concat([size_pivot, power_pivot], axis=1)\n",
        "\n",
        "    # --- Step 4: Style the Table for Publication ---\n",
        "\n",
        "    # Apply styling using the pandas Styler API.\n",
        "    styled_table = (\n",
        "        final_table.style\n",
        "        .set_caption(f\"Monte Carlo Results: {scenario_filter.replace('_', ' ').title()}\")\n",
        "        .format(\"{:.3f}\", na_rep=\"-\")\n",
        "        .background_gradient(cmap='viridis', subset=pd.IndexSlice[:, 'Empirical Power (Hₐ)'])\n",
        "        .background_gradient(\n",
        "            cmap='coolwarm',\n",
        "            subset=pd.IndexSlice[:, 'Empirical Size (H₀)'],\n",
        "            vmin=0.0,\n",
        "            vmax=0.10 # Highlight deviations from the nominal 5% size\n",
        "        )\n",
        "        .set_properties(**{'width': '70px', 'text-align': 'center'})\n",
        "    )\n",
        "\n",
        "    return styled_table\n",
        "\n",
        "def plot_power_curves(\n",
        "    power_curve_data: Dict[str, pd.DataFrame],\n",
        "    title: str = \"Power Curve Analysis by Sparsity of Alternative\",\n",
        "    figsize: Tuple[int, int] = (12, 8)\n",
        ") -> Tuple[plt.Figure, plt.Axes]:\n",
        "    \"\"\"\n",
        "    Creates a publication-quality plot of one or more power curves.\n",
        "\n",
        "    This function visualizes the results from the sparsity analysis, plotting\n",
        "    empirical power as a function of the fraction of non-zero alphas. It can\n",
        "    plot a single curve with its confidence interval or compare multiple curves\n",
        "    for different tests on the same axes.\n",
        "\n",
        "    Args:\n",
        "        power_curve_data (Dict[str, pd.DataFrame]): A dictionary where keys are\n",
        "            the names of the tests (e.g., 'OurTest', 'FLY') and values are the\n",
        "            power curve DataFrames produced by `analyze_power_by_sparsity`.\n",
        "        title (str): The title for the plot.\n",
        "        figsize (Tuple[int, int]): The size of the figure in inches.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the matplotlib Figure and Axes objects.\n",
        "    \"\"\"\n",
        "    # --- Input Validation ---\n",
        "    if not power_curve_data:\n",
        "        raise ValueError(\"Input 'power_curve_data' dictionary cannot be empty.\")\n",
        "\n",
        "    # --- Step 1: Initialize Plot ---\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # --- Step 2 & 3: Plot Power Curves and Confidence Bands ---\n",
        "\n",
        "    # Define a color cycle for comparing multiple tests.\n",
        "    colors = plt.cm.viridis(np.linspace(0, 1, len(power_curve_data)))\n",
        "\n",
        "    # Iterate through each test's power curve data provided in the dictionary.\n",
        "    for i, (test_name, df) in enumerate(power_curve_data.items()):\n",
        "        # Check if the required columns exist.\n",
        "        required_cols = {'power', 'ci_lower', 'ci_upper'}\n",
        "        if not required_cols.issubset(df.columns):\n",
        "            raise ValueError(f\"DataFrame for '{test_name}' is missing required columns.\")\n",
        "\n",
        "        # Plot the main power curve (power vs. sparsity).\n",
        "        ax.plot(\n",
        "            df.index,\n",
        "            df['power'],\n",
        "            label=test_name,\n",
        "            color=colors[i],\n",
        "            marker='o',\n",
        "            linestyle='-',\n",
        "            lw=2\n",
        "        )\n",
        "\n",
        "        # Add a shaded confidence band around the power curve.\n",
        "        ax.fill_between(\n",
        "            df.index,\n",
        "            df['ci_lower'],\n",
        "            df['ci_upper'],\n",
        "            color=colors[i],\n",
        "            alpha=0.2,\n",
        "            label=f'_{test_name}_ci' # Underscore hides it from the main legend\n",
        "        )\n",
        "\n",
        "    # --- Step 4: Final Formatting ---\n",
        "\n",
        "    # Set the title and axis labels.\n",
        "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
        "    ax.set_xlabel(\"Sparsity of Alternative (Fraction of non-zero α)\", fontsize=12)\n",
        "    ax.set_ylabel(\"Empirical Power (Rejection Rate)\", fontsize=12)\n",
        "\n",
        "    # Set logical axis limits.\n",
        "    ax.set_ylim(0, 1.05)\n",
        "    ax.set_xlim(left=0)\n",
        "\n",
        "    # Add a legend to identify the different tests.\n",
        "    ax.legend(title=\"Test\", fontsize=10)\n",
        "\n",
        "    # Ensure the layout is tight.\n",
        "    fig.tight_layout()\n",
        "\n",
        "    logger.info(\"Power curve plot generated successfully.\")\n",
        "\n",
        "    return fig, ax\n"
      ],
      "metadata": {
        "id": "Wqe_s-Xy2Vrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 19: Interpretation and Discussion\n",
        "\n",
        "def _interpret_empirical_results(\n",
        "    empirical_summary_df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes and interprets the empirical results summary table.\n",
        "\n",
        "    Args:\n",
        "        empirical_summary_df (pd.DataFrame): The summary table from `compile_empirical_results`.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing key quantitative findings.\n",
        "    \"\"\"\n",
        "    # Initialize the dictionary to store findings.\n",
        "    findings = {}\n",
        "\n",
        "    # --- Full Sample Analysis ---\n",
        "    # Extract the full sample rejection rates.\n",
        "    full_sample_rates = empirical_summary_df.loc['Full Sample']\n",
        "    # Find the model with the lowest rejection rate (best performance).\n",
        "    best_model = full_sample_rates.idxmin()\n",
        "    # Find the model with the highest rejection rate (worst performance).\n",
        "    worst_model = full_sample_rates.idxmax()\n",
        "    findings['full_sample'] = {\n",
        "        'best_performing_model': f\"{best_model} (Rejection Rate: {full_sample_rates.min():.3f})\",\n",
        "        'worst_performing_model': f\"{worst_model} (Rejection Rate: {full_sample_rates.max():.3f})\",\n",
        "        'model_ranking_by_rejection_rate': full_sample_rates.sort_values().to_dict()\n",
        "    }\n",
        "\n",
        "    # --- Crisis Period Analysis ---\n",
        "    crisis_findings = {}\n",
        "    # Isolate the crisis periods by dropping the 'Full Sample' row.\n",
        "    crisis_df = empirical_summary_df.drop('Full Sample')\n",
        "    # Find the best performing model for each crisis.\n",
        "    crisis_bests = crisis_df.idxmin(axis=1)\n",
        "    for period, model in crisis_bests.items():\n",
        "        rate = crisis_df.loc[period, model]\n",
        "        crisis_findings[period] = f\"Best performer: {model} (Rejection Rate: {rate:.3f})\"\n",
        "\n",
        "    # Analyze the degradation in performance during crises compared to the full sample.\n",
        "    performance_degradation = crisis_df.subtract(full_sample_rates, axis=1)\n",
        "    crisis_findings['max_performance_degradation'] = performance_degradation.max().idxmax()\n",
        "\n",
        "    findings['crisis_analysis'] = crisis_findings\n",
        "\n",
        "    return findings\n",
        "\n",
        "def _interpret_mc_results(\n",
        "    mc_results_df: pd.DataFrame,\n",
        "    nominal_size: float\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Analyzes and interprets the Monte Carlo simulation results.\n",
        "\n",
        "    Args:\n",
        "        mc_results_df (pd.DataFrame): The tidy DataFrame from `MonteCarloOrchestrator`.\n",
        "        nominal_size (float): The nominal size (τ) of the test.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing key quantitative findings about the test's properties.\n",
        "    \"\"\"\n",
        "    findings = {}\n",
        "\n",
        "    # --- Size Control Analysis ---\n",
        "    # Filter for the empirical size results.\n",
        "    size_df = mc_results_df[mc_results_df['hypothesis'] == 'size'].copy()\n",
        "    # Check where the nominal size is outside the result's confidence interval.\n",
        "    size_df['is_distorted'] = (size_df['ci_lower'] > nominal_size) | (size_df['ci_upper'] < nominal_size)\n",
        "\n",
        "    num_distorted = size_df['is_distorted'].sum()\n",
        "    total_experiments = len(size_df)\n",
        "\n",
        "    findings['size_control'] = {\n",
        "        'summary': f\"Test exhibits size distortion in {num_distorted} out of {total_experiments} cells.\",\n",
        "        'distorted_cells': size_df[size_df['is_distorted']][['scenario', 'N', 'T', 'rejection_rate']].to_dict('records')\n",
        "    }\n",
        "\n",
        "    # --- Power Analysis ---\n",
        "    # Filter for the empirical power results.\n",
        "    power_df = mc_results_df[mc_results_df['hypothesis'] == 'power']\n",
        "    # Find the average power across all scenarios.\n",
        "    avg_power = power_df['rejection_rate'].mean()\n",
        "    # Find conditions with low power (e.g., < 50%).\n",
        "    low_power_cells = power_df[power_df['rejection_rate'] < 0.5]\n",
        "\n",
        "    findings['power_analysis'] = {\n",
        "        'average_power': f\"{avg_power:.3f}\",\n",
        "        'summary_low_power': f\"{len(low_power_cells)} out of {len(power_df)} cells show power < 50%.\",\n",
        "        'low_power_cells': low_power_cells[['scenario', 'N', 'T', 'rejection_rate']].to_dict('records')\n",
        "    }\n",
        "\n",
        "    return findings\n",
        "\n",
        "def generate_results_summary(\n",
        "    empirical_results: Optional[pd.DataFrame] = None,\n",
        "    mc_results: Optional[pd.DataFrame] = None,\n",
        "    replication_config: Optional[Dict[str, Any]] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Generates a structured, quantitative summary of all study results.\n",
        "\n",
        "    This function serves as the final analysis step, taking the compiled\n",
        "    results from the empirical and Monte Carlo studies and producing a\n",
        "    dictionary of key, interpretable findings. It does not generate prose,\n",
        "    but rather provides the quantitative building blocks for a final research report.\n",
        "\n",
        "    Args:\n",
        "        empirical_results (Optional[pd.DataFrame]): The summary table of empirical\n",
        "            rejection rates from `compile_empirical_results`.\n",
        "        mc_results (Optional[pd.DataFrame]): The tidy DataFrame of all Monte Carlo\n",
        "            simulation results from `MonteCarloOrchestrator`.\n",
        "        replication_config (Optional[Dict[str, Any]]): The study's configuration,\n",
        "            needed for context like nominal test size.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the structured summary.\n",
        "    \"\"\"\n",
        "    logger.info(\"--- Generating Final Results Summary ---\")\n",
        "\n",
        "    # Initialize the master dictionary for all findings.\n",
        "    master_summary = {}\n",
        "\n",
        "    # --- Interpret Empirical Results if provided ---\n",
        "    if empirical_results is not None:\n",
        "        if not isinstance(empirical_results, pd.DataFrame):\n",
        "            raise TypeError(\"empirical_results must be a pandas DataFrame.\")\n",
        "        logger.info(\"Interpreting empirical results...\")\n",
        "        master_summary['empirical_findings'] = _interpret_empirical_results(empirical_results)\n",
        "\n",
        "    # --- Interpret Monte Carlo Results if provided ---\n",
        "    if mc_results is not None:\n",
        "        if not isinstance(mc_results, pd.DataFrame):\n",
        "            raise TypeError(\"mc_results must be a pandas DataFrame.\")\n",
        "        if replication_config is None:\n",
        "            raise ValueError(\"replication_config is required to interpret Monte Carlo results.\")\n",
        "\n",
        "        logger.info(\"Interpreting Monte Carlo simulation results...\")\n",
        "        # Extract the nominal size (tau) from the configuration for comparison.\n",
        "        nominal_size = replication_config['monte_carlo']['test_config']['our_test']['de_randomization']['tau']\n",
        "        master_summary['monte_carlo_findings'] = _interpret_mc_results(mc_results, nominal_size)\n",
        "\n",
        "    if not master_summary:\n",
        "        logger.warning(\"No results were provided to generate a summary.\")\n",
        "        return {\"status\": \"No results to summarize.\"}\n",
        "\n",
        "    logger.info(\"--- Final Summary Generation Complete ---\")\n",
        "\n",
        "    return master_summary\n"
      ],
      "metadata": {
        "id": "_xng3pvU3yOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Master Pipeline Function\n",
        "\n",
        "def run_full_study(\n",
        "    asset_returns: pd.DataFrame,\n",
        "    factor_returns: pd.DataFrame,\n",
        "    replication_config: Dict[str, Any],\n",
        "    run_empirical: bool = True,\n",
        "    run_monte_carlo: bool = True,\n",
        "    n_jobs: int = -1,\n",
        "    seed: int = 42\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end research pipeline for the randomized alpha test.\n",
        "\n",
        "    This master orchestrator function serves as the main entry point for the\n",
        "    entire study. It manages the execution of both the empirical analysis on the\n",
        "    provided data and the comprehensive Monte Carlo simulations to evaluate the\n",
        "    test's statistical properties.\n",
        "\n",
        "    Args:\n",
        "        asset_returns (pd.DataFrame): Raw DataFrame of asset returns.\n",
        "        factor_returns (pd.DataFrame): Raw DataFrame of factor returns.\n",
        "        replication_config (Dict[str, Any]): The complete study configuration.\n",
        "        run_empirical (bool): If True, the empirical analysis will be run.\n",
        "        run_monte_carlo (bool): If True, the Monte Carlo simulations will be run.\n",
        "        n_jobs (int): The number of CPU cores to use for parallelizable tasks,\n",
        "                      primarily the Monte Carlo simulations. -1 uses all available.\n",
        "        seed (int): A master seed for all random operations to ensure the\n",
        "                    entire study is fully reproducible.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the key outputs from each major\n",
        "                        stage of the research pipeline, including DataFrames of\n",
        "                        results, generated plots, and summary interpretations.\n",
        "    \"\"\"\n",
        "    # Initialize the dictionary to hold all outputs of the study.\n",
        "    study_outputs = {}\n",
        "\n",
        "    # --- I. EMPIRICAL ANALYSIS ---\n",
        "    if run_empirical:\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"STARTING: EMPIRICAL ANALYSIS\")\n",
        "        logger.info(\"=\"*80)\n",
        "        try:\n",
        "            # Task 9: Instantiate and run the empirical orchestrator.\n",
        "            # This single object handles validation, cleaning, setup, and execution.\n",
        "            empirical_orchestrator = EmpiricalAnalysisOrchestrator(\n",
        "                asset_returns=asset_returns,\n",
        "                factor_returns=factor_returns,\n",
        "                replication_config=replication_config,\n",
        "                seed=seed\n",
        "            )\n",
        "            empirical_results_df = empirical_orchestrator.run_analysis()\n",
        "            study_outputs['empirical_timeseries_results'] = empirical_results_df\n",
        "\n",
        "            # Task 11: Compile the empirical results into a summary table.\n",
        "            empirical_summary_table = compile_empirical_results(\n",
        "                empirical_results_df, replication_config\n",
        "            )\n",
        "            study_outputs['empirical_summary_table'] = empirical_summary_table\n",
        "\n",
        "            # Task 12: Visualize the empirical results.\n",
        "            fig, ax = plot_q_statistic_time_series(\n",
        "                empirical_results_df, replication_config\n",
        "            )\n",
        "            study_outputs['empirical_q_statistic_plot'] = (fig, ax)\n",
        "\n",
        "            logger.info(\"EMPIRICAL ANALYSIS COMPLETED SUCCESSFULLY.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any failure during the empirical analysis stage.\n",
        "            logger.error(f\"Empirical analysis failed: {e}\", exc_info=True)\n",
        "            study_outputs['empirical_analysis_error'] = str(e)\n",
        "    else:\n",
        "        logger.info(\"Skipping Empirical Analysis as per configuration.\")\n",
        "\n",
        "    # --- II. MONTE CARLO SIMULATION ---\n",
        "    if run_monte_carlo:\n",
        "        logger.info(\"=\"*80)\n",
        "        logger.info(\"STARTING: MONTE CARLO SIMULATION\")\n",
        "        logger.info(\"=\"*80)\n",
        "        try:\n",
        "            # Task 16: Instantiate and run the Monte Carlo orchestrator.\n",
        "            mc_orchestrator = MonteCarloOrchestrator(\n",
        "                replication_config=replication_config,\n",
        "                seed=seed\n",
        "            )\n",
        "            mc_results_df = mc_orchestrator.run(n_jobs=n_jobs)\n",
        "            study_outputs['monte_carlo_raw_results'] = mc_results_df\n",
        "\n",
        "            # Task 18 (Part 1): Compile MC results into summary tables.\n",
        "            mc_summary_tables = {}\n",
        "            for scenario in replication_config['monte_carlo']['run_controls']['scenarios']:\n",
        "                mc_summary_tables[scenario] = create_mc_results_table(\n",
        "                    mc_results_df, scenario_filter=scenario\n",
        "                )\n",
        "            study_outputs['monte_carlo_summary_tables'] = mc_summary_tables\n",
        "\n",
        "            logger.info(\"MONTE CARLO SIMULATION COMPLETED SUCCESSFULLY.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            # Handle any failure during the Monte Carlo simulation stage.\n",
        "            logger.error(f\"Monte Carlo simulation failed: {e}\", exc_info=True)\n",
        "            study_outputs['monte_carlo_simulation_error'] = str(e)\n",
        "    else:\n",
        "        logger.info(\"Skipping Monte Carlo Simulation as per configuration.\")\n",
        "\n",
        "    # --- III. FINAL INTERPRETATION ---\n",
        "    logger.info(\"=\"*80)\n",
        "    logger.info(\"STARTING: FINAL RESULTS INTERPRETATION\")\n",
        "    logger.info(\"=\"*80)\n",
        "    try:\n",
        "        # Task 19: Generate a quantitative summary of all available results.\n",
        "        final_summary = generate_results_summary(\n",
        "            empirical_results=study_outputs.get('empirical_summary_table', pd.DataFrame()).style.data,\n",
        "            mc_results=study_outputs.get('monte_carlo_raw_results'),\n",
        "            replication_config=replication_config\n",
        "        )\n",
        "        study_outputs['final_quantitative_summary'] = final_summary\n",
        "        logger.info(\"FINAL INTERPRETATION COMPLETED SUCCESSFULLY.\")\n",
        "    except Exception as e:\n",
        "        # Handle any failure during the final summary generation.\n",
        "        logger.error(f\"Final summary generation failed: {e}\", exc_info=True)\n",
        "        study_outputs['summary_generation_error'] = str(e)\n",
        "\n",
        "    # Return the comprehensive dictionary of all study artifacts.\n",
        "    return study_outputs\n"
      ],
      "metadata": {
        "id": "MCyDts2W6Y-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}